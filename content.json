[{"title":"种子批量转磁链","date":"2019-04-03T10:19:56.731Z","path":"2019/04/03/种子批量转磁链/","text":"历史该技术由美国的程序员布莱姆·科亨于2001年4月时发布，并于2001年7月2日时首次正式应用。 原理普通的HTTP／FTP下载使用TCP/IP协议，BitTorrent协议是架构于TCP/IP协议之上的一个P2P文件传输通信协议，处于TCP/IP结构的应用层。根据BitTorrent协议，文件发布者会根据要发布的文件生成提供一个.torrent文件，即种子文件，也简称为“种子”。 组成种子文件本质是文本文件，包括Tracker和文件信息两个部分：Tracker服务器保存所有正在下载文件的客户端的地址，有人新建连接时，会将地址反馈给新的连接。文件信息是用Bencode进行编码，是要下载的文件的索引。 Tracker信息* Tracker服务器地址 * Tracker服务器设置 文件信息 announce - tracker的URL info - 该条映射到一个字典，该字典的键将取决于共享的一个或多个文件 name - 建议保存到的文件和目录名称 piece length - 每个文件块的字节数。通常为256KB = 262144B pieces - 每个文件块的SHA-1的集成Hash。因为SHA-1会返回160-bit的Hash，所以pieces将会得到1个160-bit的整数倍的字符串。和一个length（相当于只有一个文件正在共享）或files（相当于当多个文件被共享）： length - 文件的大小（以字节为单位） files - 一个字典的列表（每个字典对应一个文件）与以下的键 path - 一个对应子目录名的字符串列表，最后一项是实际的文件名称 length - 文件的大小（以字节为单位） info-hash:每一个种子唯一的编码，由info字段的数据计算而成。 DHT网络DHT全称为分布式哈希表（Distributed Hash Table），是一种分布式存储方法。在不需要服务器的情况下，每个客户端负责一个小范围的路由，并负责存储一小部分数据，从而实现整个DHT网络的寻址和存储。 磁链历史这个标准的草稿出现于2002年，是为了对eDonkey2000的“ed2k:”和Freenet的“freenet:”两个URI格式进行“厂商与项目中立化”（vendor- and project-neutral generalization）而制定的。同时这个标准也尝试紧密地跟进IETF官方的URI标准。 原理特点： 分布式。 不依赖于ip地址 没有中心服务器 开源组成磁力链接由一组参数组成，参数间的顺序没有讲究，其格式与在HTTP链接末尾的查询字符串相同。最常见的参数是”xt”，是”exact topic”的缩写，通常是一个特定文件的内容散列函数值形成的URN。1magnet:?xt=urn:sha1:YNCKHTQCWBTRNJIV4WNAE52SJUQCZO5C 其值是Base32编码的文件的SHA-1散列。 基本描述1magnet:? xl = [字节大小]&amp; dn = [文件名（已编码URL）]&amp; xt = urn: tree: tiger: [ TTH hash（Base32）] 由参数来指定相关的内容： dn（显示名称）- 文件名 xl（绝对长度）- 文件字节数 xt（eXact Topic）- 包含文件散列函数值的URN。磁力链接的这部分最重要。用于寻找和验证包含着磁力链接中的文件。 as（可接受来源） - 在线文件的网络链接 xs（绝对资源）- P2P链接 kt（关键字）- 用于搜索的关键字 mt（文件列表）- 链接到一个包含磁力链接的元文件 (MAGMA - MAGnet MAnifest） tr（Tracker地址）- BT下载的Tracker URL开发工具和平台 计算提取info_hash关键：info字段的值，必须先解码，再对info字段的值编码，然后才计算hash。直接从文件中删除其他部分行不通。 jsbencode库1npm install bencode 如果装了nvm,则-g全局安装会装在.nvm文件夹下。否则，就装在用户目录的node_module下。 sha-1库1npm install js-sha1 实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869var fs = require(\"fs\");var bencode = require('bencode');var sha1 = require('js-sha1');var arguments = process.argv.splice(2); //获取命令行参数，第三个元素是带的参数outMagnets(getMagnets(arguments), arguments);// 获取磁链链接，arguments为文件夹路径function getMagnets(arguments) &#123; var fileslist = fs.readdirSync(arguments.toString()); var magnetlist = new Array(); for (var f in fileslist) &#123; var torrentfile = null; var filename = fileslist[f].toString(); if (filename.includes(\".torrent\")) &#123; var filepath = arguments.toString() + '/' + filename; var magnet = getInfoHash(filepath); if (magnet) &#123; magnetlist.push(magnet); &#125; //console.log(magnet); &#125; &#125; return magnetlist;&#125;// 将结果输出为txtfunction outMagnets(magnets, path) &#123; if (magnets.length &gt; 0) &#123; var writebuffer = Buffer.from(arrayToString(magnets, '\\n')); var savepath = path.toString() + '/' + \"magnets.txt\"; var writesteam = fs.createWriteStream(savepath); writesteam.write(writebuffer, 'utf-8'); writesteam.end(); writesteam.on('finish', function () &#123; console.log(\"写入完成\"); &#125;); writesteam.on('error', function (err) &#123; console.log(err); &#125;); &#125;&#125;// buffer只能输出字符，所以必须将字符数组转换为字符形式，seq为分隔符function arrayToString(arr, seq) &#123; var str_value = null; for (a of arr) &#123; var astr = a.toString(); if (str_value) &#123; str_value = str_value + seq + astr; &#125; else &#123; str_value = astr; &#125; &#125; return str_value;&#125;// 获取种子文件的info_hash值，有一个解密，再加密的过程function getInfoHash(torrentfile) &#123; var result = bencode.decode(fs.readFileSync(torrentfile)); if (result) &#123; var info = result['info']; //info 字典 var info_hash = sha1(bencode.encode(info)); var magnet = \"magnet:?xt=urn:btih:\" + info_hash.toString(); return magnet; &#125; else &#123; return null; &#125;&#125; pythonbencode库gittub安装：1pip3 install py3-bencode 实现代码123456789from bencode import bencode, bdecodefrom io import BytesIOimport hashlibobjTorrentFile = open(\"test.torrent\", \"rb\")decodedDict = bdecode(objTorrentFile.read())info_hash = hashlib.sha1(bencode(decodedDict[\"info\"])).hexdigest()print(info_hash) 在解码提取出info后，似乎还需要编码后，再求hash值 参考网址磁力链接wiki磁力链接转换为种子文件官方种子格式说明种子格式详解bencode详解python bencode可用bencode-gitthubbencode源码js-bencode 实现wiki-bencode","tags":[{"name":"javascript","slug":"javascript","permalink":"https://109383670.github.io/tags/javascript/"},{"name":"小工具","slug":"小工具","permalink":"https://109383670.github.io/tags/小工具/"}]},{"title":"iOS开发常见问题","date":"2019-03-09T06:09:01.636Z","path":"2019/03/09/iOS 开发常见问题/","text":"隐私政策问题从2018年10月3号起，所有新提交的App都必须提供隐私条例。官方链接可以利用网站在线生成，并且都提供在线页面。不过没有中文版本。部分条款要收费，不过free 版本的就足够用了。提供在线生成的网站：freeprivacypolicyprivacypolicies 视频预览问题对于竖屏的视频，mac自带的iMovies无法处理，必须用cut final pro来进行处理。导出视频的格式如下：App 预览规范屏幕快照规范","tags":[{"name":"iOS 开发","slug":"iOS-开发","permalink":"https://109383670.github.io/tags/iOS-开发/"}]},{"title":"cocos2d-x 3.0相对于2.0的变化","date":"2019-03-02T07:44:18.102Z","path":"2019/03/02/cocos2d-x 3.0相对于2.0的变化/","text":"更新步骤 下载开发包（github也可以)。 运行build目录下，cocos2d_tests.xcodeproj工程。 将编译Target选择成cpp-tests iOS。 运行。 用命令行命令生成一个模板工程：cocos new MyGame -p com.MyCompany.MyGame -l cpp -d ~/MyCompany将2.0代码加入模板工程中。如果是3.0升级就用模板工程中的cocos2d代替老版本的文件夹。 CClog 变成CCLOGCCArray 变成Array, CCSet等也变成setCCLayer中setTouchEnabled无效添加触摸事件器后，自动生效。 ccTouchBegan等改成onTouchesBegan触摸事件代码12345auto listener = EventListenerTouchAllAtOnce::create();listener-&gt;onTouchesBegan = CC_CALLBACK_2(ForceTouchTest::onTouchesBegan, this);listener-&gt;onTouchesMoved = CC_CALLBACK_2(ForceTouchTest::onTouchesMoved, this);listener-&gt;onTouchesEnded = CC_CALLBACK_2(ForceTouchTest::onTouchesEnded, this);_eventDispatcher-&gt;addEventListenerWithSceneGraphPriority(listener, this); CCObject改成RefCCPoint改成Vec2CCJumpTO等动作改成JumpToCCPointZero改成Vec2::Zero除了0向量外，还有很多的静态值。 动作回调CallFuncN有改动123456ac_go = Sequence::create(ac_p, ac_s, ac_s1,CallFuncN::create(this, callfuncN_selector(BlockBoard::call_onBeginAction)),nullptr);//改成：ac_go = Sequence::create(ac_p,ac_s,ac_s1,CallFuncN::create(CC_CALLBACK_1(BlockBoard::call_onBeginAction,this)),nullptr); CC_CALLBACK_1 CC_CALLBACK_2 CC_CALLBACK_3后面的数字表示带参数的多少。 ssize_t格式化参数%zdssize_t：有符号整数，与平台无关。适配不同平台的通用整数关键字，在32位平台为32，64位平台为long。size_t：无符号整数，与平台无关。ssize_t格式化参数是%zd, size_t是%tu。 GameCenter ReportScore有更新代码如下：123456789101112131415161718- (void) reportScore: (int64_t) score forCategory: (NSString*) category&#123; GKScore *scoreReporter = [[[GKScore alloc] initWithLeaderboardIdentifier:category] autorelease]; scoreReporter.value = score; scoreReporter.context = 0; NSArray *scores = @[scoreReporter]; [GKScore reportScores:scores withCompletionHandler:^(NSError *error) &#123; //Do something interesting here. if (error != nil)&#123; // handle the reporting error NSLog(@\"上传分数出错.\"); &#125;else &#123; NSLog(@\"上传分数成功\"); &#125; &#125;];&#125; Size会有重名错误，用cocos2d::SizeMenuItemImage 有变化代码1MenuItemImage* item_close = MenuItemImage::create(\"close.png\", \"close_p.png\", CC_CALLBACK_1(InfoShowLayer::onClose, this)); 向量操作Vec2可以直接数学符号操作 Label FontTTF, 系统字体，BMFont，都由Label类负责创建1234Label* label = Label::createWithSystemFont(str-&gt;getCString(), \"Mark Felt\", fontSize);Label::createWithTTFLabel::createWithBMFontLabel::createWithCharMap 设置资源路径已经适配模式12searchPath.push_back(\"iphone\");glview&gt;setDesignResolutionSize(750,1334,ResolutionPolicy::FIXED_HEIGHT); 启动图片和应用图标都有更新启动图片决定了初始分辨率的大小。不提供相应的启动图片，不能获得正确的初始分辨率。 参考常用格式化参数CC_CALLBACK_0, CC_CALLBACK_1, CC_CALLBACK_2, CC_CALLBACK_3","tags":[{"name":"cocos2d-x","slug":"cocos2d-x","permalink":"https://109383670.github.io/tags/cocos2d-x/"},{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"}]},{"title":"Scrapy爬虫项目纪录","date":"2019-02-19T16:46:54.381Z","path":"2019/02/20/Scrapy爬虫项目纪录/","text":"目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。 编程环境 VSCode Python3 Scrapy 安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL 提示没有安装win32api用pip 安装win32： 1pip install pywin32 安装命令1pip install scrapy 更新命令1sudo pip install --upgrade scrapy mac 下安装mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。mac下用Homebrew来进行升级 安装xcode命令行工具 1xcode-select --install https://brew.sh/ 安装Homebrew 将Homebrew加入环境变量中 12echo \"export PATH=/usr/local/bin:/usr/local/sbin:$PATH\" &gt;&gt; ~/.bashrcsource ~/.bashrc 安装python 1brew install python 如果已经安装，可以进行升级 1brew update; brew upgrade python 安装scrapy1pip3 install scrapy 学习记录生成Scrapy框架SCrapy必须在固定的框架下运行，可以自动生成后再去改动。1scrapy startproject 工程名 HelloWorld代码12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): # 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法 name = \"quotes\" # 唯一的爬虫名字，在运行时要用到 def start_requests(self): # 复写的方法，初始请求的网址 urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): # 复写的方法，在这里对爬下的数据进行处理 page = response.url.split(\"/\")[-2] filename = 'quotes-%s.html' % page with open(filename, 'wb') as f: f.write(response.body) self.log('Saved file %s' % filename) 运行命令： 1scrapy crawl quotes 深入学习例子1-提取内容123456789101112131415161718# 提取相关格言以及作者等信息import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), 'tags': quote.css('div.tags a.tag::text').getall(), &#125; 输出json或者jl(JSON Lines)命令123scrapy crawl quotes -o quotes.jsonscrapy crawl quotes -o quotes.jl 例子2-爬取下一个链接12345678910111213141516171819202122import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), 'tags': quote.css('div.tags a.tag::text').getall(), &#125; next_page = response.css('li.next a::attr(href)').get() if next_page is not None: next_page = response.urljoin(next_page) #获得真实的链接地址 yield scrapy.Request(next_page, callback=self.parse) #下一个链接的处理回调 后面两句可以用下面的代替，不用写urljoin了。 1yield response.follow(next_page, callback=self.parse) 进一步简化： 12for href in response.css('li.next a::attr(href)'): yield response.follow(href, callback=self.parse) 再进一步简化：对于a 标签，会自动使用它的href属性12for a in response.css('li.next a'): yield response.follow(a, callback=self.parse) 进阶例子123456789101112131415161718192021222324252627import scrapyclass AuthorSpider(scrapy.Spider): name = 'author' start_urls = ['http://quotes.toscrape.com/'] def parse(self, response): # follow links to author pages for href in response.css('.author + a::attr(href)'): yield response.follow(href, self.parse_author) # follow pagination links for href in response.css('li.next a::attr(href)'): yield response.follow(href, self.parse) def parse_author(self, response): def extract_with_css(query): return response.css(query).get(default='').strip() yield &#123; 'name': extract_with_css('h3.author-title::text'), 'birthdate': extract_with_css('.author-born-date::text'), 'bio': extract_with_css('.author-description::text'), &#125; 命令行参数例子123456789101112131415161718192021222324import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" def start_requests(self): url = 'http://quotes.toscrape.com/' tag = getattr(self, 'tag', None) #从命令行参数获得 if tag is not None: url = url + 'tag/' + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), &#125; next_page = response.css('li.next a::attr(href)').get() if next_page is not None: yield response.follow(next_page, self.parse) 命令 1scrapy crawl quotes -o quotes-humor.json -a tag=humor 结果 1http://quotes.toscrape.com/tag/humor item可以自己定义的数据结构格式如下1234567import scrapyclass Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) item pipeline处理item数据的地方，在parse中返回item,就会调用该方法。格式如下12345678910111213from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item.get('price'): if item.get('price_excludes_vat'): item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 1234567891011121314import jsonclass JsonWriterPipeline(object): def open_spider(self, spider): self.file = open('items.jl', 'w') def close_spider(self, spider): self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item 在setting里启动pipeline1234ITEM_PIPELINES = &#123; 'myproject.pipelines.PricePipeline': 300, #数字表示优先顺序，越小的越先执行 'myproject.pipelines.JsonWriterPipeline': 800,&#125; 例子：12345678910111213141516171819202122232425262728from mySpider.items import ItcastItemdef parse(self, response): #open(\"teacher.html\",\"wb\").write(response.body).close() # 存放老师信息的集合 #items = [] for each in response.xpath(\"//div[@class='li_txt']\"): # 将我们得到的数据封装到一个 `ItcastItem` 对象 item = ItcastItem() #extract()方法返回的都是unicode字符串 name = each.xpath(\"h3/text()\").extract() title = each.xpath(\"h4/text()\").extract() info = each.xpath(\"p/text()\").extract() #xpath返回的是包含一个元素的列表 item['name'] = name[0] item['title'] = title[0] item['info'] = info[0] #items.append(item) #将获取的数据交给pipelines yield item # 返回数据，不经过pipeline #return items 中文乱码转为utf-8python3默认为unicode,如果输出为中文，则要转为utf-8，不然会是乱码代码如下：123456789101112131415161718import jsonimport codecsimport osclass Pipeline(object): def __init__(self): self.file = codecs.open( 'items.json', 'w', encoding='utf-8') def close_spider(self, spider): self.file.seek(-1, os.SEEK_END) self.file.truncate() self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(line) return item imagepipeline各函数运行流程 imagepipeline启动 get_media_requests 将所有的下载请求一次全部完成 下载完成后再统一执行item_completed 同时下载多个图片并改名重写file_path函数实现12345678910111213141516171819202122232425262728293031 def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" #这里传递字符，或者图片列表，如果是单个的对象，则非常容易被覆盖 yield scrapy.Request(item['pic_url'], meta=&#123;'item': item['pic_name']&#125;) def file_path(self, request, response=None, info=None): \"\"\" : param request: 每一个图片下载管道请求 : param response: : param info: : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录 : return: 每套图的分类目录 \"\"\" item = request.meta['item'] folder = item folder_strip = strip(folder) # img_path = \"%s%s\" % (self.img_store, folder_strip) filename = folder_strip + '/' + folder_strip + '.jpg' return filename def strip(path): \"\"\" :param path: 需要清洗的文件夹名字 :return: 清洗掉Windows系统非法文件夹名字的字符串 \"\"\" path = re.sub(r'[？\\\\*|“&lt;&gt;:/]', '', str(path)) return path Request 回调传递参数1234scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;'item': item, 'param': name&#125;)在parse中提取参数item = response.meta['item'] 结果去重 Request的参数 dont_filter=False 默认去重 启用一个爬虫的持久化，运行以下命令:1scrapy crawl somespider -s JOBDIR=crawls/somespider-1 然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。恢复这个爬虫也是同样的命令:1scrapy crawl somespider -s JOBDIR=crawls/somespider-1 这样爬虫断掉后，再启动会接着上次的 url 跑。 如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数运行爬虫如：1scrapy crawl spider1 -L WARNING 不打印Debug信息，可以清楚得看到运行过程。 scrapy-red 错误记录pipeline is not a full path应该在 setting 中填入完整的管道的路径，如：1pic.pipelines.PicImagesDownloadPipeline 如果只填PicImagesDownloadPipeline,就会出现这个错误。 Symbol not found: _PyInt_AsLong 错误将系统python目录下的PIL和Pillow库都删除，再用pip3安装在 Python3的安装目录下系统python安装目录：1/Library/Python/2.7/site-packages Missing scheme in request url: h相关URL必须是一个List，所以遇到该错误只需要将url转换成list即可。例如：start_urls = [‘someurls’]如果是images_url也是如此，使用item存储的时候改成list即可。item[‘images_urls’] = [‘image_url’] Request url must be str or unicode请求的url参数不能是一个列表，必须是一个字符 在item_complete中改名多个图片不成功item_complete并不是在get_media_requests下载图片后马上启动的，它是要等所有的图片下载完成，再统一启动complete事件，这样就导致多个图片没法改名，不能获得之前的item的字段。改名需要重写file_path get_media_requests中回调参数要小心meta中可以加入回调的参数，如果传递的是对象要非常小心，如果对象发生变化，会导致后面所有的回调参数发生变化，传递的如果是字符，就没有这个风险。1234567def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" yield scrapy.Request(item['pic_url'], meta=&#123;'item': item['pic_name']&#125;) Filtered duplicate request有重复下载的请求，如果要重复下载，在Request函数里加上参数 dont_filter=True，默认是False 最终代码piczz.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import scrapyfrom piczz.items import PiczzItemclass piczzSpider(scrapy.Spider): name = \"piczz\" allowed_domains = [\"\"] start_urls = [\"\"] img_paths = [] def parse(self, response): for each in response.xpath( \"//div[@class = 'post_box']\"): # extract()方法返回的都是unicode字符串 item = PiczzItem() item['name'] = 'startpage' self.img_paths.clear() item['pic_name'] = each.xpath( \"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/text()\").extract()[0] item['pic_url'] = each.xpath( \"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/@href\").extract()[0] yield scrapy.Request(item['pic_url'], callback=self.parse_imgs, meta=&#123;'item': item&#125;) #递归下一页图片 next_path = response.xpath( \"descendant::div[@class = 'page_num']/a[last()]\") next_con = next_path.xpath(\"text()\").extract()[0] next_con = next_con.strip() next_page = \"\" if next_con == \"下一頁 »\": next_page = next_path.xpath(\"@href\").extract()[0] print(next_page) if next_path is not None: yield scrapy.Request(next_page, self.parse) else: return # 下载一个索引页的图片 def parse_imgs(self, response): self.img_paths.clear() item = response.meta['item'] imgs = response.xpath( \"descendant::div[@class = 'entry-content']/p/img/@src\").extract() for e in imgs: self.img_paths.append(e) item['pic_paths'] = self.img_paths next_path = response.xpath( \"descendant::div[@class = 'wp-pagenavi']/p/a[last()]\") next_con = next_path.xpath(\"text()\").extract()[0] next_con = next_con.strip() if next_con == \"下一页\": next_page = next_path.xpath(\"@href\").extract()[0] if next_page is not None: yield scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;'item': item&#125;) yield item item.py 12345678import scrapyclass PiczzItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pic_name = scrapy.Field() # 图片目录名 pic_url = scrapy.Field() # 图片索引首页地址 pic_paths = scrapy.Field() # 图片下载地址列表 pipeline.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import jsonimport shutilimport codecsimport osimport reimport scrapyimport PILfrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.utils.project import get_project_settingsclass PiczzImagesDownloadPipeline(ImagesPipeline): def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" for img_url in item['pic_paths']: yield scrapy.Request(img_url, meta=&#123;'item': item['pic_name']&#125;) def file_path(self, request, response=None, info=None): \"\"\" : param request: 每一个图片下载管道请求 : param response: : param info: : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录 : return: 每套图的分类目录 \"\"\" item = request.meta['item'] folder = item folder_strip = strip(folder) image_guid = request.url.split('/')[-1] filename = folder_strip + '/' + image_guid + '.jpg' return filename def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") return itemdef strip(path): \"\"\" :param path: 需要清洗的文件夹名字 :return: 清洗掉Windows系统非法文件夹名字的字符串 \"\"\" path = re.sub(r'[？\\\\*|“&lt;&gt;:/]', '', str(path)) return path 总结从搭建环境到断断续续的学习花了大概五天时间 ，每天平均花二个小时学习，终于成功的将设定的目标完成。 参考网站官网中文参考网站xPath语法Python中yield的解释mac os Python路径总结Scrapy框架入门简介ImagesPipeline下载图片ImagesPipeline下载图片保持原文件名小白进阶之Scrapy第四篇Python中yield的解释scrapy调用parse()中使用yield引发对yield的分析","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Python","slug":"Python","permalink":"https://109383670.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://109383670.github.io/tags/爬虫/"}]},{"title":"egret与cocos creator","date":"2017-03-11T08:18:43.000Z","path":"2017/03/11/egret与cocos creator/","text":"花了点时间分别体验了两个html5游戏制作工具：egret与cocos2d creator。为了更好的对比，同时用两个工具做一个简单的射击demo。使用后的体会： egret （白鹭）优点 开发工具齐全，从龙骨、粒子系统、资源管理、编译器等，基本上你需要的都提供了，省去了四处去找第三方工具库的麻烦，这个非常好。 开发用的是TypeScript语言。在没有任何基础的情况下，我边看例子，边学习TypeScript, 没有什么大的障碍。调试功能也整合得很好。 用exml进行UI可视化，直观，上手容易。 提供云测试空间，可以直接发布免费提供的ft空间进行测试。想到真是太周到了，这是一条龙服务的节奏。 缺点： 教程文档混乱，调试一个问题花了几个小时没找到原因，结果发现是一个类文件没有放在src的文件夹下，这么重要的东西，竟然在文档中没任何提及。 exml这样的ui方式，对于初学者来说太过复杂了，刚开始学习的人，估计一头雾水，加上文档有混乱，入坑门槛比较高。 在物理系统，碰撞系统的支持上，比较弱，可能html5游戏应该也用不到这么复杂的功能。 总结：如果要开发html5游戏，之前用过cocos2d-x之类的，没有用过unity开发的，可以用egret。如果要进行大规模商业开发的，也建议用这个，各个功能比较完善，是成熟的产品。 coco2d creator优点： 文档、教程详细，写的非常好，基本上你想知道的全部有。从零开始到完成helloworld, 一套龙。教程友好，文档规范合理，用起来太舒服了。 用javascript进行开发，基于数据驱动的组件思想，入手快，开发直观。 对碰撞、地图的支持很好。 缺点： vscode的代码提示基本没用，得一个一个查文档。 调试功能差，估计调试错误花费的时间比较高。 有些功能还不完善。 总结：如果是之前用过unity开发的，那就太舒服了，基本上可以一边看文档一边做，没什么难度，上手快，用来做小游戏应该很爽。 对比总结：cocos2d creator就是模仿unity，打造一个轻型的html5制作工具。不得不说击中了unity在html5开发上的弱点。很看好cocos2d creator，相信在王哲的带领下，功能会越来越多，工具也会越来越完善。cocos2d creator虽然以cocos2d-x为底层，但是有意思的是，egret才是以代码为驱动，用代码控制一切，cocos2d creator以数据为驱动，用组件的方式，跟unity一致。egret像cocos2d-x, cocos2d creator像unity。两个工具各有千秋，一个是以代码驱动，一个以数据驱动。选择一种就是选择一种不同设计方法。 Demo地址Github代码点击预览游戏","tags":[{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"},{"name":"html5","slug":"html5","permalink":"https://109383670.github.io/tags/html5/"},{"name":"egret","slug":"egret","permalink":"https://109383670.github.io/tags/egret/"},{"name":"cccreator","slug":"cccreator","permalink":"https://109383670.github.io/tags/cccreator/"}]},{"title":"ios游戏分辨率问题","date":"2017-03-06T04:48:56.000Z","path":"2017/03/06/ios游戏分辨率问题/","text":"屏幕分辨率一般指的是屏幕上像素的多少。所谓像素就是屏幕上的最小发光点，Led灯屏幕的一个像素就是一个Led灯。例如：640*960指的就是屏幕的宽和高上分别有640和960个像素。分辨率越高，图像越精细，也就是常说的高清。 iPhones设备分辨率英寸 像素尺寸 尺寸表 游戏开发中用到的分辨率iPhone: iPad 为什么适配不同分辨率不同的设备有不同的分辨率，为了减少美术设计人员的工作量，统一化产品设计就必须适配各种分辨率。尽量做到一套设计，不同分辨率的设备都可以通用，不需要美术设计人员针对每一个分辨率版本都给出不同的设计方案，也便于维护升级。一套设计也便于减少游戏安装包大小，优化资源，提高游戏运行速度。 适配分辨率方案1、针对不同的分辨率，给出不同的设计。 优点：效果最好，因为针对每一个分辨率都做了专门的适配，不同的分辨率都能体现最好的设计效果。 缺点：工作量大，维护困难，每一次升级修改都需要针对每一个分辨率的版本进行更新，大大增加了工作时间和出错的可能性。不利于扩展，如果市场上出现了新的设备，不同的分辨率，又得更新版本升级。 2、 按实际屏幕大小进行缩放针对不同的分辨率，将游戏画面整个进行缩放，填充满整个屏幕。 优点：通用性高，工作量低。不管什么屏幕都是一套素材，一套代码，不需要额外的工作。 缺点：画面严重失真，因为是按照实际屏幕进行缩放，所以如果实际屏幕的宽高比与设计的宽高比不同的话，画面就会出现变形。整个画面看起来像是被压扁或是拉长。 如下图，变形了： 3、 按设计比例进行缩放针对不同的分辨率，按固定的宽高比进行缩放。 优点：最大程度的还原设计师的设计，可以做到一套设计通用，不会出现失真。 缺点：会在屏幕上下或者左右留下黑边，影响游戏体验。 如下图，有黑边，UI位置暴露了： 4、固定高度适配 在3号方案的基础上，对按钮等UI元素根据分辨率进行动态计算调整。 优点一套设计通用，不会出现2、3中的问题。 缺点：不能完全的还原设计师设计，要做出妥协。 实际开发中采用的方案实际开发中采用的方案是4号方案。4号方案能在保证画面不变形和出现黑边的情况下，最大程度的减少工作量。但是需要设计师巧妙的设计游戏背景图画。 以下图为例：正常的设计分辨率： ipad适配后的分辨率： 设计师设计比例根据游戏的主要用户和市场上手机的主要分辨率，决定设计师设计游戏UI时使用的分辨率。设计师只需要注意分辨率的宽高比，宽高比决定了屏幕上的布局。设计师作图时，应该根据宽高比，最大化画布的大小。比如：如果设计师的画布大小只有640x1136大小，当一旦需要1242x2208大小的图片时，设计师只能放大图片，这样就会导致图片质量下降。而如果设计师一开始的画布大小是2484x4416时，只需要将导出的图片缩小就可以了，不会过多的影响图片质量。 实际使用比例游戏的主要人群是iPhone用户，而市场上的主流设备是iphone5以上，所以采用的设备宽高比是0.562，也就是iphone6的宽高比。 计算背景图片需要大小：根据要适配的屏幕宽高比，主要有3种： iphone 6 : 0.562 iphone 4s : 0.667 ipad：0.75 假设高度为1，那么这3种分辨率中，宽度最大的是ipad的宽度，为0.75。那么设计师要设计的背景图片的宽高比根据最大宽度原则，采用0.75。 设计师如何工作说明图如下： 真实的分辨率：750x1334 背景图片大小：高度 = 1334宽度 = 1002。计算过程：1334x0.75 = 1000.5 。近似取偶数 = 1002最终大小：1002x1334设计师做图时，可以选择做一个2倍大的背景图。1002x2 = 2004、 1334x2 = 2668。 设计师设计步骤： 新建大小为 1500x2668的画布 安排按钮等UI布局 设计游戏背景图 将背景图单独拿出来，扩充为2004x2668大小的画布，将多出来的部分过渡好。 注意：设计师主要精力放在1500x2668这个画布上，主要的元素都要在这个画布上呈现。背景宽度扩充的部分只需要过渡好，让玩家看起来不突兀，自然就好。 作品：设计师需要提交1002x1334的背景图，其他的ui元素正常提交，没有变动。 参考 iPhone屏幕分辨率和适配规则（基础篇） iOS app屏幕快照规范","tags":[{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"},{"name":"ios","slug":"ios","permalink":"https://109383670.github.io/tags/ios/"}]},{"title":"Scrapy学习记录","date":"2017-02-27T05:19:50.000Z","path":"2017/02/27/Scrapy学习记录/","text":"Scrapy Shell 命令: 开始抓取网页: 1scrapy shell 'http://www.dytt8.net/index.htm' selector内容: 1response.xpath(\"//a/@href\").extract()[0] 输出jsonItem： 1scrapy crawl dmoz -o items.json xpath: following-sibling:除自身外后面的同辈兄弟。如：td/following-sibling::td 同级td兄弟。 xpath中的序列从1开始：/a[1],代表a的第一个元素。没有[0]。 遍历多个变量： 1for t , l in izip(response.xpath(strname), response.xpath(strurl)): r在Python的string前面加上‘r’， 是为了告诉编译器这个string是个raw string，不要转意backslash ‘\\’ 。 例如，\\n 在raw string中，是两个字符，\\和n， 而不会转意为换行符。由于正则表达式和 \\ 会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上’r’。 激活pipeline:在setting.py里，为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子: 123ITEM_PIPELINES = &#123; &apos;myproject.pipelines.PricePipeline&apos;: 300,&#125; 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。如： 1ITEM_PIPELINES = &#123;&apos;工程名.pipelines.自定义处理pipe类名&apos;: 1&#125; 使用相对XPaths:/或者//永远表示的是绝对路径，在嵌套xpath里，用’a/text()’这样的相对路径。 response.urljoin:方法建立绝对路径并且产生新的请求，并注册回调函数parse_dir_contents()来爬取需要的数据。 123456789101112def parse(self, response): for href in response.css(\"ul.directory.dir-col &gt; li &gt; a::attr('href')\"): url = response.urljoin(href.extract()) yield scrapy.Request(url, callback=self.parse_dir_contents) def parse_dir_contents(self, response): for sel in response.xpath('//ul/li'): item = DmozItem() item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item 递归抓取:123456789101112131415161718class Blurb2Spider(BaseSpider): name = \"blurb2\" allowed_domains = [\"www.domain.com\"] def start_requests(self): yield self.make_requests_from_url(\"http://www.domain.com/bookstore/new\") def parse(self, response): hxs = HtmlXPathSelector(response) urls = hxs.select('//div[@class=\"bookListingBookTitle\"]/a/@href').extract() for i in urls: yield Request(urlparse.urljoin('https://www.domain.com/', i[1:]),callback=self.parse_url) def parse_url(self, response): hxs = HtmlXPathSelector(response) print response,'-------&gt;' 相对地址:12import urljoinurlparse.urljoin(response.url, myurl) 定制图片管道的例子:下面是一个图片管道的完整例子，其方法如上所示: 12345678910111213141516import scrapyfrom scrapy.pipeline.images import ImagesPipelinefrom scrapy.exceptions import DropItemclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item 定位要详细://div[@id = “Zoom”]//img[1]/@srcdiv的定位要详细，如果是//div/span/img[1]/@src就返回为null,虽然firebug里面也没有问题。 Strip():Python strip() 方法用于移除字符串头尾指定的字符（默认为空格）。MapCompose(unicode.strip, unicode.title)) ，移除空格与换行例如: 1l.add_xpath('image_time', '//div[@class = \"co_content8\"]/ul/text()[1]', MapCompose(unicode.strip, unicode.title)) 下载图片:settings.py中有一行ROBOTSTXT_OBEY = True，需要改成False，否则可能下载不了图片。ROBOTSTXT_OBEY是否遵守robot协议，有些网站的robot.txt中表明，不允许爬去，这时候，如果要爬去的话，就要设置为false，不遵守。 No Moulde PIL Find:直接用pycharm自带的interpreter安装pillow mac下要注意python的安装路径参考： http://www.jianshu.com/p/078ad2067419http://www.cnblogs.com/kylinlin/p/5405246.htmlhttp://wiki.jikexueyuan.com/project/scrapy/item-pipeline.htmlhttp://www.open-open.com/lib/view/open1432868637316.html","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Learning","slug":"Learning","permalink":"https://109383670.github.io/tags/Learning/"}]},{"title":"Scrapy安装与运行记录","date":"2017-02-27T04:02:44.000Z","path":"2017/02/27/Scrapy安装与运行记录/","text":"安装Homebrewruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装pythonbrew install python Homebrew会自动安装好Setuptools和 pip 。Setuptools提供 easy_install 命令，实现通过网络（通常Internet）下载和安装第三方Python包。 还可以轻松地将这种网络安装的方式加入到自己开发的Python应用中。pip 是一款方便安装和管理Python 包的工具。 安装Scrapypip install scrapy Scrapy 使用: IDE工具：pycharm社区免费版本 教程参考: http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial 命令: 生成HelloWorld的Scrapy工程 scrapy startproject HelloWorld 在pycharm IDE中配置命令 原理： 执行: scrapy crawl MyPa (MyPa是自己在类中定义的爬虫名字)， 相当于在终端执行： /usr/local/bin/python /usr/local/lib/python2.7/site-packages/scrapy/cmdline.py crawl MyPa 注意：要小心python的路径，如果python的路径不对，还是会报错。这里指的路径是系统路径与pycharm里设置的python路径。 在终端里用which python查看一下路径,如果与pycharm设置里的不同，将修改成更系统路径一样的。 中文问题: shell里输出的是utf-8编码,用print可打印出中文。 用变量格式化的方式，不直接在xpath中用中文字符，而是用一个变量代替。如’中文’,用u’中文’。或者在字符串前加u。如u”//a/text()” 打印的时候可以参考： 1234for sel in response.xpath(\"//div[@id='mcontent']/div/p\"): conect = sel.xpath(\"text()\").extract() for t in conect: print(t.encode(\"utf-8\")) pycharm中支持中文 代码页加入: # -*-coding:utf-8-*- 代码: 1234strpath = u\"//td[descendant::a[contains(text(),'中文字符')]]\"。或者strz = '中文字符'strpath = u\"//td[descendant::a[contains(text(),%s)]]%strz\" json输出中文： 12345678910def __init__(self): self.file = codecs.open(\"items.json\", \"wb\", encoding=\"utf-8\") def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(line) return item def spider_closed(self, spider): self.file.close() ##读取文件with codecs.open(file_name, “r”,encoding=’utf-8’, errors=’ignore’) as fdata: ##decode encodedecode 总是返回unicode字符encode 总是接受一个unicode字符进行转换","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Setup","slug":"Setup","permalink":"https://109383670.github.io/tags/Setup/"}]},{"title":"hex+mac安装记录","date":"2017-02-22T12:06:47.000Z","path":"2017/02/22/hex+mac安装记录/","text":"有用的命令(hexo所在目录)： sudo hexo clean -清除 sudo hexo g -d 直接发布部署 sudo hexo g 生成 sudo hexo s 打开本地服务器 http://localhost:4000/ 浏览 本地预览步骤： sudo hexo g sudo hexo s http://localhost:4000/ 安装参考：参考域名绑定部分、修改主题各种出现的问题总结 hexo更新更新Hexo版本和Next主题hexo官网 具体安装时出现的问题：不能执行hexo命令只有init,help,version三个命令。解决方案：要在hexo目录下执行。 注意坑：执行hexo server时出错_config.xml里，type: repo: branch:后面，要有一个空格。 在DNS的配置里，加入固定的两个IP @ A 192.30.252.153@ A 192.30.252.154 不能连接git，提示22端口错误git网站中的ssh证书失效，要重新生成，参考帮助说明。ssh生成 git证书生成时，不能填写passphrase这个东西，自己回车跳过git@github.com: Permission denied每过一段时间不用，就会出现这个错误。经过测试发现，是因为更换了路由器造成的，可能ip的变化导致ssh密匙的拒绝。 其他：Next风格不错Next的设置GitWiki里很详细字体及字体大小修改","tags":[{"name":"hexo","slug":"hexo","permalink":"https://109383670.github.io/tags/hexo/"}]},{"title":"Hello World","date":"2017-02-22T04:41:28.000Z","path":"2017/02/22/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]