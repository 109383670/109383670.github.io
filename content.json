[{"title":"绘画名家：倪瓒","date":"2019-04-04T13:28:41.891Z","path":"2019/04/04/倪瓒/","text":"姓名倪瓒 长相 国籍元代。江浙行省无锡州（今江苏省无锡市）。 在世时间1301年－1374年 个人经历倪瓒（1301年—1374年），初名倪珽，字泰宇，别字元镇，号云林子、荆蛮民、幻霞子，江苏无锡人。元末明初画家、诗人，与黄公望、王蒙、吴镇合称“元四家”。倪瓒家中富有，博学好古，四方名士常至其门。元顺帝至正初年，散尽家财，浪迹太湖一带。倪瓒擅画山水和墨竹，师法董源，受赵孟頫影响。早年画风清润，晚年变法，平淡天真。疏林坡岸，幽秀旷逸，笔简意远，惜墨如金。以侧锋干笔作皴，名为“折带皴”。墨竹偃仰有姿，寥寥数笔，逸气横生。书法从隶书入，有晋人风度，亦擅诗文。洪武七年卒，时年七十四岁，存世作品有《渔庄秋霁图》《六君子图》《容膝斋图》等。著有《清閟阁集》。 有何特长超级洁癖。开创了水墨山水的一代画风。倪是古怪之人，有客来访时“见其言貌粗率，大怒，掌其颊”、倪瓒好饮茶，特制“清泉白石茶”[8]，赵行恕慕名而来，倪用此等好茶来招待他。赵行恕却觉得此茶不怎样。倪生气道：“吾以子为王孙，故出此品，乃略不知风味，真俗物也。”遂与之绝交。倪氏不仅善画，更穷治馔羞，其发明之“云林鹅”为清代袁枚所推崇，[9]至今已为无锡名菜之一。 作品展示《容膝斋图》 《水竹居图》 《六君子图》 《琪树秋风图》 《虞山林壑图》 《幽涧寒松图》 《秋亭嘉树图》 《雨后空林图》 参考链接wiki百度百科","tags":[{"name":"美术名家","slug":"美术名家","permalink":"https://109383670.github.io/tags/美术名家/"},{"name":"倪瓒","slug":"倪瓒","permalink":"https://109383670.github.io/tags/倪瓒/"}]},{"title":"绘画名家：波提切利","date":"2019-04-04T13:06:49.036Z","path":"2019/04/04/波提切利/","text":"姓名桑德罗·波提切利Sandro Botticelli 长相 国籍意大利佛罗伦萨 在世时间1445年3月1日－1510年5月17日 个人经历波提切利出生于意大利佛罗伦萨的一个中产阶级家庭，“波提切利”是他的绰号，意为“小桶”（意大利语：botte，桶）。从小酷爱绘画的他最早被做皮革匠的父亲送去学做一名金银艺匠学徒，但后遵从他本人的意愿，将他送到菲力浦·利比的画室学习绘画。利比以哥特式的手法，对三维立体事物的把握、对细微人物脸部表情的表现和对细节的重视都对波提切利日后的绘画风格造成了深远影响。此外，波拉约洛兄弟的雕塑作品也对波提切利产生过影响。之后他又从师韦罗基奥，曾与小他7岁的列奥纳多·达·芬奇是同学。1470年，他自立门户，开设个人绘画工作室，很快就受到美第奇家族的赏识，向他订购了大量的画作。与强大的美第奇家族保持着良好的关系也使画家获得政治上的保护，并享有有利的绘画条件。此外也是通过这一层关系，波提切利得以接触到佛罗伦萨上流社会和文艺界名流，开拓了视野，接触各方面多种的知识。 有何特长在美第奇家族掌权的期间，波提切利为他们做了多幅名画，声名大躁。1477年他以诗人波利蒂安歌颂爱神维纳斯的长诗为主题，为美第奇别墅所画《春》。这幅画已经和《维纳斯的诞生》一起，成为波提切利一生中最著名的两幅画作。在这幅画中，波提切利运用自己的想像力对古代神话故事重新演绎，人物线条流畅，色彩明亮灿烂，却又在充满着欢乐详和的气氛中，带有一丝忧愁。画面右上方是风神，他拥抱着春神，春神又拥着花神，被鲜花装点的花神向大地撒着鲜花；画面中间立着女神维纳斯，在她头顶处飞翔着手执爱情之箭的小爱神丘比特；维纳斯的右手边是三美神手拉手翩翩起舞，她们分别象征“华美”、“贞淑”和“欢悦”；画面的左下方是主神宙斯的特使墨丘利。1485年完成的《维纳斯的诞生》是波提切利的另一幅杰作，表现的是希腊神话中代表爱与美的女神维纳斯从大海中诞生的场景，这幅画的绘画风格在当时颇为与众不同，不强调明暗法来表现人体造型，而更强调轮廓线，使得人体有浅浮雕的感觉，而且极适合装饰作用。画面中的女神肌肤洁白，金色的长发飘逸，无愧为是完美的化身；但脸上却又挂有淡淡的忧愁、迷惘和困惑。另一幅为世人所熟知的画作是他的《三博士来朝》。这幅画为他在整个欧洲赢得了声誉，并也因此于1481年7月被教皇召唤到罗马，为西斯廷礼拜堂作壁画。1492年，佛罗伦萨发生政治巨变，美第奇家族遭驱逐，宗教极端的萨佛纳罗拉掌权，波提切利也是他的追随者之一，并曾在臭名昭著的“虚荣的篝火”中烧毁过多幅自己的画作，或许是因为这个原因，波提切利的后半生声名下滑，晚年贫困潦倒，只能靠救济度日，最终于1510年去世，安葬于佛罗伦萨“全体圣徒”教堂墓地。 作品展示《圣母颂》波提切利的圣母像非常著名，这件圣母像是他的代表作品。作品《圣母颂》描绘的是一群天使们正围绕着圣母，圣母怀抱着的小耶稣，其中两位天使分列两侧对称式举起金冠，顶部的圣灵金光洒射在人物的头上，另外的天使手捧墨水瓶和圣经，由着圣母蘸水书写，从空隙处远望可以看到一片金色平静的田野。 人物形象充满着波提切利特有的“妩媚”神态。整个画面没有欢乐，只有庄重、严肃和哀怨，这预示着耶稣未来的悲惨命运。现在这幅《圣母颂》藏于意大利佛罗伦萨乌菲齐美术馆。 《三博士来朝》三博士来朝是意大利画家桑德罗·波提切利创作于1475年－1476年间的名画。画面显示《圣经》中东方三博士朝拜耶稣基督的故事。本作品是为了圣新玛利亚教堂绘制的圣餐台油画。现藏于意大利佛罗伦萨的乌菲兹美术馆。画中画家非常巧妙的展现了自己以及其主要资助人美第奇家族的主要人物，根据乔尔乔·瓦萨里解释，画面上那个接触圣婴的脚的年老贤士，正是被誉为佛罗伦萨国父的科西莫·德·美第奇（即老科西莫）；穿白色袍子跪着的人是老科西莫的孙子朱利亚诺·德·美第奇（也是“豪华者”洛伦佐·德·美第奇的兄弟）；在他后面对那个孩子表现出感激崇拜的人，是老科西莫的次子乔瓦尼·德·美第奇；在他膝下画面中央前景出的人被认为是老柯西莫的长子皮耶罗一世·德·美第奇（即朱利亚诺·德·美第奇与洛伦佐·德·美第奇之父）；穿着黑底肩上有红色条纹长袍者可能是理想化了的洛伦佐·伊·玛尼菲科。最右面看向观者的黄衣青年正是画家本人。 《春》1482年他以诗人波利蒂安歌颂爱神维纳斯的长诗为主题，为美第奇别墅所画。这幅画和《维纳斯的诞生》一起，成为波提切利一生中最著名的两幅画作。现藏于意大利佛罗伦萨的乌菲兹美术馆。 《维纳斯与战神》纳斯与战神（意大利语：Venere e Marte；英语：Venus and Mars）是桑德罗·波提切利约于1483年绘成的作品，现藏于英国伦敦的国家美术馆。爱神维纳斯和战神玛尔斯躺在一起。维纳斯穿着衣服，胸前挂着连接头发的饰品。端坐，两眼有神地看着玛尔斯。而玛尔斯则是半裸地睡着了。边上四个半羊人小鬼（法翁，faun）或者是半兽人（萨堤尔，satyr）在一旁戏弄熟睡的玛尔斯，穿着他的铠甲拿着他的武器，并且在他的耳边吹号角（象征战斗）。玛尔斯睡得像块木头，环绕背景的是一片爱神树林（Myrtle Trees）。整个画面构图平稳，宁静。象征爱的力量征服了战争。 《维纳斯的诞生》《维纳斯的诞生》是意大利文艺复兴时期画家桑德罗·波提切利最著名的作品之一，这件作品根据波利齐安诺的长诗吉奥斯特纳而作，描述罗马神话中女神维纳斯从海中诞生的情景：她赤裸著身子踩在一个贝壳之上，右边春之女神正在为她披上华服而左边的风神送来暖风阵阵，吹起她的发丝。《维纳斯的诞生》目前存放在佛罗伦萨的乌菲兹美术馆中。在早期的文艺复兴，大约由这幅画开始，作画题材由圣经故事改为希腊（罗马）神话，即由宗教变成异教题材。人物比例不对，颈部较长，下半身较大，肩膀也是窄小下塌，正是为了使她的身体线条更加优美而忽视了应有的正常形态，画家重视感觉胜于比例。画中有不少光暗，使人物穿的衣物有了柔软、轻薄的感觉。 《年轻女子肖像》15世纪70年代的佛罗伦萨，曾有一位绝色倾城而又红颜薄命的女子，名叫西莫内塔。她来自港口城市热那亚，嫁到韦斯普奇家之后来到了佛罗伦萨。她的丈夫马尔科与美第奇家族关系十分密切，她自己也陷入到了与美第奇二当家朱利亚诺的一段绯闻中。1475年，在一次骑士比武大赛上，朱利亚诺举着画有西莫内塔肖像的旗帜进入了赛场，使她的美貌惊动了全城。而那幅肖像，就是出自波提切利之手。然而仅仅过了一年，23岁的西莫内塔就死于肺病。香消玉殒之后，她迅速被人们所遗忘，丈夫马尔科也很快娶了另一位女子。但世界上却还有一个人还对她念念不忘，那就是曾为她画像的波提切利。作品《春》当中那个神情落寞的维纳斯，就是他脑海中西莫内塔的身影。对美好时光太过短暂的哀叹，也正是作品中隐隐透露的那一丝忧伤。不久后波提切利创作的一幅《年轻女子肖像》，被认为是出自他脑海中对西莫内塔的记忆。 《地狱图》 《雅典娜与半人马仙托》取材于希腊神话故事：宙斯劫夺欧罗巴来到克里特岛生了两个儿子，一个叫弥诺斯，成了克里特王，他娶了帕西淮为妻，不安分的帕西淮和一头公牛偷情生下一个人牛各半的怪物肯陶洛斯，后被关进迷宫以避丑闻，弥洛斯令战败国雅典每七年要向克里特进贡七对童男女供肯陶洛斯吃掉，这幅画就是描绘帕拉斯捉拿肯陶洛斯情景。文艺复兴时期的画家尚不能从神灵控制下摆脱出来，因此只能借用神话宗教题材来描绘自己对现实生活的认识和理解，借以传达自己对世界、人生的看法和态度。这幅画是对惩恶扬善的赞颂。画中既发挥了写实造型功夫又具装饰性，画家以线造型为主，同时介入光暗法，人物形象苗条修长优美，衣着风动飘逸，繁复的裙纹形成线条的流动节奏变化美感，贴身的衣褶紧裹着的身体已显示出妩媚多姿的人体美。画家重形式结构美，忽视对人物之间内在精神联系的刻画，因此十分完美而不太动人。 参考链接wiki","tags":[{"name":"美术名家","slug":"美术名家","permalink":"https://109383670.github.io/tags/美术名家/"},{"name":"波提切利","slug":"波提切利","permalink":"https://109383670.github.io/tags/波提切利/"}]},{"title":"绘画名家：席里柯","date":"2019-04-04T07:43:50.073Z","path":"2019/04/04/席里柯/","text":"姓名泰奥多尔·席里柯Théodore Géricault 长相 国籍::法国鲁昂:: 在世时间::1792-1824:: 个人经历他出生于法国鲁昂，曾师从卡勒·韦尔内及皮埃尔·纳西斯·格林学画。但不久就离开教室前往卢浮宫自己揣摩各名家作品，1810年至15年间曾临摹过提香、鲁本斯、伦勃朗和委拉斯凯玆的作品，发展出了他认为不同于新古典主义的艺术风格[1]。后来又曾在凡尔赛学习解剖学、画马。在1821年回到法国后，他描绘了五幅精神偏执狂人的肖像，探究人类在异常状态下的精神表现，这几幅作品的题材在绘画史上非常罕见，对后期现实主义画派的出现有很大的影响。由于意外落马事故和慢性结核感染病发，在经历了长时间的痛苦后，于1824年在巴黎去世。 有何特长::画精神病的画家。::杰利柯善于描绘体育运动题材，早期描绘大型巴洛克风格的巨幅画，创作了一批军事题材的绘画，构图有强烈的运动感。早期代表作为《一个轻骑兵》，描绘了骑士和马的运动感。1816年访问意大利，到了佛罗伦萨和罗马，甚为欣赏米开朗基罗的风格。1818年他创作了著名的《梅杜萨之筏》，描绘了梅杜萨号船遇难后，残存的人们在木筏上漂流多日，遇到远方船只后呼救的情景。这幅画以它三角型的构图和丰富的色彩，表达了强烈的震撼力，成为浪漫主义画派的开山作品。 作品展示《梅杜萨之筏》画作《梅杜萨之筏》是画家席里柯创作于1819年的一幅油画。现由巴黎卢浮宫收藏。“梅杜萨号事件”当时在法国轰动一时。梅杜萨号巡洋舰载着四百多位官兵开往非洲，在西非海岸不慎搁浅，船体陷入沙里不能自拔。船长是位对航海知识一窍不通的贵族，经过两天混乱而无效的努力后，他便带着一部分官员乘救生艇逃之夭夭，扔下150多位官兵困在茫茫大海上。遇难的人们靠一个临时拼起的木筏，在汪洋大海上漂流了十几天，筏上发生了一幕幕骇人听闻的惨剧。人们在惊涛骇浪和饥寒交迫中同死神搏斗，为了活下去，有的人被迫啃吃死人的骨肉，有的人精神失常，尸体在海水的浸泡中开始腐烂。经过十几天的漂流，筏上最后只剩下15人。当幸存者向报界披露了这一丑闻后，引起了社会的极大震动。26岁的席里柯决定将这场悲剧搬上画布。为使油画作品《梅杜萨之筏》有真实的细节，画家席里柯到存尸所研究尸体，造了一个同样的木筏，又仔细观察了处于各种非常情况下人类的种种表情，还去精神病院画病人，同那儿的医生交朋友。经过一年半的努力，席里柯终于完成了这幅7米长的油画巨作。画家席里柯选取了绝望的幸存者突然发现天边一线船影拼死呼救的情节。画的高潮是几个青年挣扎使出最后的力量将一位黑人青年举起，他手中拼命挥动着求救的红巾。有的人对这突如其来的转机或将信将疑，或又惊又喜。一位坐在死去儿子身边的老人已完全陷入绝望之中，对这激动人心的时刻竟置若罔闻，木筏边还漂浮着腐烂变色的尸体。画上的细节如此真实生动，不能不使观者为之动情。作品采用了不稳定的斜线构图，整幅画没有过多的悲惨景象，但却笼罩着绝处逢生的骚乱和激动，非常富有戏剧性。画家在构成上也十分考究，从画幅前坐着躺着的人一直推向手举红巾呼救的青年，从静到动逐步形成一个高潮，让观者的视线最后集中于呼救这个中心。后来人们把这件充满激情的巨幅绘画视为浪漫主义的伟大宣言。油画《梅杜萨之筏》的视角表现出木筏的极不稳定性。两条对角线是整个构图的框架：一条线把观众的视线引向画面左侧正向木筏扑来的大浪，另一条线把观众的视线引向地平线上几乎看不见的救援船的微小侧影。油画《梅杜萨之筏》中人物表现出各种心理状态：怀抱死去儿子的男人的沮丧和迷茫、垂死者的突然振奋和向救援船挥手的人们的强烈渴望。但就在此时，谁也不知道这可怕的天平会向哪边倾斜。在这震撼人心的故事里，人类是唯一的主人公，至今仍令人感动。 《轻骑兵军官的冲锋》轻骑兵军官在冲锋现存于巴黎卢浮宫。年仅21岁的席里柯创作出题为《轻骑兵军官的冲锋》精彩的骑马人物像，这幅画使席里柯第一次入选1812年的沙龙画展。席里柯对绘画主题的选择极为巧妙，他强调以后腿站立的战马，希望给人留下强而有力的印象，而绘画中因胜利昂然自得的气氛也捕捉到了时代的气息。当时是拿破仑在莫斯科遭到惨败并开始撤退之前的几个月，拿破仑还处于气势如虹的状态之时，他在沙龙上展出了这幅作品。不过，这幅画不完全符合沙龙所承认的任何领域，因此，它被分类到肖像画。但总体来说，艺评家对这幅画的评价很高，席里柯获得了金奖。 《艾普松赛马》《艾普松赛马》这幅画现藏于巴黎卢浮宫。席里柯是一个热情洋溢、富于幻想的画家，他喜欢米开朗基罗和鲁本斯的画风，更喜欢同时代英国风景画家的色彩。1816年他曾去过意大利，1820年又前往英国，与康斯太博和波宁顿的结识使他成了第一位受英国绘画色彩影响的法国画家。1821年，他根据在英国艾普松郊外参加赛马会的真实感受创作了《艾普松赛马》。 《疯女人》","tags":[{"name":"美术名家","slug":"美术名家","permalink":"https://109383670.github.io/tags/美术名家/"},{"name":"席里柯","slug":"席里柯","permalink":"https://109383670.github.io/tags/席里柯/"}]},{"title":"绘画名家：博斯","date":"2019-04-04T06:57:26.200Z","path":"2019/04/04/博斯/","text":"姓名希罗尼穆斯·博斯 (Hieronymus Bosch)。 长相 国籍::荷兰:: 在世时间::1450-1516:: 个人经历出生在艺术世家，祖父和父亲都是地方有名的画家，博斯本人也声名显赫。他多数的画作多在描绘罪恶与人类道德的沉沦。博斯以恶魔、半人半兽甚至是机械的形象来表现人的邪恶。他的图画复杂，有高度的原创性、想像力，并大量使用各式的象征与符号，其中有些甚至在他的时代中也非常晦涩难解。博斯被认为是20世纪的超现实主义的启发者之一。他的真名是耶罗尼米斯（或“耶罗恩”）·范·阿肯（Jheronimus（或Jeroen） van Aken），意思是“亚琛来的人”。他在一部分画作上署名Bosch（荷兰文，音近英文Boss），取自他的出生地斯海尔托亨博斯。在西班牙文中他则多被称为El Bosco。博斯出生于绘画世家，他的双亲分别是荷兰与德国人。他大部分的人生都在斯海尔托亨博斯渡过，这是十五世纪当时布拉班特（今荷兰南部）一个热闹的城市。1463年时，约13岁的他可能曾目睹在当地发生的严重火灾。不久之后他成为知名的画家，甚至曾接到海外的委托。1488年他加入了圣母兄弟会，一个极端保守的信仰组织，由40位斯海尔托亨博斯当地有权势的市民，以及欧洲各地7000多名的会员组成。 有何特长他富有想象力的画作充满了荒唐的形式和怪异的象征主义。在被忽视了几个世纪后，他的原创性、讽刺的运用、及技法在今日已得到推崇。博斯晚期的作品无论是构图和造型，还是色彩和笔触都具有新的创见和高深的造诣。博斯在美术史上历来被认为是个不可思议的画家，他那充满着奇思怪想的画面像迷一样难解，其实他所创造的艺术形象并非凭空臆造，而是为了表现自己强烈 的反封建思想。他从传统的哥特式雕塑中、中世纪动物故事插图、色彩抄本和中世纪的宝石古钱币中吸收有意思的形象，同时还借用占星术来间接表达自己的思想。他的艺术创造和影响是超越时空的，他被誉为-现代绘画的始祖。 作品展示《圣安东尼的诱惑》博斯的代表作品。圣安东尼是一位虔诚的基督教徒，在父母去世后，他将财产尽数分给穷人，自己隐居墓地，苦苦修行。其时经历了魔鬼的种种诱惑，从未动摇过他的坚定信念。画家画了满幅离奇古怪的各种动物、人物、半人半兽的怪物，借以影射天主教会、教士的虚伪。圣安东尼跪在平台上举着一碗清水，而周围都沉浸在花天酒地的寻欢作乐中，在平台右下角，那个长着狐狸头、老鼠脸、长鼻上架着一付眼镜的伪君子，假正经地在阅读圣经；屋顶上那个教士正和一个女人饮酒作乐，旁边立着一位裸女。圣安东尼提倡人应绝欲，可是他周围的人却在拼命地追求各种欲念。这些都表现出教会的虚伪、可耻、可笑。 《愚者之船》15世纪尼德兰的人文主义学者们，已经拿起笔揭露教会的腐败，曾创作有《 愚人船》和《愚蠢的颂赞》等讽刺性文学作品，对教士的贪婪淫荡、神学家的虚妄无知、封建统治者的不劳而获和愚蠢顽固进行尖锐的讽刺、揭露和抨击。画中的“愚者”象征社会上各种罪恶行径，他们同乘一条船，由一个傻子驾驶着开往所谓“愚人的天堂”。船上一群荒淫无道者只顾吃喝弹唱，却不知傻子驾驶的船行将覆灭的警告。 《干草车》希罗尼穆斯·波希是当时最有群众基础的一位画师。油画《干草车》也是波希的代表作之一。围绕这辆缓缓前进的干草车的是一些人兽混杂、荒诞怪异的艺术形象。这是一幅隐喻性与真实性相结合的寓言画。《干草车》这幅画是三叶祭坛画的中央一块，现藏西班牙马德里的普拉多美术馆。《干草车》来自尼德兰一句古老的佛兰芒谚语：“世界是一个干草垛：人人在上为所欲为。”在波希的这幅油画中有天使与魔鬼接吻的情节；有弹曼陀铃的浪子坐在修女膝头调情的情节；干草垛的顶上既有怪物奏乐，也有圣女祈祷。干草车被几头奇怪的生物拖曳着。车后跟随着兴高采烈的教皇、国王与众百姓。有些人已跑在前头，有的则在车轮间东奔西窜，有的已被碾死在车轮下。人兽夹杂，尊卑交错。长着马脚的鱼张着大嘴，后面又有一只大老鼠，它的背上掮着古怪的树枝；有人要搬梯子爬上草垛去。车前几个人因互相倾轧而攀登不成，于是相互斗殴。有的把对方打倒在地，使劲扼住对方喉咙，欲置之死地而后快，然而干草车依然缓缓前进。这些渺小的生物气势汹汹，但也忙忙碌碌。构成荒诞古怪的艺术氛围，这象征人世间的什么，就不言而喻了。天空是一片蔚蓝。在干草车上空有一黄色祥云，里面钻出一个瘦小的基督。在画家波希眼里，这个世界就是这样可悲可笑，到处是光怪陆离。我们欣赏波希的画作，很难逐一破译这些怪诞人物和生物的生活“密码”。不过，这类画在于意会，图解性说明似无全然必要，艺术与生活事实上也不是对等的。这里所必须注意到的倒是《干草车》这幅画的用途，它是祭坛画。祭坛画上描绘这些晦涩的民间谚语题材，不能不使人感到惊讶。难道教会能容忍这种讽刺艺术吗？ 《人间欢乐园》博斯制作了多幅三联画──绘在三片接合起来的木质屏风上的画作，其中最有名的是《人间乐园》（亦作《尘世乐园》）。这件三连画的左幅，描绘了乐园中的亚当与夏娃与众多奇妙的生物；中幅以大量裸身的人体、巨大的水果和鸟类描写人间的乐园；右幅则是地狱的情境，充斥着大量造型奇幻的狱卒，以各式怪异的酷刑逞罚罪人。三件画作合起时，观赏者可见上帝创造地球的灰色装饰画。这些画作有一层较粗糙的颜料表层，与传统弗拉芒风格，以平滑的表面修饰人为的不自然的手法大异其趣。到了晚年时，博斯的风格已有所转变，改以描绘大型、接近观赏者的人物为表现方式。代表作是《戴刺冠的基督》（Christ Crowned with Thorns）。博斯从未在画作上注明日期，也仅在部分作品上签名（某些签名则被认为并非本人）；总括来说，目前确认出自博斯之手的画作，仅有25幅。西班牙国王腓力二世在博斯死后收藏了他的大部分作品，因此目前西班牙马德里的普拉多美术馆收藏了博斯最多的作品，包括《人间乐园》。稍晚期的弗拉芒画家老彼得·布吕赫尔受博斯影响，其作品风格与博斯相当近似，如1562年之《死亡的胜利》。 《死亡与守财奴》 《最后的审判》 《七宗罪》 《戴荆冠的基督》希罗尼穆斯·波希的这件作品《戴荆冠的基督》描绘的是基督被罗马巡抚彼拉多判处死刑之后，看守他的兵丁们为了侮辱他，讽刺他是犹太王，又用荆棘做了一顶荆冠戴在他头上，“恭喜”他为王。油画中左边的看守正在往基督头上戴荆棘环，右边扶着基督的肩膀，一脸的得意之状。左下角的一个则是嬉皮笑脸，右下角的看守抓住基督的衣服扮下跪之状，引得旁边人的嬉笑。而基督则是一脸的安详。作者通过夸张的写法，把看守戏弄，侮辱基督的场景展现得淋漓尽致。充分展现出作者深厚功底以及对场景及人物表情的捕捉能力。 《治疗愚蠢》 《天堂与地狱》!{ 《人生之路》 《人之树》","tags":[{"name":"美术名家","slug":"美术名家","permalink":"https://109383670.github.io/tags/美术名家/"},{"name":"博斯","slug":"博斯","permalink":"https://109383670.github.io/tags/博斯/"}]},{"title":"种子批量转磁链","date":"2019-04-03T10:19:56.731Z","path":"2019/04/03/种子批量转磁链/","text":"Torrent历史该技术由美国的程序员布莱姆·科亨于2001年4月时发布，并于2001年7月2日时首次正式应用。 原理普通的HTTP／FTP下载使用TCP/IP协议，BitTorrent协议是架构于TCP/IP协议之上的一个P2P文件传输通信协议，处于TCP/IP结构的应用层。根据BitTorrent协议，文件发布者会根据要发布的文件生成提供一个.torrent文件，即种子文件，也简称为“种子”。 组成种子文件本质是文本文件，包括Tracker和文件信息两个部分：Tracker服务器保存所有正在下载文件的客户端的地址，有人新建连接时，会将地址反馈给新的连接。文件信息是用Bencode进行编码，是要下载的文件的索引。 Tracker信息* Tracker服务器地址 * Tracker服务器设置 文件信息 announce - tracker的URL info - 该条映射到一个字典，该字典的键将取决于共享的一个或多个文件 name - 建议保存到的文件和目录名称 piece length - 每个文件块的字节数。通常为256KB = 262144B pieces - 每个文件块的SHA-1的集成Hash。因为SHA-1会返回160-bit的Hash，所以pieces将会得到1个160-bit的整数倍的字符串。和一个length（相当于只有一个文件正在共享）或files（相当于当多个文件被共享）： length - 文件的大小（以字节为单位） files - 一个字典的列表（每个字典对应一个文件）与以下的键 path - 一个对应子目录名的字符串列表，最后一项是实际的文件名称 length - 文件的大小（以字节为单位） info-hash:每一个种子唯一的编码，由info字段的数据计算而成。 DHT网络DHT全称为分布式哈希表（Distributed Hash Table），是一种分布式存储方法。在不需要服务器的情况下，每个客户端负责一个小范围的路由，并负责存储一小部分数据，从而实现整个DHT网络的寻址和存储。 磁链历史这个标准的草稿出现于2002年，是为了对eDonkey2000的“ed2k:”和Freenet的“freenet:”两个URI格式进行“厂商与项目中立化”（vendor- and project-neutral generalization）而制定的。同时这个标准也尝试紧密地跟进IETF官方的URI标准。 原理特点： 分布式。 不依赖于ip地址 没有中心服务器 开源组成磁力链接由一组参数组成，参数间的顺序没有讲究，其格式与在HTTP链接末尾的查询字符串相同。最常见的参数是”xt”，是”exact topic”的缩写，通常是一个特定文件的内容散列函数值形成的URN。1magnet:?xt=urn:sha1:YNCKHTQCWBTRNJIV4WNAE52SJUQCZO5C 其值是Base32编码的文件的SHA-1散列。 基本描述1magnet:? xl = [字节大小]&amp; dn = [文件名（已编码URL）]&amp; xt = urn: tree: tiger: [ TTH hash（Base32）] 由参数来指定相关的内容： dn（显示名称）- 文件名 xl（绝对长度）- 文件字节数 xt（eXact Topic）- 包含文件散列函数值的URN。磁力链接的这部分最重要。用于寻找和验证包含着磁力链接中的文件。 as（可接受来源） - 在线文件的网络链接 xs（绝对资源）- P2P链接 kt（关键字）- 用于搜索的关键字 mt（文件列表）- 链接到一个包含磁力链接的元文件 (MAGMA - MAGnet MAnifest） tr（Tracker地址）- BT下载的Tracker URL开发工具和平台 计算提取info_hash关键：info字段的值，必须先解码，再对info字段的值编码，然后才计算hash。直接从文件中删除其他部分行不通。 jsbencode库1npm install bencode 如果装了nvm,则-g全局安装会装在.nvm文件夹下。否则，就装在用户目录的node_module下。 sha-1库1npm install js-sha1 实现代码输出提取的磁力链接存放在种子目录下名为magnets.txt的文件中。 使用1node t2m.js &lt;种子文件夹路径&gt; js代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869var fs = require(\"fs\");var bencode = require('bencode');var sha1 = require('js-sha1');var arguments = process.argv.splice(2); //获取命令行参数，第三个元素是带的参数outMagnets(getMagnets(arguments), arguments);// 获取磁链链接，arguments为文件夹路径function getMagnets(arguments) &#123; var fileslist = fs.readdirSync(arguments.toString()); var magnetlist = new Array(); for (var f in fileslist) &#123; var torrentfile = null; var filename = fileslist[f].toString(); if (filename.includes(\".torrent\")) &#123; var filepath = arguments.toString() + '/' + filename; var magnet = getInfoHash(filepath); if (magnet) &#123; magnetlist.push(magnet); &#125; //console.log(magnet); &#125; &#125; return magnetlist;&#125;// 将结果输出为txtfunction outMagnets(magnets, path) &#123; if (magnets.length &gt; 0) &#123; var writebuffer = Buffer.from(arrayToString(magnets, '\\n')); var savepath = path.toString() + '/' + \"magnets.txt\"; var writesteam = fs.createWriteStream(savepath); writesteam.write(writebuffer, 'utf-8'); writesteam.end(); writesteam.on('finish', function () &#123; console.log(\"写入完成\"); &#125;); writesteam.on('error', function (err) &#123; console.log(err); &#125;); &#125;&#125;// buffer只能输出字符，所以必须将字符数组转换为字符形式，seq为分隔符function arrayToString(arr, seq) &#123; var str_value = null; for (a of arr) &#123; var astr = a.toString(); if (str_value) &#123; str_value = str_value + seq + astr; &#125; else &#123; str_value = astr; &#125; &#125; return str_value;&#125;// 获取种子文件的info_hash值，有一个解密，再加密的过程function getInfoHash(torrentfile) &#123; var result = bencode.decode(fs.readFileSync(torrentfile)); if (result) &#123; var info = result['info']; //info 字典 var info_hash = sha1(bencode.encode(info)); var magnet = \"magnet:?xt=urn:btih:\" + info_hash.toString(); return magnet; &#125; else &#123; return null; &#125;&#125; pythonbencode库gittub安装：1pip3 install py3-bencode 实现代码123456789from bencode import bencode, bdecodefrom io import BytesIOimport hashlibobjTorrentFile = open(\"test.torrent\", \"rb\")decodedDict = bdecode(objTorrentFile.read())info_hash = hashlib.sha1(bencode(decodedDict[\"info\"])).hexdigest()print(info_hash) 在解码提取出info后，似乎还需要编码后，再求hash值 参考网址磁力链接wiki磁力链接转换为种子文件官方种子格式说明种子格式详解bencode详解python bencode可用bencode-gitthubbencode源码js-bencode 实现wiki-bencode","tags":[{"name":"javascript","slug":"javascript","permalink":"https://109383670.github.io/tags/javascript/"},{"name":"小工具","slug":"小工具","permalink":"https://109383670.github.io/tags/小工具/"}]},{"title":"iOS开发常见问题","date":"2019-03-09T06:09:01.636Z","path":"2019/03/09/iOS 开发常见问题/","text":"隐私政策问题从2018年10月3号起，所有新提交的App都必须提供隐私条例。官方链接可以利用网站在线生成，并且都提供在线页面。不过没有中文版本。部分条款要收费，不过free 版本的就足够用了。提供在线生成的网站：freeprivacypolicyprivacypolicies 视频预览问题对于竖屏的视频，mac自带的iMovies无法处理，必须用cut final pro来进行处理。导出视频的格式如下：App 预览规范屏幕快照规范","tags":[{"name":"iOS 开发","slug":"iOS-开发","permalink":"https://109383670.github.io/tags/iOS-开发/"}]},{"title":"cocos2d-x 3.0相对于2.0的变化","date":"2019-03-02T07:44:18.102Z","path":"2019/03/02/cocos2d-x 3.0相对于2.0的变化/","text":"更新步骤 下载开发包（github也可以)。 运行build目录下，cocos2d_tests.xcodeproj工程。 将编译Target选择成cpp-tests iOS。 运行。 用命令行命令生成一个模板工程：cocos new MyGame -p com.MyCompany.MyGame -l cpp -d ~/MyCompany将2.0代码加入模板工程中。如果是3.0升级就用模板工程中的cocos2d代替老版本的文件夹。 CClog 变成CCLOGCCArray 变成Array, CCSet等也变成setCCLayer中setTouchEnabled无效添加触摸事件器后，自动生效。 ccTouchBegan等改成onTouchesBegan触摸事件代码12345auto listener = EventListenerTouchAllAtOnce::create();listener-&gt;onTouchesBegan = CC_CALLBACK_2(ForceTouchTest::onTouchesBegan, this);listener-&gt;onTouchesMoved = CC_CALLBACK_2(ForceTouchTest::onTouchesMoved, this);listener-&gt;onTouchesEnded = CC_CALLBACK_2(ForceTouchTest::onTouchesEnded, this);_eventDispatcher-&gt;addEventListenerWithSceneGraphPriority(listener, this); CCObject改成RefCCPoint改成Vec2CCJumpTO等动作改成JumpToCCPointZero改成Vec2::Zero除了0向量外，还有很多的静态值。 动作回调CallFuncN有改动123456ac_go = Sequence::create(ac_p, ac_s, ac_s1,CallFuncN::create(this, callfuncN_selector(BlockBoard::call_onBeginAction)),nullptr);//改成：ac_go = Sequence::create(ac_p,ac_s,ac_s1,CallFuncN::create(CC_CALLBACK_1(BlockBoard::call_onBeginAction,this)),nullptr); CC_CALLBACK_1 CC_CALLBACK_2 CC_CALLBACK_3后面的数字表示带参数的多少。 ssize_t格式化参数%zdssize_t：有符号整数，与平台无关。适配不同平台的通用整数关键字，在32位平台为32，64位平台为long。size_t：无符号整数，与平台无关。ssize_t格式化参数是%zd, size_t是%tu。 GameCenter ReportScore有更新代码如下：123456789101112131415161718- (void) reportScore: (int64_t) score forCategory: (NSString*) category&#123; GKScore *scoreReporter = [[[GKScore alloc] initWithLeaderboardIdentifier:category] autorelease]; scoreReporter.value = score; scoreReporter.context = 0; NSArray *scores = @[scoreReporter]; [GKScore reportScores:scores withCompletionHandler:^(NSError *error) &#123; //Do something interesting here. if (error != nil)&#123; // handle the reporting error NSLog(@\"上传分数出错.\"); &#125;else &#123; NSLog(@\"上传分数成功\"); &#125; &#125;];&#125; Size会有重名错误，用cocos2d::SizeMenuItemImage 有变化代码1MenuItemImage* item_close = MenuItemImage::create(\"close.png\", \"close_p.png\", CC_CALLBACK_1(InfoShowLayer::onClose, this)); 向量操作Vec2可以直接数学符号操作 Label FontTTF, 系统字体，BMFont，都由Label类负责创建1234Label* label = Label::createWithSystemFont(str-&gt;getCString(), \"Mark Felt\", fontSize);Label::createWithTTFLabel::createWithBMFontLabel::createWithCharMap 设置资源路径已经适配模式12searchPath.push_back(\"iphone\");glview&gt;setDesignResolutionSize(750,1334,ResolutionPolicy::FIXED_HEIGHT); 启动图片和应用图标都有更新启动图片决定了初始分辨率的大小。不提供相应的启动图片，不能获得正确的初始分辨率。 参考常用格式化参数CC_CALLBACK_0, CC_CALLBACK_1, CC_CALLBACK_2, CC_CALLBACK_3","tags":[{"name":"cocos2d-x","slug":"cocos2d-x","permalink":"https://109383670.github.io/tags/cocos2d-x/"},{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"}]},{"title":"Scrapy爬虫项目纪录","date":"2019-02-19T16:46:54.381Z","path":"2019/02/20/Scrapy爬虫项目纪录/","text":"目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。 编程环境 VSCode Python3 Scrapy 安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL 提示没有安装win32api用pip 安装win32： 1pip install pywin32 安装命令1pip install scrapy 更新命令1sudo pip install --upgrade scrapy mac 下安装mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。mac下用Homebrew来进行升级 安装xcode命令行工具 1xcode-select --install https://brew.sh/ 安装Homebrew 将Homebrew加入环境变量中 12echo \"export PATH=/usr/local/bin:/usr/local/sbin:$PATH\" &gt;&gt; ~/.bashrcsource ~/.bashrc 安装python 1brew install python 如果已经安装，可以进行升级 1brew update; brew upgrade python 安装scrapy1pip3 install scrapy 学习记录生成Scrapy框架SCrapy必须在固定的框架下运行，可以自动生成后再去改动。1scrapy startproject 工程名 HelloWorld代码12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): # 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法 name = \"quotes\" # 唯一的爬虫名字，在运行时要用到 def start_requests(self): # 复写的方法，初始请求的网址 urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): # 复写的方法，在这里对爬下的数据进行处理 page = response.url.split(\"/\")[-2] filename = 'quotes-%s.html' % page with open(filename, 'wb') as f: f.write(response.body) self.log('Saved file %s' % filename) 运行命令： 1scrapy crawl quotes 深入学习例子1-提取内容123456789101112131415161718# 提取相关格言以及作者等信息import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), 'tags': quote.css('div.tags a.tag::text').getall(), &#125; 输出json或者jl(JSON Lines)命令123scrapy crawl quotes -o quotes.jsonscrapy crawl quotes -o quotes.jl 例子2-爬取下一个链接12345678910111213141516171819202122import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), 'tags': quote.css('div.tags a.tag::text').getall(), &#125; next_page = response.css('li.next a::attr(href)').get() if next_page is not None: next_page = response.urljoin(next_page) #获得真实的链接地址 yield scrapy.Request(next_page, callback=self.parse) #下一个链接的处理回调 后面两句可以用下面的代替，不用写urljoin了。 1yield response.follow(next_page, callback=self.parse) 进一步简化： 12for href in response.css('li.next a::attr(href)'): yield response.follow(href, callback=self.parse) 再进一步简化：对于a 标签，会自动使用它的href属性12for a in response.css('li.next a'): yield response.follow(a, callback=self.parse) 进阶例子123456789101112131415161718192021222324252627import scrapyclass AuthorSpider(scrapy.Spider): name = 'author' start_urls = ['http://quotes.toscrape.com/'] def parse(self, response): # follow links to author pages for href in response.css('.author + a::attr(href)'): yield response.follow(href, self.parse_author) # follow pagination links for href in response.css('li.next a::attr(href)'): yield response.follow(href, self.parse) def parse_author(self, response): def extract_with_css(query): return response.css(query).get(default='').strip() yield &#123; 'name': extract_with_css('h3.author-title::text'), 'birthdate': extract_with_css('.author-born-date::text'), 'bio': extract_with_css('.author-description::text'), &#125; 命令行参数例子123456789101112131415161718192021222324import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" def start_requests(self): url = 'http://quotes.toscrape.com/' tag = getattr(self, 'tag', None) #从命令行参数获得 if tag is not None: url = url + 'tag/' + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), &#125; next_page = response.css('li.next a::attr(href)').get() if next_page is not None: yield response.follow(next_page, self.parse) 命令 1scrapy crawl quotes -o quotes-humor.json -a tag=humor 结果 1http://quotes.toscrape.com/tag/humor item可以自己定义的数据结构格式如下1234567import scrapyclass Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) item pipeline处理item数据的地方，在parse中返回item,就会调用该方法。格式如下12345678910111213from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item.get('price'): if item.get('price_excludes_vat'): item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 1234567891011121314import jsonclass JsonWriterPipeline(object): def open_spider(self, spider): self.file = open('items.jl', 'w') def close_spider(self, spider): self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item 在setting里启动pipeline1234ITEM_PIPELINES = &#123; 'myproject.pipelines.PricePipeline': 300, #数字表示优先顺序，越小的越先执行 'myproject.pipelines.JsonWriterPipeline': 800,&#125; 例子：12345678910111213141516171819202122232425262728from mySpider.items import ItcastItemdef parse(self, response): #open(\"teacher.html\",\"wb\").write(response.body).close() # 存放老师信息的集合 #items = [] for each in response.xpath(\"//div[@class='li_txt']\"): # 将我们得到的数据封装到一个 `ItcastItem` 对象 item = ItcastItem() #extract()方法返回的都是unicode字符串 name = each.xpath(\"h3/text()\").extract() title = each.xpath(\"h4/text()\").extract() info = each.xpath(\"p/text()\").extract() #xpath返回的是包含一个元素的列表 item['name'] = name[0] item['title'] = title[0] item['info'] = info[0] #items.append(item) #将获取的数据交给pipelines yield item # 返回数据，不经过pipeline #return items 中文乱码转为utf-8python3默认为unicode,如果输出为中文，则要转为utf-8，不然会是乱码代码如下：123456789101112131415161718import jsonimport codecsimport osclass Pipeline(object): def __init__(self): self.file = codecs.open( 'items.json', 'w', encoding='utf-8') def close_spider(self, spider): self.file.seek(-1, os.SEEK_END) self.file.truncate() self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(line) return item imagepipeline各函数运行流程 imagepipeline启动 get_media_requests 将所有的下载请求一次全部完成 下载完成后再统一执行item_completed 同时下载多个图片并改名重写file_path函数实现12345678910111213141516171819202122232425262728293031 def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" #这里传递字符，或者图片列表，如果是单个的对象，则非常容易被覆盖 yield scrapy.Request(item['pic_url'], meta=&#123;'item': item['pic_name']&#125;) def file_path(self, request, response=None, info=None): \"\"\" : param request: 每一个图片下载管道请求 : param response: : param info: : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录 : return: 每套图的分类目录 \"\"\" item = request.meta['item'] folder = item folder_strip = strip(folder) # img_path = \"%s%s\" % (self.img_store, folder_strip) filename = folder_strip + '/' + folder_strip + '.jpg' return filename def strip(path): \"\"\" :param path: 需要清洗的文件夹名字 :return: 清洗掉Windows系统非法文件夹名字的字符串 \"\"\" path = re.sub(r'[？\\\\*|“&lt;&gt;:/]', '', str(path)) return path Request 回调传递参数1234scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;'item': item, 'param': name&#125;)在parse中提取参数item = response.meta['item'] 结果去重 Request的参数 dont_filter=False 默认去重 启用一个爬虫的持久化，运行以下命令:1scrapy crawl somespider -s JOBDIR=crawls/somespider-1 然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。恢复这个爬虫也是同样的命令:1scrapy crawl somespider -s JOBDIR=crawls/somespider-1 这样爬虫断掉后，再启动会接着上次的 url 跑。 如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数运行爬虫如：1scrapy crawl spider1 -L WARNING 不打印Debug信息，可以清楚得看到运行过程。 scrapy-red 错误记录pipeline is not a full path应该在 setting 中填入完整的管道的路径，如：1pic.pipelines.PicImagesDownloadPipeline 如果只填PicImagesDownloadPipeline,就会出现这个错误。 Symbol not found: _PyInt_AsLong 错误将系统python目录下的PIL和Pillow库都删除，再用pip3安装在 Python3的安装目录下系统python安装目录：1/Library/Python/2.7/site-packages Missing scheme in request url: h相关URL必须是一个List，所以遇到该错误只需要将url转换成list即可。例如：start_urls = [‘someurls’]如果是images_url也是如此，使用item存储的时候改成list即可。item[‘images_urls’] = [‘image_url’] Request url must be str or unicode请求的url参数不能是一个列表，必须是一个字符 在item_complete中改名多个图片不成功item_complete并不是在get_media_requests下载图片后马上启动的，它是要等所有的图片下载完成，再统一启动complete事件，这样就导致多个图片没法改名，不能获得之前的item的字段。改名需要重写file_path get_media_requests中回调参数要小心meta中可以加入回调的参数，如果传递的是对象要非常小心，如果对象发生变化，会导致后面所有的回调参数发生变化，传递的如果是字符，就没有这个风险。1234567def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" yield scrapy.Request(item['pic_url'], meta=&#123;'item': item['pic_name']&#125;) Filtered duplicate request有重复下载的请求，如果要重复下载，在Request函数里加上参数 dont_filter=True，默认是False 最终代码piczz.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import scrapyfrom piczz.items import PiczzItemclass piczzSpider(scrapy.Spider): name = \"piczz\" allowed_domains = [\"\"] start_urls = [\"\"] img_paths = [] def parse(self, response): for each in response.xpath( \"//div[@class = 'post_box']\"): # extract()方法返回的都是unicode字符串 item = PiczzItem() item['name'] = 'startpage' self.img_paths.clear() item['pic_name'] = each.xpath( \"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/text()\").extract()[0] item['pic_url'] = each.xpath( \"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/@href\").extract()[0] yield scrapy.Request(item['pic_url'], callback=self.parse_imgs, meta=&#123;'item': item&#125;) #递归下一页图片 next_path = response.xpath( \"descendant::div[@class = 'page_num']/a[last()]\") next_con = next_path.xpath(\"text()\").extract()[0] next_con = next_con.strip() next_page = \"\" if next_con == \"下一頁 »\": next_page = next_path.xpath(\"@href\").extract()[0] print(next_page) if next_path is not None: yield scrapy.Request(next_page, self.parse) else: return # 下载一个索引页的图片 def parse_imgs(self, response): self.img_paths.clear() item = response.meta['item'] imgs = response.xpath( \"descendant::div[@class = 'entry-content']/p/img/@src\").extract() for e in imgs: self.img_paths.append(e) item['pic_paths'] = self.img_paths next_path = response.xpath( \"descendant::div[@class = 'wp-pagenavi']/p/a[last()]\") next_con = next_path.xpath(\"text()\").extract()[0] next_con = next_con.strip() if next_con == \"下一页\": next_page = next_path.xpath(\"@href\").extract()[0] if next_page is not None: yield scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;'item': item&#125;) yield item item.py 12345678import scrapyclass PiczzItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pic_name = scrapy.Field() # 图片目录名 pic_url = scrapy.Field() # 图片索引首页地址 pic_paths = scrapy.Field() # 图片下载地址列表 pipeline.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import jsonimport shutilimport codecsimport osimport reimport scrapyimport PILfrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.utils.project import get_project_settingsclass PiczzImagesDownloadPipeline(ImagesPipeline): def get_media_requests(self, item, info): \"\"\" :param item: spider.py中返回的item :param info: :return: \"\"\" for img_url in item['pic_paths']: yield scrapy.Request(img_url, meta=&#123;'item': item['pic_name']&#125;) def file_path(self, request, response=None, info=None): \"\"\" : param request: 每一个图片下载管道请求 : param response: : param info: : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录 : return: 每套图的分类目录 \"\"\" item = request.meta['item'] folder = item folder_strip = strip(folder) image_guid = request.url.split('/')[-1] filename = folder_strip + '/' + image_guid + '.jpg' return filename def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") return itemdef strip(path): \"\"\" :param path: 需要清洗的文件夹名字 :return: 清洗掉Windows系统非法文件夹名字的字符串 \"\"\" path = re.sub(r'[？\\\\*|“&lt;&gt;:/]', '', str(path)) return path 总结从搭建环境到断断续续的学习花了大概五天时间 ，每天平均花二个小时学习，终于成功的将设定的目标完成。 参考网站官网中文参考网站xPath语法Python中yield的解释mac os Python路径总结Scrapy框架入门简介ImagesPipeline下载图片ImagesPipeline下载图片保持原文件名小白进阶之Scrapy第四篇Python中yield的解释scrapy调用parse()中使用yield引发对yield的分析","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Python","slug":"Python","permalink":"https://109383670.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://109383670.github.io/tags/爬虫/"}]},{"title":"egret与cocos creator","date":"2017-03-11T08:18:43.000Z","path":"2017/03/11/egret与cocos creator/","text":"花了点时间分别体验了两个html5游戏制作工具：egret与cocos2d creator。为了更好的对比，同时用两个工具做一个简单的射击demo。使用后的体会： egret （白鹭）优点 开发工具齐全，从龙骨、粒子系统、资源管理、编译器等，基本上你需要的都提供了，省去了四处去找第三方工具库的麻烦，这个非常好。 开发用的是TypeScript语言。在没有任何基础的情况下，我边看例子，边学习TypeScript, 没有什么大的障碍。调试功能也整合得很好。 用exml进行UI可视化，直观，上手容易。 提供云测试空间，可以直接发布免费提供的ft空间进行测试。想到真是太周到了，这是一条龙服务的节奏。 缺点： 教程文档混乱，调试一个问题花了几个小时没找到原因，结果发现是一个类文件没有放在src的文件夹下，这么重要的东西，竟然在文档中没任何提及。 exml这样的ui方式，对于初学者来说太过复杂了，刚开始学习的人，估计一头雾水，加上文档有混乱，入坑门槛比较高。 在物理系统，碰撞系统的支持上，比较弱，可能html5游戏应该也用不到这么复杂的功能。 总结：如果要开发html5游戏，之前用过cocos2d-x之类的，没有用过unity开发的，可以用egret。如果要进行大规模商业开发的，也建议用这个，各个功能比较完善，是成熟的产品。 coco2d creator优点： 文档、教程详细，写的非常好，基本上你想知道的全部有。从零开始到完成helloworld, 一套龙。教程友好，文档规范合理，用起来太舒服了。 用javascript进行开发，基于数据驱动的组件思想，入手快，开发直观。 对碰撞、地图的支持很好。 缺点： vscode的代码提示基本没用，得一个一个查文档。 调试功能差，估计调试错误花费的时间比较高。 有些功能还不完善。 总结：如果是之前用过unity开发的，那就太舒服了，基本上可以一边看文档一边做，没什么难度，上手快，用来做小游戏应该很爽。 对比总结：cocos2d creator就是模仿unity，打造一个轻型的html5制作工具。不得不说击中了unity在html5开发上的弱点。很看好cocos2d creator，相信在王哲的带领下，功能会越来越多，工具也会越来越完善。cocos2d creator虽然以cocos2d-x为底层，但是有意思的是，egret才是以代码为驱动，用代码控制一切，cocos2d creator以数据为驱动，用组件的方式，跟unity一致。egret像cocos2d-x, cocos2d creator像unity。两个工具各有千秋，一个是以代码驱动，一个以数据驱动。选择一种就是选择一种不同设计方法。 Demo地址Github代码点击预览游戏","tags":[{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"},{"name":"html5","slug":"html5","permalink":"https://109383670.github.io/tags/html5/"},{"name":"egret","slug":"egret","permalink":"https://109383670.github.io/tags/egret/"},{"name":"cccreator","slug":"cccreator","permalink":"https://109383670.github.io/tags/cccreator/"}]},{"title":"ios游戏分辨率问题","date":"2017-03-06T04:48:56.000Z","path":"2017/03/06/ios游戏分辨率问题/","text":"屏幕分辨率一般指的是屏幕上像素的多少。所谓像素就是屏幕上的最小发光点，Led灯屏幕的一个像素就是一个Led灯。例如：640*960指的就是屏幕的宽和高上分别有640和960个像素。分辨率越高，图像越精细，也就是常说的高清。 iPhones设备分辨率英寸 像素尺寸 尺寸表 游戏开发中用到的分辨率iPhone: iPad 为什么适配不同分辨率不同的设备有不同的分辨率，为了减少美术设计人员的工作量，统一化产品设计就必须适配各种分辨率。尽量做到一套设计，不同分辨率的设备都可以通用，不需要美术设计人员针对每一个分辨率版本都给出不同的设计方案，也便于维护升级。一套设计也便于减少游戏安装包大小，优化资源，提高游戏运行速度。 适配分辨率方案1、针对不同的分辨率，给出不同的设计。 优点：效果最好，因为针对每一个分辨率都做了专门的适配，不同的分辨率都能体现最好的设计效果。 缺点：工作量大，维护困难，每一次升级修改都需要针对每一个分辨率的版本进行更新，大大增加了工作时间和出错的可能性。不利于扩展，如果市场上出现了新的设备，不同的分辨率，又得更新版本升级。 2、 按实际屏幕大小进行缩放针对不同的分辨率，将游戏画面整个进行缩放，填充满整个屏幕。 优点：通用性高，工作量低。不管什么屏幕都是一套素材，一套代码，不需要额外的工作。 缺点：画面严重失真，因为是按照实际屏幕进行缩放，所以如果实际屏幕的宽高比与设计的宽高比不同的话，画面就会出现变形。整个画面看起来像是被压扁或是拉长。 如下图，变形了： 3、 按设计比例进行缩放针对不同的分辨率，按固定的宽高比进行缩放。 优点：最大程度的还原设计师的设计，可以做到一套设计通用，不会出现失真。 缺点：会在屏幕上下或者左右留下黑边，影响游戏体验。 如下图，有黑边，UI位置暴露了： 4、固定高度适配 在3号方案的基础上，对按钮等UI元素根据分辨率进行动态计算调整。 优点一套设计通用，不会出现2、3中的问题。 缺点：不能完全的还原设计师设计，要做出妥协。 实际开发中采用的方案实际开发中采用的方案是4号方案。4号方案能在保证画面不变形和出现黑边的情况下，最大程度的减少工作量。但是需要设计师巧妙的设计游戏背景图画。 以下图为例：正常的设计分辨率： ipad适配后的分辨率： 设计师设计比例根据游戏的主要用户和市场上手机的主要分辨率，决定设计师设计游戏UI时使用的分辨率。设计师只需要注意分辨率的宽高比，宽高比决定了屏幕上的布局。设计师作图时，应该根据宽高比，最大化画布的大小。比如：如果设计师的画布大小只有640x1136大小，当一旦需要1242x2208大小的图片时，设计师只能放大图片，这样就会导致图片质量下降。而如果设计师一开始的画布大小是2484x4416时，只需要将导出的图片缩小就可以了，不会过多的影响图片质量。 实际使用比例游戏的主要人群是iPhone用户，而市场上的主流设备是iphone5以上，所以采用的设备宽高比是0.562，也就是iphone6的宽高比。 计算背景图片需要大小：根据要适配的屏幕宽高比，主要有3种： iphone 6 : 0.562 iphone 4s : 0.667 ipad：0.75 假设高度为1，那么这3种分辨率中，宽度最大的是ipad的宽度，为0.75。那么设计师要设计的背景图片的宽高比根据最大宽度原则，采用0.75。 设计师如何工作说明图如下： 真实的分辨率：750x1334 背景图片大小：高度 = 1334宽度 = 1002。计算过程：1334x0.75 = 1000.5 。近似取偶数 = 1002最终大小：1002x1334设计师做图时，可以选择做一个2倍大的背景图。1002x2 = 2004、 1334x2 = 2668。 设计师设计步骤： 新建大小为 1500x2668的画布 安排按钮等UI布局 设计游戏背景图 将背景图单独拿出来，扩充为2004x2668大小的画布，将多出来的部分过渡好。 注意：设计师主要精力放在1500x2668这个画布上，主要的元素都要在这个画布上呈现。背景宽度扩充的部分只需要过渡好，让玩家看起来不突兀，自然就好。 作品：设计师需要提交1002x1334的背景图，其他的ui元素正常提交，没有变动。 参考 iPhone屏幕分辨率和适配规则（基础篇） iOS app屏幕快照规范","tags":[{"name":"游戏开发","slug":"游戏开发","permalink":"https://109383670.github.io/tags/游戏开发/"},{"name":"ios","slug":"ios","permalink":"https://109383670.github.io/tags/ios/"}]},{"title":"Scrapy学习记录","date":"2017-02-27T05:19:50.000Z","path":"2017/02/27/Scrapy学习记录/","text":"Scrapy Shell 命令: 开始抓取网页: 1scrapy shell 'http://www.dytt8.net/index.htm' selector内容: 1response.xpath(\"//a/@href\").extract()[0] 输出jsonItem： 1scrapy crawl dmoz -o items.json xpath: following-sibling:除自身外后面的同辈兄弟。如：td/following-sibling::td 同级td兄弟。 xpath中的序列从1开始：/a[1],代表a的第一个元素。没有[0]。 遍历多个变量： 1for t , l in izip(response.xpath(strname), response.xpath(strurl)): r在Python的string前面加上‘r’， 是为了告诉编译器这个string是个raw string，不要转意backslash ‘\\’ 。 例如，\\n 在raw string中，是两个字符，\\和n， 而不会转意为换行符。由于正则表达式和 \\ 会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上’r’。 激活pipeline:在setting.py里，为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子: 123ITEM_PIPELINES = &#123; &apos;myproject.pipelines.PricePipeline&apos;: 300,&#125; 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。如： 1ITEM_PIPELINES = &#123;&apos;工程名.pipelines.自定义处理pipe类名&apos;: 1&#125; 使用相对XPaths:/或者//永远表示的是绝对路径，在嵌套xpath里，用’a/text()’这样的相对路径。 response.urljoin:方法建立绝对路径并且产生新的请求，并注册回调函数parse_dir_contents()来爬取需要的数据。 123456789101112def parse(self, response): for href in response.css(\"ul.directory.dir-col &gt; li &gt; a::attr('href')\"): url = response.urljoin(href.extract()) yield scrapy.Request(url, callback=self.parse_dir_contents) def parse_dir_contents(self, response): for sel in response.xpath('//ul/li'): item = DmozItem() item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item 递归抓取:123456789101112131415161718class Blurb2Spider(BaseSpider): name = \"blurb2\" allowed_domains = [\"www.domain.com\"] def start_requests(self): yield self.make_requests_from_url(\"http://www.domain.com/bookstore/new\") def parse(self, response): hxs = HtmlXPathSelector(response) urls = hxs.select('//div[@class=\"bookListingBookTitle\"]/a/@href').extract() for i in urls: yield Request(urlparse.urljoin('https://www.domain.com/', i[1:]),callback=self.parse_url) def parse_url(self, response): hxs = HtmlXPathSelector(response) print response,'-------&gt;' 相对地址:12import urljoinurlparse.urljoin(response.url, myurl) 定制图片管道的例子:下面是一个图片管道的完整例子，其方法如上所示: 12345678910111213141516import scrapyfrom scrapy.pipeline.images import ImagesPipelinefrom scrapy.exceptions import DropItemclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item 定位要详细://div[@id = “Zoom”]//img[1]/@srcdiv的定位要详细，如果是//div/span/img[1]/@src就返回为null,虽然firebug里面也没有问题。 Strip():Python strip() 方法用于移除字符串头尾指定的字符（默认为空格）。MapCompose(unicode.strip, unicode.title)) ，移除空格与换行例如: 1l.add_xpath('image_time', '//div[@class = \"co_content8\"]/ul/text()[1]', MapCompose(unicode.strip, unicode.title)) 下载图片:settings.py中有一行ROBOTSTXT_OBEY = True，需要改成False，否则可能下载不了图片。ROBOTSTXT_OBEY是否遵守robot协议，有些网站的robot.txt中表明，不允许爬去，这时候，如果要爬去的话，就要设置为false，不遵守。 No Moulde PIL Find:直接用pycharm自带的interpreter安装pillow mac下要注意python的安装路径参考： http://www.jianshu.com/p/078ad2067419http://www.cnblogs.com/kylinlin/p/5405246.htmlhttp://wiki.jikexueyuan.com/project/scrapy/item-pipeline.htmlhttp://www.open-open.com/lib/view/open1432868637316.html","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Learning","slug":"Learning","permalink":"https://109383670.github.io/tags/Learning/"}]},{"title":"Scrapy安装与运行记录","date":"2017-02-27T04:02:44.000Z","path":"2017/02/27/Scrapy安装与运行记录/","text":"安装Homebrewruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装pythonbrew install python Homebrew会自动安装好Setuptools和 pip 。Setuptools提供 easy_install 命令，实现通过网络（通常Internet）下载和安装第三方Python包。 还可以轻松地将这种网络安装的方式加入到自己开发的Python应用中。pip 是一款方便安装和管理Python 包的工具。 安装Scrapypip install scrapy Scrapy 使用: IDE工具：pycharm社区免费版本 教程参考: http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial 命令: 生成HelloWorld的Scrapy工程 scrapy startproject HelloWorld 在pycharm IDE中配置命令 原理： 执行: scrapy crawl MyPa (MyPa是自己在类中定义的爬虫名字)， 相当于在终端执行： /usr/local/bin/python /usr/local/lib/python2.7/site-packages/scrapy/cmdline.py crawl MyPa 注意：要小心python的路径，如果python的路径不对，还是会报错。这里指的路径是系统路径与pycharm里设置的python路径。 在终端里用which python查看一下路径,如果与pycharm设置里的不同，将修改成更系统路径一样的。 中文问题: shell里输出的是utf-8编码,用print可打印出中文。 用变量格式化的方式，不直接在xpath中用中文字符，而是用一个变量代替。如’中文’,用u’中文’。或者在字符串前加u。如u”//a/text()” 打印的时候可以参考： 1234for sel in response.xpath(\"//div[@id='mcontent']/div/p\"): conect = sel.xpath(\"text()\").extract() for t in conect: print(t.encode(\"utf-8\")) pycharm中支持中文 代码页加入: # -*-coding:utf-8-*- 代码: 1234strpath = u\"//td[descendant::a[contains(text(),'中文字符')]]\"。或者strz = '中文字符'strpath = u\"//td[descendant::a[contains(text(),%s)]]%strz\" json输出中文： 12345678910def __init__(self): self.file = codecs.open(\"items.json\", \"wb\", encoding=\"utf-8\") def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(line) return item def spider_closed(self, spider): self.file.close() ##读取文件with codecs.open(file_name, “r”,encoding=’utf-8’, errors=’ignore’) as fdata: ##decode encodedecode 总是返回unicode字符encode 总是接受一个unicode字符进行转换","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://109383670.github.io/tags/Scrapy/"},{"name":"Setup","slug":"Setup","permalink":"https://109383670.github.io/tags/Setup/"}]},{"title":"hex+mac安装记录","date":"2017-02-22T12:06:47.000Z","path":"2017/02/22/hex+mac安装记录/","text":"有用的命令(hexo所在目录)： sudo hexo clean -清除 sudo hexo g -d 直接发布部署 sudo hexo g 生成 sudo hexo s 打开本地服务器 http://localhost:4000/ 浏览 本地预览步骤： sudo hexo g sudo hexo s http://localhost:4000/ 安装参考：参考域名绑定部分、修改主题各种出现的问题总结 hexo更新更新Hexo版本和Next主题hexo官网 具体安装时出现的问题：不能执行hexo命令只有init,help,version三个命令。解决方案：要在hexo目录下执行。 注意坑：执行hexo server时出错_config.xml里，type: repo: branch:后面，要有一个空格。 在DNS的配置里，加入固定的两个IP @ A 192.30.252.153@ A 192.30.252.154 不能连接git，提示22端口错误git网站中的ssh证书失效，要重新生成，参考帮助说明。ssh生成 git证书生成时，不能填写passphrase这个东西，自己回车跳过git@github.com: Permission denied每过一段时间不用，就会出现这个错误。经过测试发现，是因为更换了路由器造成的，可能ip的变化导致ssh密匙的拒绝。 其他：Next风格不错Next的设置GitWiki里很详细字体及字体大小修改","tags":[{"name":"hexo","slug":"hexo","permalink":"https://109383670.github.io/tags/hexo/"}]},{"title":"Hello World","date":"2017-02-22T04:41:28.000Z","path":"2017/02/22/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]