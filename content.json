[{"title":"Scrapy学习记录","date":"2017-02-27T05:19:50.000Z","path":"2017/02/27/2017-02-27-1/","text":"Scrapy Shell 命令: 开始抓取网页: 1scrapy shell 'http://www.dytt8.net/index.htm' selector内容: 1response.xpath(\"//a/@href\").extract()[0] 输出jsonItem： 1scrapy crawl dmoz -o items.json xpath: following-sibling:除自身外后面的同辈兄弟。如：td/following-sibling::td 同级td兄弟。 xpath中的序列从1开始：/a[1],代表a的第一个元素。没有[0]。 遍历多个变量： 1for t , l in izip(response.xpath(strname), response.xpath(strurl)): r在Python的string前面加上‘r’， 是为了告诉编译器这个string是个raw string，不要转意backslash ‘\\’ 。 例如，\\n 在raw string中，是两个字符，\\和n， 而不会转意为换行符。由于正则表达式和 \\ 会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上’r’。 激活pipeline:在setting.py里，为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子: 123ITEM_PIPELINES = &#123; &apos;myproject.pipelines.PricePipeline&apos;: 300,&#125; 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。如： 1ITEM_PIPELINES = &#123;&apos;工程名.pipelines.自定义处理pipe类名&apos;: 1&#125; 使用相对XPaths:/或者//永远表示的是绝对路径，在嵌套xpath里，用’a/text()’这样的相对路径。 response.urljoin:方法建立绝对路径并且产生新的请求，并注册回调函数parse_dir_contents()来爬取需要的数据。 123456789101112def parse(self, response): for href in response.css(\"ul.directory.dir-col &gt; li &gt; a::attr('href')\"): url = response.urljoin(href.extract()) yield scrapy.Request(url, callback=self.parse_dir_contents) def parse_dir_contents(self, response): for sel in response.xpath('//ul/li'): item = DmozItem() item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item 递归抓取:123456789101112131415161718class Blurb2Spider(BaseSpider): name = \"blurb2\" allowed_domains = [\"www.domain.com\"] def start_requests(self): yield self.make_requests_from_url(\"http://www.domain.com/bookstore/new\") def parse(self, response): hxs = HtmlXPathSelector(response) urls = hxs.select('//div[@class=\"bookListingBookTitle\"]/a/@href').extract() for i in urls: yield Request(urlparse.urljoin('https://www.domain.com/', i[1:]),callback=self.parse_url) def parse_url(self, response): hxs = HtmlXPathSelector(response) print response,'-------&gt;' 相对地址:12import urljoinurlparse.urljoin(response.url, myurl) 定制图片管道的例子:下面是一个图片管道的完整例子，其方法如上所示: 12345678910111213141516import scrapyfrom scrapy.pipeline.images import ImagesPipelinefrom scrapy.exceptions import DropItemclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item 定位要详细://div[@id = “Zoom”]//img[1]/@srcdiv的定位要详细，如果是//div/span/img[1]/@src就返回为null,虽然firebug里面也没有问题。 Strip():Python strip() 方法用于移除字符串头尾指定的字符（默认为空格）。MapCompose(unicode.strip, unicode.title)) ，移除空格与换行例如: 1l.add_xpath('image_time', '//div[@class = \"co_content8\"]/ul/text()[1]', MapCompose(unicode.strip, unicode.title)) 下载图片:settings.py中有一行ROBOTSTXT_OBEY = True，需要改成False，否则可能下载不了图片。ROBOTSTXT_OBEY是否遵守robot协议，有些网站的robot.txt中表明，不允许爬去，这时候，如果要爬去的话，就要设置为false，不遵守。 No Moulde PIL Find:直接用pycharm自带的interpreter安装pillow mac下要注意python的安装路径参考： http://www.jianshu.com/p/078ad2067419http://www.cnblogs.com/kylinlin/p/5405246.htmlhttp://wiki.jikexueyuan.com/project/scrapy/item-pipeline.htmlhttp://www.open-open.com/lib/view/open1432868637316.html","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://codeshuai.com/tags/Scrapy/"},{"name":"Learning","slug":"Learning","permalink":"http://codeshuai.com/tags/Learning/"}]},{"title":"Scrapy安装与运行记录","date":"2017-02-27T04:02:44.000Z","path":"2017/02/27/2017-02-27/","text":"安装Homebrewruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装pythonbrew install python Homebrew会自动安装好Setuptools和 pip 。Setuptools提供 easy_install 命令，实现通过网络（通常Internet）下载和安装第三方Python包。 还可以轻松地将这种网络安装的方式加入到自己开发的Python应用中。pip 是一款方便安装和管理Python 包的工具。 安装Scrapypip install scrapy Scrapy 使用: IDE工具：pycharm社区免费版本 教程参考: http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial 命令: 生成HelloWorld的Scrapy工程 scrapy startproject HelloWorld 在pycharm IDE中配置命令 原理： 执行: scrapy crawl MyPa (MyPa是自己在类中定义的爬虫名字)， 相当于在终端执行： /usr/local/bin/python /usr/local/lib/python2.7/site-packages/scrapy/cmdline.py crawl MyPa 注意：要小心python的路径，如果python的路径不对，还是会报错。这里指的路径是系统路径与pycharm里设置的python路径。 在终端里用which python查看一下路径,如果与pycharm设置里的不同，将修改成更系统路径一样的。 中文问题: shell里输出的是utf-8编码,用print可打印出中文。 用变量格式化的方式，不直接在xpath中用中文字符，而是用一个变量代替。如’中文’,用u’中文’。或者在字符串前加u。如u”//a/text()” 打印的时候可以参考： 1234for sel in response.xpath(\"//div[@id='mcontent']/div/p\"): conect = sel.xpath(\"text()\").extract() for t in conect: print(t.encode(\"utf-8\")) pycharm中支持中文 代码页加入: # -*-coding:utf-8-*- 代码: 1234strpath = u\"//td[descendant::a[contains(text(),'中文字符')]]\"。或者strz = '中文字符'strpath = u\"//td[descendant::a[contains(text(),%s)]]%strz\" json输出中文： 12345678910def __init__(self): self.file = codecs.open(\"items.json\", \"wb\", encoding=\"utf-8\") def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(line) return item def spider_closed(self, spider): self.file.close() ##读取文件with codecs.open(file_name, “r”,encoding=’utf-8’, errors=’ignore’) as fdata: ##decode encodedecode 总是返回unicode字符encode 总是接受一个unicode字符进行转换","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://codeshuai.com/tags/Scrapy/"},{"name":"Setup","slug":"Setup","permalink":"http://codeshuai.com/tags/Setup/"}]},{"title":"hex+mac安装记录","date":"2017-02-22T12:06:47.000Z","path":"2017/02/22/2017-02-22/","text":"有用的命令： hexo g -d 直接发布部署 hexo g 生成 hexo s 打开本地服务器 http://localhost:4000/ 浏览 本地预览步骤： hexo g hexo s http://localhost:4000/ 安装参考：参考域名绑定部分、修改主题各种出现的问题总结 具体安装时出现的问题： 不能执行hexo命令，只有init,help,version三个命令。解决方案：要在hexo目录下执行。 注意坑：执行hexo server时出错，_config.xml里，type: repo: branch:后面，要有一个空格。 在DNS的配置里，加入固定的两个IP @ A 192.30.252.153@ A 192.30.252.154 hexo主题：Next风格不错Next的设置GitWiki里很详细","tags":[{"name":"hexo","slug":"hexo","permalink":"http://codeshuai.com/tags/hexo/"}]},{"title":"Hello World","date":"2017-02-22T04:41:28.000Z","path":"2017/02/22/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]