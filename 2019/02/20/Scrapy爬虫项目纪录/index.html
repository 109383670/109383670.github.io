<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Scrapy,Python,爬虫,">





  <link rel="alternate" href="/atom.xml" title="一点乐趣" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。 编程环境 VSCode Python3 Scrapy  安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL 提示没有安装win32api用pip 安装win32： 1pip install pywin32 安装命令1pip install scrapy 更新命令1sudo pip">
<meta name="keywords" content="Scrapy,Python,爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬虫项目纪录">
<meta property="og:url" content="https://109383670.github.io/2019/02/20/Scrapy爬虫项目纪录/index.html">
<meta property="og:site_name" content="一点乐趣">
<meta property="og:description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。 编程环境 VSCode Python3 Scrapy  安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL 提示没有安装win32api用pip 安装win32： 1pip install pywin32 安装命令1pip install scrapy 更新命令1sudo pip">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-03T08:54:03.522Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy爬虫项目纪录">
<meta name="twitter:description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。 编程环境 VSCode Python3 Scrapy  安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL 提示没有安装win32api用pip 安装win32： 1pip install pywin32 安装命令1pip install scrapy 更新命令1sudo pip">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://109383670.github.io/2019/02/20/Scrapy爬虫项目纪录/">





  <title> Scrapy爬虫项目纪录 | 一点乐趣 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一点乐趣</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://109383670.github.io/2019/02/20/Scrapy爬虫项目纪录/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="BoomCode">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="一点乐趣">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="一点乐趣" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Scrapy爬虫项目纪录
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-20T00:46:54+08:00">
                2019-02-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。</p>
<h3 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h3><ul>
<li>VSCode</li>
<li>Python3</li>
<li>Scrapy</li>
</ul>
<h3 id="安装记录"><a href="#安装记录" class="headerlink" title="安装记录"></a>安装记录</h3><h4 id="win下安装"><a href="#win下安装" class="headerlink" title="win下安装"></a>win下安装</h4><h5 id="用pip命令安装Scrapy时提示没有MS框架"><a href="#用pip命令安装Scrapy时提示没有MS框架" class="headerlink" title="用pip命令安装Scrapy时提示没有MS框架"></a>用pip命令安装Scrapy时提示没有MS框架</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">安装MS Build TOOL</span><br></pre></td></tr></table></figure>
<h5 id="提示没有安装win32api"><a href="#提示没有安装win32api" class="headerlink" title="提示没有安装win32api"></a>提示没有安装win32api</h5><p>用pip 安装win32：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure>
<h5 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
<p>更新命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install --upgrade scrapy</span><br></pre></td></tr></table></figure></p>
<h4 id="mac-下安装"><a href="#mac-下安装" class="headerlink" title="mac 下安装"></a>mac 下安装</h4><p>mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。<br>mac下用Homebrew来进行升级</p>
<ol>
<li><p>安装xcode命令行工具</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://brew.sh/" target="_blank" rel="noopener">https://brew.sh/</a> 安装Homebrew</p>
</li>
<li><p>将Homebrew加入环境变量中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export PATH=/usr/local/bin:/usr/local/sbin:<span class="variable">$PATH</span>"</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装python</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install python</span><br></pre></td></tr></table></figure>
<p> 如果已经安装，可以进行升级</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew update; brew upgrade python</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>安装scrapy<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install scrapy</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h3 id="学习记录"><a href="#学习记录" class="headerlink" title="学习记录"></a>学习记录</h3><h4 id="生成Scrapy框架"><a href="#生成Scrapy框架" class="headerlink" title="生成Scrapy框架"></a>生成Scrapy框架</h4><p>SCrapy必须在固定的框架下运行，可以自动生成后再去改动。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 工程名</span><br></pre></td></tr></table></figure></p>
<h4 id="HelloWorld代码"><a href="#HelloWorld代码" class="headerlink" title="HelloWorld代码"></a>HelloWorld代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment"># 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">"quotes"</span>    <span class="comment"># 唯一的爬虫名字，在运行时要用到</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span>    <span class="comment"># 复写的方法，初始请求的网址</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span>       <span class="comment"># 复写的方法，在这里对爬下的数据进行处理</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>
 <a id="more"></a>
<h4 id="深入学习"><a href="#深入学习" class="headerlink" title="深入学习"></a>深入学习</h4><h5 id="例子1-提取内容"><a href="#例子1-提取内容" class="headerlink" title="例子1-提取内容"></a>例子1-提取内容</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取相关格言以及作者等信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">                <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>输出json或者jl(JSON Lines)命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.json</span><br><span class="line"></span><br><span class="line">scrapy crawl quotes -o quotes.jl</span><br></pre></td></tr></table></figure></p>
<h5 id="例子2-爬取下一个链接"><a href="#例子2-爬取下一个链接" class="headerlink" title="例子2-爬取下一个链接"></a>例子2-爬取下一个链接</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">                <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            next_page = response.urljoin(next_page)     <span class="comment">#获得真实的链接地址</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse)  <span class="comment">#下一个链接的处理回调</span></span><br></pre></td></tr></table></figure>
<p>后面两句可以用下面的代替，不用写urljoin了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> response.follow(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure>
<p>进一步简化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">    <span class="keyword">yield</span> response.follow(href, callback=self.parse)</span><br></pre></td></tr></table></figure>
<p>再进一步简化：<br>对于a 标签，会自动使用它的href属性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> response.css(<span class="string">'li.next a'</span>):</span><br><span class="line">    <span class="keyword">yield</span> response.follow(a, callback=self.parse)</span><br></pre></td></tr></table></figure></p>
<h5 id="进阶例子"><a href="#进阶例子" class="headerlink" title="进阶例子"></a>进阶例子</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthorSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'author'</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># follow links to author pages</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'.author + a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse_author)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># follow pagination links</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_author</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">extract_with_css</span><span class="params">(query)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">''</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'name'</span>: extract_with_css(<span class="string">'h3.author-title::text'</span>),</span><br><span class="line">            <span class="string">'birthdate'</span>: extract_with_css(<span class="string">'.author-born-date::text'</span>),</span><br><span class="line">            <span class="string">'bio'</span>: extract_with_css(<span class="string">'.author-description::text'</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h5 id="命令行参数例子"><a href="#命令行参数例子" class="headerlink" title="命令行参数例子"></a>命令行参数例子</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        url = <span class="string">'http://quotes.toscrape.com/'</span></span><br><span class="line">        tag = getattr(self, <span class="string">'tag'</span>, <span class="literal">None</span>)    <span class="comment">#从命令行参数获得</span></span><br><span class="line">        <span class="keyword">if</span> tag <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            url = url + <span class="string">'tag/'</span> + tag</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br></pre></td></tr></table></figure>
<p>命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes-humor.json -a tag=humor</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://quotes.toscrape.com/tag/humor</span><br></pre></td></tr></table></figure>
<h5 id="item"><a href="#item" class="headerlink" title="item"></a>item</h5><p>可以自己定义的数据结构<br>格式如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br></pre></td></tr></table></figure></p>
<h5 id="item-pipeline"><a href="#item-pipeline" class="headerlink" title="item pipeline"></a>item pipeline</h5><p>处理item数据的地方，在parse中返回item,就会调用该方法。<br>格式如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item.get(<span class="string">'price'</span>):</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'price_excludes_vat'</span>):</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>在setting里启动pipeline<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,   <span class="comment">#数字表示优先顺序，越小的越先执行</span></span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> ItcastItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment">#open("teacher.html","wb").write(response.body).close()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存放老师信息的集合</span></span><br><span class="line">    <span class="comment">#items = []</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='li_txt']"</span>):</span><br><span class="line">        <span class="comment"># 将我们得到的数据封装到一个 `ItcastItem` 对象</span></span><br><span class="line">        item = ItcastItem()</span><br><span class="line">        <span class="comment">#extract()方法返回的都是unicode字符串</span></span><br><span class="line">        name = each.xpath(<span class="string">"h3/text()"</span>).extract()</span><br><span class="line">        title = each.xpath(<span class="string">"h4/text()"</span>).extract()</span><br><span class="line">        info = each.xpath(<span class="string">"p/text()"</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#xpath返回的是包含一个元素的列表</span></span><br><span class="line">        item[<span class="string">'name'</span>] = name[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'title'</span>] = title[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'info'</span>] = info[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#items.append(item)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将获取的数据交给pipelines</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据，不经过pipeline</span></span><br><span class="line">    <span class="comment">#return items</span></span><br></pre></td></tr></table></figure></p>
<h5 id="中文乱码转为utf-8"><a href="#中文乱码转为utf-8" class="headerlink" title="中文乱码转为utf-8"></a>中文乱码转为utf-8</h5><p>python3默认为unicode,如果输出为中文，则要转为utf-8，不然会是乱码<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(</span><br><span class="line">            <span class="string">'items.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.seek(<span class="number">-1</span>, os.SEEK_END)</span><br><span class="line">        self.file.truncate()</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h5 id="imagepipeline各函数运行流程"><a href="#imagepipeline各函数运行流程" class="headerlink" title="imagepipeline各函数运行流程"></a>imagepipeline各函数运行流程</h5><ol>
<li>imagepipeline启动</li>
<li>get_media_requests 将所有的下载请求一次全部完成</li>
<li>下载完成后再统一执行item_completed</li>
</ol>
<h5 id="同时下载多个图片并改名"><a href="#同时下载多个图片并改名" class="headerlink" title="同时下载多个图片并改名"></a>同时下载多个图片并改名</h5><p>重写file_path函数实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">      :param info:</span></span><br><span class="line"><span class="string">      :return:</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">      <span class="comment">#这里传递字符，或者图片列表，如果是单个的对象，则非常容易被覆盖</span></span><br><span class="line">      <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>], meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      : param request: 每一个图片下载管道请求</span></span><br><span class="line"><span class="string">      : param response:</span></span><br><span class="line"><span class="string">      : param info:</span></span><br><span class="line"><span class="string">      : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</span></span><br><span class="line"><span class="string">      : return: 每套图的分类目录</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">      item = request.meta[<span class="string">'item'</span>]</span><br><span class="line">      folder = item</span><br><span class="line">      folder_strip = strip(folder)</span><br><span class="line">      <span class="comment"># img_path = "%s%s" % (self.img_store, folder_strip)</span></span><br><span class="line">      filename = folder_strip + <span class="string">'/'</span> + folder_strip + <span class="string">'.jpg'</span></span><br><span class="line">      <span class="keyword">return</span> filename</span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(path)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  :param path: 需要清洗的文件夹名字</span></span><br><span class="line"><span class="string">  :return: 清洗掉Windows系统非法文件夹名字的字符串</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  path = re.sub(<span class="string">r'[？\\*|“&lt;&gt;:/]'</span>, <span class="string">''</span>, str(path))</span><br><span class="line">  <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure></p>
<h5 id="Request-回调传递参数"><a href="#Request-回调传递参数" class="headerlink" title="Request 回调传递参数"></a>Request 回调传递参数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item, <span class="string">'param'</span>: name&#125;)</span><br><span class="line"></span><br><span class="line">在parse中提取参数</span><br><span class="line">item = response.meta[<span class="string">'item'</span>]</span><br></pre></td></tr></table></figure>
<h5 id="结果去重"><a href="#结果去重" class="headerlink" title="结果去重"></a>结果去重</h5><ol>
<li>Request的参数 dont_filter=False 默认去重</li>
<li>启用一个爬虫的持久化，运行以下命令:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。<br>恢复这个爬虫也是同样的命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</span><br></pre></td></tr></table></figure></p>
<p>这样爬虫断掉后，再启动会接着上次的 url 跑。</p>
<p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数<br>运行爬虫如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spider1 -L WARNING</span><br></pre></td></tr></table></figure></p>
<p>不打印Debug信息，可以清楚得看到运行过程。</p>
<ol>
<li>scrapy-red</li>
</ol>
<h3 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h3><h4 id="pipeline-is-not-a-full-path"><a href="#pipeline-is-not-a-full-path" class="headerlink" title="pipeline is not a full path"></a>pipeline is not a full path</h4><p>应该在 setting 中填入完整的管道的路径，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pic.pipelines.PicImagesDownloadPipeline</span><br></pre></td></tr></table></figure></p>
<p>如果只填PicImagesDownloadPipeline,就会出现这个错误。</p>
<h4 id="Symbol-not-found-PyInt-AsLong-错误"><a href="#Symbol-not-found-PyInt-AsLong-错误" class="headerlink" title="Symbol not found:  _PyInt_AsLong 错误"></a>Symbol not found:  _PyInt_AsLong 错误</h4><p>将系统python目录下的PIL和Pillow库都删除，再用pip3安装在 Python3的安装目录下<br>系统python安装目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Library/Python/2.7/site-packages</span><br></pre></td></tr></table></figure></p>
<h4 id="Missing-scheme-in-request-url-h"><a href="#Missing-scheme-in-request-url-h" class="headerlink" title="Missing scheme in request url: h"></a>Missing scheme in request url: h</h4><p>相关URL必须是一个List，所以遇到该错误只需要将url转换成list即可。<br>例如：<br>start_urls = [‘someurls’]<br>如果是images_url也是如此，使用item存储的时候改成list即可。<br>item[‘images_urls’] = [‘image_url’]</p>
<h4 id="Request-url-must-be-str-or-unicode"><a href="#Request-url-must-be-str-or-unicode" class="headerlink" title="Request url must be str or unicode"></a>Request url must be str or unicode</h4><p>请求的url参数不能是一个列表，必须是一个字符</p>
<h4 id="在item-complete中改名多个图片不成功"><a href="#在item-complete中改名多个图片不成功" class="headerlink" title="在item_complete中改名多个图片不成功"></a>在item_complete中改名多个图片不成功</h4><p>item_complete并不是在get_media_requests下载图片后马上启动的，它是要等所有的图片下载完成，再统一启动complete事件，这样就导致多个图片没法改名，不能获得之前的item的字段。改名需要重写file_path</p>
<h4 id="get-media-requests中回调参数要小心"><a href="#get-media-requests中回调参数要小心" class="headerlink" title="get_media_requests中回调参数要小心"></a>get_media_requests中回调参数要小心</h4><p>meta中可以加入回调的参数，如果传递的是对象要非常小心，如果对象发生变化，会导致后面所有的回调参数发生变化，传递的如果是字符，就没有这个风险。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">       :param info:</span></span><br><span class="line"><span class="string">       :return:</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>], meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br></pre></td></tr></table></figure></p>
<h4 id="Filtered-duplicate-request"><a href="#Filtered-duplicate-request" class="headerlink" title="Filtered duplicate request"></a>Filtered duplicate request</h4><p>有重复下载的请求，如果要重复下载，在<em>Request函数</em>里加上参数 <em>dont_filter=True</em>，默认是<em>False</em></p>
<h3 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h3><p>piczz.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> piczz.items <span class="keyword">import</span> PiczzItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">piczzSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"piczz"</span></span><br><span class="line">    allowed_domains = [<span class="string">""</span>]</span><br><span class="line">    start_urls = [<span class="string">""</span>]</span><br><span class="line">    img_paths = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(</span><br><span class="line">                <span class="string">"//div[@class = 'post_box']"</span>):</span><br><span class="line">            <span class="comment"># extract()方法返回的都是unicode字符串</span></span><br><span class="line">            item = PiczzItem()</span><br><span class="line">            item[<span class="string">'name'</span>] = <span class="string">'startpage'</span></span><br><span class="line"></span><br><span class="line">            self.img_paths.clear()</span><br><span class="line">            item[<span class="string">'pic_name'</span>] = each.xpath(</span><br><span class="line">                <span class="string">"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            item[<span class="string">'pic_url'</span>] = each.xpath(</span><br><span class="line">                <span class="string">"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>],</span><br><span class="line">                                 callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#递归下一页图片</span></span><br><span class="line">        next_path = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'page_num']/a[last()]"</span>)</span><br><span class="line">        next_con = next_path.xpath(<span class="string">"text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        next_con = next_con.strip()</span><br><span class="line">        next_page = <span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> next_con == <span class="string">"下一頁 »"</span>:</span><br><span class="line">            next_page = next_path.xpath(<span class="string">"@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            print(next_page)</span><br><span class="line">            <span class="keyword">if</span> next_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(next_page, self.parse)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 下载一个索引页的图片</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_imgs</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.img_paths.clear()</span><br><span class="line">        item = response.meta[<span class="string">'item'</span>]</span><br><span class="line">        imgs = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'entry-content']/p/img/@src"</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> imgs:</span><br><span class="line">            self.img_paths.append(e)</span><br><span class="line">        item[<span class="string">'pic_paths'</span>] = self.img_paths</span><br><span class="line">        next_path = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'wp-pagenavi']/p/a[last()]"</span>)</span><br><span class="line">        next_con = next_path.xpath(<span class="string">"text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        next_con = next_con.strip()</span><br><span class="line">        <span class="keyword">if</span> next_con == <span class="string">"下一页"</span>:</span><br><span class="line">            next_page = next_path.xpath(<span class="string">"@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item&#125;)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p>
<p>item.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PiczzItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    pic_name = scrapy.Field()  <span class="comment"># 图片目录名</span></span><br><span class="line">    pic_url = scrapy.Field()  <span class="comment"># 图片索引首页地址</span></span><br><span class="line">    pic_paths = scrapy.Field()  <span class="comment"># 图片下载地址列表</span></span><br></pre></td></tr></table></figure>
<p>pipeline.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PiczzImagesDownloadPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">        :param info:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> img_url <span class="keyword">in</span> item[<span class="string">'pic_paths'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(img_url, meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        : param request: 每一个图片下载管道请求</span></span><br><span class="line"><span class="string">        : param response:</span></span><br><span class="line"><span class="string">        : param info:</span></span><br><span class="line"><span class="string">        : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</span></span><br><span class="line"><span class="string">        : return: 每套图的分类目录</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        item = request.meta[<span class="string">'item'</span>]</span><br><span class="line">        folder = item</span><br><span class="line">        folder_strip = strip(folder)</span><br><span class="line">        image_guid = request.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        filename = folder_strip + <span class="string">'/'</span> + image_guid + <span class="string">'.jpg'</span></span><br><span class="line">        <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Item contains no images"</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param path: 需要清洗的文件夹名字</span></span><br><span class="line"><span class="string">    :return: 清洗掉Windows系统非法文件夹名字的字符串</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    path = re.sub(<span class="string">r'[？\\*|“&lt;&gt;:/]'</span>, <span class="string">''</span>, str(path))</span><br><span class="line">    <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从搭建环境到断断续续的学习花了大概五天时间 ，每天平均花二个小时学习，终于成功的将设定的目标完成。</p>
<h3 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h3><p><a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="noopener">官网</a><br><a href="https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html" target="_blank" rel="noopener">中文参考网站</a><br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">xPath语法</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="noopener">Python中yield的解释</a><br><a href="https://blog.csdn.net/a542551042/article/details/47149959" target="_blank" rel="noopener">mac os Python路径总结</a><br><a href="https://segmentfault.com/a/1190000013178839" target="_blank" rel="noopener">Scrapy框架入门简介</a><br><a href="https://segmentfault.com/a/1190000009597329" target="_blank" rel="noopener">ImagesPipeline下载图片</a><br><a href="https://segmentfault.com/q/1010000000413334" target="_blank" rel="noopener">ImagesPipeline下载图片保持原文件名</a><br><a href="https://cuiqingcai.com/4421.html" target="_blank" rel="noopener">小白进阶之Scrapy第四篇</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="noopener">Python中yield的解释</a><br><a href="https://blog.csdn.net/heheyanyanjun/article/details/79199378" target="_blank" rel="noopener">scrapy调用parse()中使用yield引发对yield的分析</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Scrapy/" rel="tag"># Scrapy</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/11/egret与cocos creator/" rel="next" title="egret与cocos creator">
                <i class="fa fa-chevron-left"></i> egret与cocos creator
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/02/cocos2d-x 3.0相对于2.0的变化/" rel="prev" title="cocos2d-x 3.0相对于2.0的变化">
                cocos2d-x 3.0相对于2.0的变化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="BoomCode">
          <p class="site-author-name" itemprop="name">BoomCode</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#目标"><span class="nav-number">1.</span> <span class="nav-text">目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程环境"><span class="nav-number">2.</span> <span class="nav-text">编程环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装记录"><span class="nav-number">3.</span> <span class="nav-text">安装记录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#win下安装"><span class="nav-number">3.1.</span> <span class="nav-text">win下安装</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#用pip命令安装Scrapy时提示没有MS框架"><span class="nav-number">3.1.1.</span> <span class="nav-text">用pip命令安装Scrapy时提示没有MS框架</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#提示没有安装win32api"><span class="nav-number">3.1.2.</span> <span class="nav-text">提示没有安装win32api</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#安装命令"><span class="nav-number">3.1.3.</span> <span class="nav-text">安装命令</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mac-下安装"><span class="nav-number">3.2.</span> <span class="nav-text">mac 下安装</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习记录"><span class="nav-number">4.</span> <span class="nav-text">学习记录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#生成Scrapy框架"><span class="nav-number">4.1.</span> <span class="nav-text">生成Scrapy框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HelloWorld代码"><span class="nav-number">4.2.</span> <span class="nav-text">HelloWorld代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深入学习"><span class="nav-number">4.3.</span> <span class="nav-text">深入学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#例子1-提取内容"><span class="nav-number">4.3.1.</span> <span class="nav-text">例子1-提取内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#例子2-爬取下一个链接"><span class="nav-number">4.3.2.</span> <span class="nav-text">例子2-爬取下一个链接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#进阶例子"><span class="nav-number">4.3.3.</span> <span class="nav-text">进阶例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#命令行参数例子"><span class="nav-number">4.3.4.</span> <span class="nav-text">命令行参数例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#item"><span class="nav-number">4.3.5.</span> <span class="nav-text">item</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#item-pipeline"><span class="nav-number">4.3.6.</span> <span class="nav-text">item pipeline</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#中文乱码转为utf-8"><span class="nav-number">4.3.7.</span> <span class="nav-text">中文乱码转为utf-8</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#imagepipeline各函数运行流程"><span class="nav-number">4.3.8.</span> <span class="nav-text">imagepipeline各函数运行流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#同时下载多个图片并改名"><span class="nav-number">4.3.9.</span> <span class="nav-text">同时下载多个图片并改名</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Request-回调传递参数"><span class="nav-number">4.3.10.</span> <span class="nav-text">Request 回调传递参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#结果去重"><span class="nav-number">4.3.11.</span> <span class="nav-text">结果去重</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#错误记录"><span class="nav-number">5.</span> <span class="nav-text">错误记录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pipeline-is-not-a-full-path"><span class="nav-number">5.1.</span> <span class="nav-text">pipeline is not a full path</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Symbol-not-found-PyInt-AsLong-错误"><span class="nav-number">5.2.</span> <span class="nav-text">Symbol not found:  _PyInt_AsLong 错误</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Missing-scheme-in-request-url-h"><span class="nav-number">5.3.</span> <span class="nav-text">Missing scheme in request url: h</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Request-url-must-be-str-or-unicode"><span class="nav-number">5.4.</span> <span class="nav-text">Request url must be str or unicode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在item-complete中改名多个图片不成功"><span class="nav-number">5.5.</span> <span class="nav-text">在item_complete中改名多个图片不成功</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#get-media-requests中回调参数要小心"><span class="nav-number">5.6.</span> <span class="nav-text">get_media_requests中回调参数要小心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Filtered-duplicate-request"><span class="nav-number">5.7.</span> <span class="nav-text">Filtered duplicate request</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最终代码"><span class="nav-number">6.</span> <span class="nav-text">最终代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考网站"><span class="nav-number">8.</span> <span class="nav-text">参考网站</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BoomCode</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  

  

  

  


  

</body>
</html>
