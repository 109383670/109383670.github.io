<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Scrapy,Python,爬虫," />





  <link rel="alternate" href="/atom.xml" title="一点乐趣" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。
编程环境
VSCode
Python3
Scrapy

安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL
提示没有安装win32api用pip 安装win32：
1pip install pywin32
安装命令1pip install scrapy
更新命令1sudo pip">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬虫项目纪录">
<meta property="og:url" content="http://codeshuai.com/2019/02/20/Scrapy爬虫项目纪录/index.html">
<meta property="og:site_name" content="一点乐趣">
<meta property="og:description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。
编程环境
VSCode
Python3
Scrapy

安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL
提示没有安装win32api用pip 安装win32：
1pip install pywin32
安装命令1pip install scrapy
更新命令1sudo pip">
<meta property="og:updated_time" content="2019-02-19T16:49:16.659Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy爬虫项目纪录">
<meta name="twitter:description" content="目标从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。
编程环境
VSCode
Python3
Scrapy

安装记录win下安装用pip命令安装Scrapy时提示没有MS框架1安装MS Build TOOL
提示没有安装win32api用pip 安装win32：
1pip install pywin32
安装命令1pip install scrapy
更新命令1sudo pip">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://codeshuai.com/2019/02/20/Scrapy爬虫项目纪录/"/>





  <title> Scrapy爬虫项目纪录 | 一点乐趣 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一点乐趣</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://codeshuai.com/2019/02/20/Scrapy爬虫项目纪录/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="BoomCode">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="一点乐趣">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="一点乐趣" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Scrapy爬虫项目纪录
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-20T00:46:54+08:00">
                2019-02-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。</p>
<h3 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h3><ul>
<li>VSCode</li>
<li>Python3</li>
<li>Scrapy</li>
</ul>
<h3 id="安装记录"><a href="#安装记录" class="headerlink" title="安装记录"></a>安装记录</h3><h4 id="win下安装"><a href="#win下安装" class="headerlink" title="win下安装"></a>win下安装</h4><h5 id="用pip命令安装Scrapy时提示没有MS框架"><a href="#用pip命令安装Scrapy时提示没有MS框架" class="headerlink" title="用pip命令安装Scrapy时提示没有MS框架"></a>用pip命令安装Scrapy时提示没有MS框架</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">安装MS Build TOOL</div></pre></td></tr></table></figure>
<h5 id="提示没有安装win32api"><a href="#提示没有安装win32api" class="headerlink" title="提示没有安装win32api"></a>提示没有安装win32api</h5><p>用pip 安装win32：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pywin32</div></pre></td></tr></table></figure>
<h5 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install scrapy</div></pre></td></tr></table></figure>
<p>更新命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo pip install --upgrade scrapy</div></pre></td></tr></table></figure></p>
<h4 id="mac-下安装"><a href="#mac-下安装" class="headerlink" title="mac 下安装"></a>mac 下安装</h4><p>mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。<br>mac下用Homebrew来进行升级</p>
<ol>
<li><p>安装xcode命令行工具</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure>
</li>
<li><p><a href="https://brew.sh/" target="_blank" rel="external">https://brew.sh/</a> 安装Homebrew</p>
</li>
<li><p>将Homebrew加入环境变量中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo &quot;export PATH=/usr/local/bin:/usr/local/sbin:$PATH&quot; &gt;&gt; ~/.bashrc</div><div class="line">source ~/.bashrc</div></pre></td></tr></table></figure>
</li>
<li><p>安装python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install python</div></pre></td></tr></table></figure>
<p> 如果已经安装，可以进行升级</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew update; brew upgrade python</div></pre></td></tr></table></figure>
<ol>
<li>安装scrapy<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install scrapy</div></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h3 id="学习记录"><a href="#学习记录" class="headerlink" title="学习记录"></a>学习记录</h3><h5 id="生成Scrapy框架"><a href="#生成Scrapy框架" class="headerlink" title="生成Scrapy框架"></a>生成Scrapy框架</h5><p>SCrapy必须在固定的框架下运行，可以自动生成后再去改动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject 工程名</div></pre></td></tr></table></figure></p>
<h5 id="HelloWorld代码"><a href="#HelloWorld代码" class="headerlink" title="HelloWorld代码"></a>HelloWorld代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">class QuotesSpider(scrapy.Spider):  # 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法</div><div class="line"></div><div class="line">    name = &quot;quotes&quot;    # 唯一的爬虫名字，在运行时要用到</div><div class="line">    </div><div class="line">    def start_requests(self):    # 复写的方法，初始请求的网址</div><div class="line">        urls = [</div><div class="line">            &apos;http://quotes.toscrape.com/page/1/&apos;,</div><div class="line">            &apos;http://quotes.toscrape.com/page/2/&apos;,</div><div class="line">        ]</div><div class="line">        for url in urls:</div><div class="line">            yield scrapy.Request(url=url, callback=self.parse)</div><div class="line">            </div><div class="line">    def parse(self, response):       # 复写的方法，在这里对爬下的数据进行处理</div><div class="line">        page = response.url.split(&quot;/&quot;)[-2]</div><div class="line">        filename = &apos;quotes-%s.html&apos; % page</div><div class="line">        with open(filename, &apos;wb&apos;) as f:</div><div class="line">            f.write(response.body)</div><div class="line">        self.log(&apos;Saved file %s&apos; % filename)</div></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl quotes</div></pre></td></tr></table></figure>
<h4 id="深入学习"><a href="#深入学习" class="headerlink" title="深入学习"></a>深入学习</h4><h5 id="例子1-提取内容"><a href="#例子1-提取内容" class="headerlink" title="例子1-提取内容"></a>例子1-提取内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># 提取相关格言以及作者等信息</div><div class="line"></div><div class="line">import scrapy</div><div class="line"></div><div class="line">class QuotesSpider(scrapy.Spider):</div><div class="line">    name = &quot;quotes&quot;</div><div class="line">    start_urls = [</div><div class="line">        &apos;http://quotes.toscrape.com/page/1/&apos;,</div><div class="line">        &apos;http://quotes.toscrape.com/page/2/&apos;,</div><div class="line">    ]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        for quote in response.css(&apos;div.quote&apos;):</div><div class="line">            yield &#123;</div><div class="line">                &apos;text&apos;: quote.css(&apos;span.text::text&apos;).get(),</div><div class="line">                &apos;author&apos;: quote.css(&apos;small.author::text&apos;).get(),</div><div class="line">                &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).getall(),</div><div class="line">            &#125;</div></pre></td></tr></table></figure>
<p>输出json或者jl(JSON Lines)命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scrapy crawl quotes -o quotes.json</div><div class="line"></div><div class="line">scrapy crawl quotes -o quotes.jl</div></pre></td></tr></table></figure></p>
<h5 id="例子2-爬取下一个链接"><a href="#例子2-爬取下一个链接" class="headerlink" title="例子2-爬取下一个链接"></a>例子2-爬取下一个链接</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class QuotesSpider(scrapy.Spider):</div><div class="line">    name = &quot;quotes&quot;</div><div class="line">    start_urls = [</div><div class="line">        &apos;http://quotes.toscrape.com/page/1/&apos;,</div><div class="line">    ]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        for quote in response.css(&apos;div.quote&apos;):</div><div class="line">            yield &#123;</div><div class="line">                &apos;text&apos;: quote.css(&apos;span.text::text&apos;).get(),</div><div class="line">                &apos;author&apos;: quote.css(&apos;small.author::text&apos;).get(),</div><div class="line">                &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).getall(),</div><div class="line">            &#125;</div><div class="line"></div><div class="line">        next_page = response.css(&apos;li.next a::attr(href)&apos;).get()</div><div class="line">        if next_page is not None:</div><div class="line">            next_page = response.urljoin(next_page)     #获得真实的链接地址</div><div class="line">            yield scrapy.Request(next_page, callback=self.parse)  #下一个链接的处理回调</div></pre></td></tr></table></figure>
<p>后面两句可以用下面的代替，不用写urljoin了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yield response.follow(next_page, callback=self.parse)</div></pre></td></tr></table></figure>
<p>进一步简化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">for href in response.css(&apos;li.next a::attr(href)&apos;):</div><div class="line">    yield response.follow(href, callback=self.parse)</div></pre></td></tr></table></figure>
<p>再进一步简化：<br>对于a 标签，会自动使用它的href属性<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">for a in response.css(&apos;li.next a&apos;):</div><div class="line">    yield response.follow(a, callback=self.parse)</div></pre></td></tr></table></figure></p>
<h5 id="进阶例子"><a href="#进阶例子" class="headerlink" title="进阶例子"></a>进阶例子</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class AuthorSpider(scrapy.Spider):</div><div class="line">    name = &apos;author&apos;</div><div class="line"></div><div class="line">    start_urls = [&apos;http://quotes.toscrape.com/&apos;]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        # follow links to author pages</div><div class="line">        for href in response.css(&apos;.author + a::attr(href)&apos;):</div><div class="line">            yield response.follow(href, self.parse_author)</div><div class="line"></div><div class="line">        # follow pagination links</div><div class="line">        for href in response.css(&apos;li.next a::attr(href)&apos;):</div><div class="line">            yield response.follow(href, self.parse)</div><div class="line"></div><div class="line">    def parse_author(self, response):</div><div class="line">        def extract_with_css(query):</div><div class="line">            return response.css(query).get(default=&apos;&apos;).strip()</div><div class="line"></div><div class="line">        yield &#123;</div><div class="line">            &apos;name&apos;: extract_with_css(&apos;h3.author-title::text&apos;),</div><div class="line">            &apos;birthdate&apos;: extract_with_css(&apos;.author-born-date::text&apos;),</div><div class="line">            &apos;bio&apos;: extract_with_css(&apos;.author-description::text&apos;),</div><div class="line">        &#125;</div></pre></td></tr></table></figure>
<h5 id="命令行参数例子"><a href="#命令行参数例子" class="headerlink" title="命令行参数例子"></a>命令行参数例子</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class QuotesSpider(scrapy.Spider):</div><div class="line">    name = &quot;quotes&quot;</div><div class="line"></div><div class="line">    def start_requests(self):</div><div class="line">        url = &apos;http://quotes.toscrape.com/&apos;</div><div class="line">        tag = getattr(self, &apos;tag&apos;, None)    #从命令行参数获得</div><div class="line">        if tag is not None:</div><div class="line">            url = url + &apos;tag/&apos; + tag</div><div class="line">        yield scrapy.Request(url, self.parse)</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        for quote in response.css(&apos;div.quote&apos;):</div><div class="line">            yield &#123;</div><div class="line">                &apos;text&apos;: quote.css(&apos;span.text::text&apos;).get(),</div><div class="line">                &apos;author&apos;: quote.css(&apos;small.author::text&apos;).get(),</div><div class="line">            &#125;</div><div class="line"></div><div class="line">        next_page = response.css(&apos;li.next a::attr(href)&apos;).get()</div><div class="line">        if next_page is not None:</div><div class="line">            yield response.follow(next_page, self.parse)</div></pre></td></tr></table></figure>
<p>命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl quotes -o quotes-humor.json -a tag=humor</div></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://quotes.toscrape.com/tag/humor</div></pre></td></tr></table></figure>
<h5 id="item"><a href="#item" class="headerlink" title="item"></a>item</h5><p>可以自己定义的数据结构<br>格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class Product(scrapy.Item):</div><div class="line">    name = scrapy.Field()</div><div class="line">    price = scrapy.Field()</div><div class="line">    stock = scrapy.Field()</div><div class="line">    last_updated = scrapy.Field(serializer=str)</div></pre></td></tr></table></figure></p>
<h5 id="item-pipeline"><a href="#item-pipeline" class="headerlink" title="item pipeline"></a>item pipeline</h5><p>处理item数据的地方，在parse中返回item,就会调用该方法。<br>格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">from scrapy.exceptions import DropItem</div><div class="line"></div><div class="line">class PricePipeline(object):</div><div class="line"></div><div class="line">    vat_factor = 1.15</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        if item.get(&apos;price&apos;):</div><div class="line">            if item.get(&apos;price_excludes_vat&apos;):</div><div class="line">                item[&apos;price&apos;] = item[&apos;price&apos;] * self.vat_factor</div><div class="line">            return item</div><div class="line">        else:</div><div class="line">            raise DropItem(&quot;Missing price in %s&quot; % item)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line"></div><div class="line">class JsonWriterPipeline(object):</div><div class="line"></div><div class="line">    def open_spider(self, spider):</div><div class="line">        self.file = open(&apos;items.jl&apos;, &apos;w&apos;)</div><div class="line"></div><div class="line">    def close_spider(self, spider):</div><div class="line">        self.file.close()</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        line = json.dumps(dict(item)) + &quot;\n&quot;</div><div class="line">        self.file.write(line)</div><div class="line">        return item</div></pre></td></tr></table></figure>
<p>在setting里启动pipeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    &apos;myproject.pipelines.PricePipeline&apos;: 300,   #数字表示优先顺序，越小的越先执行</div><div class="line">    &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800,</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">from mySpider.items import ItcastItem</div><div class="line"></div><div class="line">def parse(self, response):</div><div class="line">    #open(&quot;teacher.html&quot;,&quot;wb&quot;).write(response.body).close()</div><div class="line"></div><div class="line">    # 存放老师信息的集合</div><div class="line">    #items = []</div><div class="line"></div><div class="line">    for each in response.xpath(&quot;//div[@class=&apos;li_txt&apos;]&quot;):</div><div class="line">        # 将我们得到的数据封装到一个 `ItcastItem` 对象</div><div class="line">        item = ItcastItem()</div><div class="line">        #extract()方法返回的都是unicode字符串</div><div class="line">        name = each.xpath(&quot;h3/text()&quot;).extract()</div><div class="line">        title = each.xpath(&quot;h4/text()&quot;).extract()</div><div class="line">        info = each.xpath(&quot;p/text()&quot;).extract()</div><div class="line"></div><div class="line">        #xpath返回的是包含一个元素的列表</div><div class="line">        item[&apos;name&apos;] = name[0]</div><div class="line">        item[&apos;title&apos;] = title[0]</div><div class="line">        item[&apos;info&apos;] = info[0]</div><div class="line"></div><div class="line">        #items.append(item)</div><div class="line"></div><div class="line">        #将获取的数据交给pipelines</div><div class="line">        yield item</div><div class="line"></div><div class="line">    # 返回数据，不经过pipeline</div><div class="line">    #return items</div></pre></td></tr></table></figure></p>
<h5 id="中文乱码转为utf-8"><a href="#中文乱码转为utf-8" class="headerlink" title="中文乱码转为utf-8"></a>中文乱码转为utf-8</h5><p>python3默认为unicode,如果输出为中文，则要转为utf-8，不然会是乱码<br>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">import codecs</div><div class="line">import os</div><div class="line"></div><div class="line">class Pipeline(object):</div><div class="line">    def __init__(self):</div><div class="line">        self.file = codecs.open(</div><div class="line">            &apos;items.json&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)</div><div class="line"></div><div class="line">    def close_spider(self, spider):</div><div class="line">        self.file.seek(-1, os.SEEK_END)</div><div class="line">        self.file.truncate()</div><div class="line">        self.file.close()</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        line = json.dumps(dict(item), ensure_ascii=False) + &quot;\n&quot;</div><div class="line">        self.file.write(line)</div><div class="line">        return item</div></pre></td></tr></table></figure></p>
<h5 id="imagepipeline各函数运行流程"><a href="#imagepipeline各函数运行流程" class="headerlink" title="imagepipeline各函数运行流程"></a>imagepipeline各函数运行流程</h5><ol>
<li>imagepipeline启动</li>
<li>get_media_requests 将所有的下载请求一次全部完成</li>
<li>下载完成后再统一执行item_completed</li>
</ol>
<h5 id="同时下载多个图片并改名"><a href="#同时下载多个图片并改名" class="headerlink" title="同时下载多个图片并改名"></a>同时下载多个图片并改名</h5><p>重写file_path函数实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">  def get_media_requests(self, item, info):</div><div class="line">      &quot;&quot;&quot;</div><div class="line">      :param item: spider.py中返回的item</div><div class="line">      :param info:</div><div class="line">      :return:</div><div class="line">      &quot;&quot;&quot;</div><div class="line">      #这里传递字符，或者图片列表，如果是单个的对象，则非常容易被覆盖</div><div class="line">      yield scrapy.Request(item[&apos;pic_url&apos;], meta=&#123;&apos;item&apos;: item[&apos;pic_name&apos;]&#125;)</div><div class="line"></div><div class="line">  def file_path(self, request, response=None, info=None):</div><div class="line">      &quot;&quot;&quot;</div><div class="line">      : param request: 每一个图片下载管道请求</div><div class="line">      : param response:</div><div class="line">      : param info:</div><div class="line">      : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</div><div class="line">      : return: 每套图的分类目录</div><div class="line">      &quot;&quot;&quot;</div><div class="line">      item = request.meta[&apos;item&apos;]</div><div class="line">      folder = item</div><div class="line">      folder_strip = strip(folder)</div><div class="line">      # img_path = &quot;%s%s&quot; % (self.img_store, folder_strip)</div><div class="line">      filename = folder_strip + &apos;/&apos; + folder_strip + &apos;.jpg&apos;</div><div class="line">      return filename</div><div class="line">      </div><div class="line">def strip(path):</div><div class="line">  &quot;&quot;&quot;</div><div class="line">  :param path: 需要清洗的文件夹名字</div><div class="line">  :return: 清洗掉Windows系统非法文件夹名字的字符串</div><div class="line">  &quot;&quot;&quot;</div><div class="line">  path = re.sub(r&apos;[？\\*|“&lt;&gt;:/]&apos;, &apos;&apos;, str(path))</div><div class="line">  return path</div></pre></td></tr></table></figure></p>
<h5 id="Request-回调传递参数"><a href="#Request-回调传递参数" class="headerlink" title="Request 回调传递参数"></a>Request 回调传递参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;&apos;item&apos;: item, &apos;param&apos;: name&#125;)</div><div class="line"></div><div class="line">在parse中提取参数</div><div class="line">item = response.meta[&apos;item&apos;]</div></pre></td></tr></table></figure>
<h5 id="结果去重"><a href="#结果去重" class="headerlink" title="结果去重"></a>结果去重</h5><ol>
<li>Request的参数 dont_filter=False 默认去重</li>
<li>启用一个爬虫的持久化，运行以下命令:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</div></pre></td></tr></table></figure>
</li>
</ol>
<p>然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。<br>恢复这个爬虫也是同样的命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</div></pre></td></tr></table></figure></p>
<p>这样爬虫断掉后，再启动会接着上次的 url 跑。</p>
<p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数<br>运行爬虫如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl spider1 -L WARNING</div></pre></td></tr></table></figure></p>
<p>不打印Debug信息，可以清楚得看到运行过程。</p>
<ol>
<li>scrapy-redis<h3 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h3><h4 id="pipeline-is-not-a-full-path"><a href="#pipeline-is-not-a-full-path" class="headerlink" title="pipeline is not a full path"></a>pipeline is not a full path</h4>应该在 setting 中填入完整的管道的路径，如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pic.pipelines.PicImagesDownloadPipeline</div></pre></td></tr></table></figure>
</li>
</ol>
<p>如果只填PicImagesDownloadPipeline,就会出现这个错误。</p>
<h4 id="Symbol-not-found-PyInt-AsLong-错误"><a href="#Symbol-not-found-PyInt-AsLong-错误" class="headerlink" title="Symbol not found:  _PyInt_AsLong 错误"></a>Symbol not found:  _PyInt_AsLong 错误</h4><p>将系统python目录下的PIL和Pillow库都删除，再用pip3安装在 Python3的安装目录下<br>系统python安装目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/Library/Python/2.7/site-packages</div></pre></td></tr></table></figure></p>
<h4 id="Missing-scheme-in-request-url-h"><a href="#Missing-scheme-in-request-url-h" class="headerlink" title="Missing scheme in request url: h"></a>Missing scheme in request url: h</h4><p>相关URL必须是一个List，所以遇到该错误只需要将url转换成list即可。<br>例如：<br>start_urls = [‘someurls’]<br>如果是images_url也是如此，使用item存储的时候改成list即可。<br>item[‘images_urls’] = [‘image_url’]</p>
<h4 id="Request-url-must-be-str-or-unicode"><a href="#Request-url-must-be-str-or-unicode" class="headerlink" title="Request url must be str or unicode"></a>Request url must be str or unicode</h4><p>请求的url参数不能是一个列表，必须是一个字符</p>
<h4 id="在item-complete中改名多个图片不成功"><a href="#在item-complete中改名多个图片不成功" class="headerlink" title="在item_complete中改名多个图片不成功"></a>在item_complete中改名多个图片不成功</h4><p>item_complete并不是在get_media_requests下载图片后马上启动的，它是要等所有的图片下载完成，再统一启动complete事件，这样就导致多个图片没法改名，不能获得之前的item的字段。改名需要重写file_path</p>
<h4 id="get-media-requests中回调参数要小心"><a href="#get-media-requests中回调参数要小心" class="headerlink" title="get_media_requests中回调参数要小心"></a>get_media_requests中回调参数要小心</h4><p>meta中可以加入回调的参数，如果传递的是对象要非常小心，如果对象发生变化，会导致后面所有的回调参数发生变化，传递的如果是字符，就没有这个风险。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def get_media_requests(self, item, info):</div><div class="line">       &quot;&quot;&quot;</div><div class="line">       :param item: spider.py中返回的item</div><div class="line">       :param info:</div><div class="line">       :return:</div><div class="line">       &quot;&quot;&quot;</div><div class="line">       yield scrapy.Request(item[&apos;pic_url&apos;], meta=&#123;&apos;item&apos;: item[&apos;pic_name&apos;]&#125;)</div></pre></td></tr></table></figure></p>
<h3 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h3><p>piczz.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">from piczz.items import PiczzItem</div><div class="line"></div><div class="line"></div><div class="line">class piczzSpider(scrapy.Spider):</div><div class="line">    name = &quot;piczz&quot;</div><div class="line">    allowed_domains = [&quot;&quot;]</div><div class="line">    start_urls = [&quot;&quot;]</div><div class="line">    img_paths = []</div><div class="line">    def parse(self, response):</div><div class="line"></div><div class="line">        for each in response.xpath(</div><div class="line">                &quot;//div[@class = &apos;post_box&apos;]&quot;):</div><div class="line">            # extract()方法返回的都是unicode字符串</div><div class="line">            item = PiczzItem()</div><div class="line">            item[&apos;name&apos;] = &apos;startpage&apos;</div><div class="line"></div><div class="line">            self.img_paths.clear()</div><div class="line">            item[&apos;pic_name&apos;] = each.xpath(</div><div class="line">                &quot;descendant::div[@class = &apos;tit&apos;]/h2[@class = &apos;h1&apos;]/a/text()&quot;).extract()[0]</div><div class="line">            item[&apos;pic_url&apos;] = each.xpath(</div><div class="line">                &quot;descendant::div[@class = &apos;tit&apos;]/h2[@class = &apos;h1&apos;]/a/@href&quot;).extract()[0]</div><div class="line"></div><div class="line">            yield scrapy.Request(item[&apos;pic_url&apos;],</div><div class="line">                                 callback=self.parse_imgs, meta=&#123;&apos;item&apos;: item&#125;)</div><div class="line"></div><div class="line">        #递归下一页图片</div><div class="line">        next_path = response.xpath(</div><div class="line">            &quot;descendant::div[@class = &apos;page_num&apos;]/a[last()]&quot;)</div><div class="line">        next_con = next_path.xpath(&quot;text()&quot;).extract()[0]</div><div class="line">        next_con = next_con.strip()</div><div class="line">        next_page = &quot;&quot;</div><div class="line">        if next_con == &quot;下一頁 »&quot;:</div><div class="line">            next_page = next_path.xpath(&quot;@href&quot;).extract()[0]</div><div class="line">            print(next_page)</div><div class="line">            if next_path is not None:</div><div class="line">                yield scrapy.Request(next_page, self.parse)</div><div class="line">        else:</div><div class="line">            return</div><div class="line">    </div><div class="line">    # 下载一个索引页的图片</div><div class="line">    def parse_imgs(self, response):</div><div class="line">        self.img_paths.clear()</div><div class="line">        item = response.meta[&apos;item&apos;]</div><div class="line">        imgs = response.xpath(</div><div class="line">            &quot;descendant::div[@class = &apos;entry-content&apos;]/p/img/@src&quot;).extract()</div><div class="line">        for e in imgs:</div><div class="line">            self.img_paths.append(e)</div><div class="line">        item[&apos;pic_paths&apos;] = self.img_paths</div><div class="line">        next_path = response.xpath(</div><div class="line">            &quot;descendant::div[@class = &apos;wp-pagenavi&apos;]/p/a[last()]&quot;)</div><div class="line">        next_con = next_path.xpath(&quot;text()&quot;).extract()[0]</div><div class="line">        next_con = next_con.strip()</div><div class="line">        if next_con == &quot;下一页&quot;:</div><div class="line">            next_page = next_path.xpath(&quot;@href&quot;).extract()[0]</div><div class="line">            if next_page is not None:</div><div class="line">                yield scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;&apos;item&apos;: item&#125;)</div><div class="line">        yield item</div></pre></td></tr></table></figure></p>
<p>item.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class PiczzItem(scrapy.Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    name = scrapy.Field()</div><div class="line">    pic_name = scrapy.Field()  # 图片目录名</div><div class="line">    pic_url = scrapy.Field()  # 图片索引首页地址</div><div class="line">    pic_paths = scrapy.Field()  # 图片下载地址列表</div></pre></td></tr></table></figure>
<p>pipeline.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">import shutil</div><div class="line">import codecs</div><div class="line">import os</div><div class="line">import re</div><div class="line">import scrapy</div><div class="line">import PIL</div><div class="line">from scrapy.pipelines.images import ImagesPipeline</div><div class="line">from scrapy.exceptions import DropItem</div><div class="line">from scrapy.utils.project import get_project_settings</div><div class="line"></div><div class="line">class PiczzImagesDownloadPipeline(ImagesPipeline):</div><div class="line"></div><div class="line">    def get_media_requests(self, item, info):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :param item: spider.py中返回的item</div><div class="line">        :param info:</div><div class="line">        :return:</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        for img_url in item[&apos;pic_paths&apos;]:</div><div class="line">            yield scrapy.Request(img_url, meta=&#123;&apos;item&apos;: item[&apos;pic_name&apos;]&#125;)</div><div class="line"></div><div class="line">    def file_path(self, request, response=None, info=None):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        : param request: 每一个图片下载管道请求</div><div class="line">        : param response:</div><div class="line">        : param info:</div><div class="line">        : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</div><div class="line">        : return: 每套图的分类目录</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        item = request.meta[&apos;item&apos;]</div><div class="line">        folder = item</div><div class="line">        folder_strip = strip(folder)</div><div class="line">        image_guid = request.url.split(&apos;/&apos;)[-1]</div><div class="line">        filename = folder_strip + &apos;/&apos; + image_guid + &apos;.jpg&apos;</div><div class="line">        return filename</div><div class="line"></div><div class="line">    def item_completed(self, results, item, info):</div><div class="line">        image_paths = [x[&apos;path&apos;] for ok, x in results if ok]</div><div class="line">        if not image_paths:</div><div class="line">            raise DropItem(&quot;Item contains no images&quot;)</div><div class="line">        return item</div><div class="line"></div><div class="line"></div><div class="line">def strip(path):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    :param path: 需要清洗的文件夹名字</div><div class="line">    :return: 清洗掉Windows系统非法文件夹名字的字符串</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    path = re.sub(r&apos;[？\\*|“&lt;&gt;:/]&apos;, &apos;&apos;, str(path))</div><div class="line">    return path</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从搭建环境到断断续续的学习花了大概五天时间 ，每天平均花二个小时学习，终于成功的将设定的目标完成。</p>
<h3 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h3><p><a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="external">官网</a><br><a href="https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html" target="_blank" rel="external">中文参考网站</a><br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="external">xPath语法</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="external">Python中yield的解释</a><br><a href="https://blog.csdn.net/a542551042/article/details/47149959" target="_blank" rel="external">mac os Python路径总结</a><br><a href="https://segmentfault.com/a/1190000013178839" target="_blank" rel="external">Scrapy框架入门简介</a><br><a href="https://segmentfault.com/a/1190000009597329" target="_blank" rel="external">ImagesPipeline下载图片</a><br><a href="https://segmentfault.com/q/1010000000413334" target="_blank" rel="external">ImagesPipeline下载图片保持原文件名</a><br><a href="https://cuiqingcai.com/4421.html" target="_blank" rel="external">小白进阶之Scrapy第四篇</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="external">Python中yield的解释</a><br><a href="https://blog.csdn.net/heheyanyanjun/article/details/79199378" target="_blank" rel="external">scrapy调用parse()中使用yield引发对yield的分析</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Scrapy/" rel="tag"># Scrapy</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/11/egret与cocos creator/" rel="next" title="egret与cocos creator">
                <i class="fa fa-chevron-left"></i> egret与cocos creator
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="BoomCode" />
          <p class="site-author-name" itemprop="name">BoomCode</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.baidu.com" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#目标"><span class="nav-number">1.</span> <span class="nav-text">目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程环境"><span class="nav-number">2.</span> <span class="nav-text">编程环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装记录"><span class="nav-number">3.</span> <span class="nav-text">安装记录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#win下安装"><span class="nav-number">3.1.</span> <span class="nav-text">win下安装</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#用pip命令安装Scrapy时提示没有MS框架"><span class="nav-number">3.1.1.</span> <span class="nav-text">用pip命令安装Scrapy时提示没有MS框架</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#提示没有安装win32api"><span class="nav-number">3.1.2.</span> <span class="nav-text">提示没有安装win32api</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#安装命令"><span class="nav-number">3.1.3.</span> <span class="nav-text">安装命令</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mac-下安装"><span class="nav-number">3.2.</span> <span class="nav-text">mac 下安装</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习记录"><span class="nav-number">4.</span> <span class="nav-text">学习记录</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#生成Scrapy框架"><span class="nav-number">4.0.1.</span> <span class="nav-text">生成Scrapy框架</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HelloWorld代码"><span class="nav-number">4.0.2.</span> <span class="nav-text">HelloWorld代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深入学习"><span class="nav-number">4.1.</span> <span class="nav-text">深入学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#例子1-提取内容"><span class="nav-number">4.1.1.</span> <span class="nav-text">例子1-提取内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#例子2-爬取下一个链接"><span class="nav-number">4.1.2.</span> <span class="nav-text">例子2-爬取下一个链接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#进阶例子"><span class="nav-number">4.1.3.</span> <span class="nav-text">进阶例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#命令行参数例子"><span class="nav-number">4.1.4.</span> <span class="nav-text">命令行参数例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#item"><span class="nav-number">4.1.5.</span> <span class="nav-text">item</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#item-pipeline"><span class="nav-number">4.1.6.</span> <span class="nav-text">item pipeline</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#中文乱码转为utf-8"><span class="nav-number">4.1.7.</span> <span class="nav-text">中文乱码转为utf-8</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#imagepipeline各函数运行流程"><span class="nav-number">4.1.8.</span> <span class="nav-text">imagepipeline各函数运行流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#同时下载多个图片并改名"><span class="nav-number">4.1.9.</span> <span class="nav-text">同时下载多个图片并改名</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Request-回调传递参数"><span class="nav-number">4.1.10.</span> <span class="nav-text">Request 回调传递参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#结果去重"><span class="nav-number">4.1.11.</span> <span class="nav-text">结果去重</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#错误记录"><span class="nav-number">5.</span> <span class="nav-text">错误记录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pipeline-is-not-a-full-path"><span class="nav-number">5.1.</span> <span class="nav-text">pipeline is not a full path</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Symbol-not-found-PyInt-AsLong-错误"><span class="nav-number">5.2.</span> <span class="nav-text">Symbol not found:  _PyInt_AsLong 错误</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Missing-scheme-in-request-url-h"><span class="nav-number">5.3.</span> <span class="nav-text">Missing scheme in request url: h</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Request-url-must-be-str-or-unicode"><span class="nav-number">5.4.</span> <span class="nav-text">Request url must be str or unicode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在item-complete中改名多个图片不成功"><span class="nav-number">5.5.</span> <span class="nav-text">在item_complete中改名多个图片不成功</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#get-media-requests中回调参数要小心"><span class="nav-number">5.6.</span> <span class="nav-text">get_media_requests中回调参数要小心</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最终代码"><span class="nav-number">6.</span> <span class="nav-text">最终代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考网站"><span class="nav-number">8.</span> <span class="nav-text">参考网站</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BoomCode</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  

  

  

  


  

</body>
</html>
