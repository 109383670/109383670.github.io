<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一点乐趣</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://109383670.github.io/"/>
  <updated>2019-03-04T15:52:07.389Z</updated>
  <id>https://109383670.github.io/</id>
  
  <author>
    <name>BoomCode</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cocos2d-x 3.0相对于2.0的变化</title>
    <link href="https://109383670.github.io/2019/03/02/cocos2d-x%203.0%E7%9B%B8%E5%AF%B9%E4%BA%8E2.0%E7%9A%84%E5%8F%98%E5%8C%96/"/>
    <id>https://109383670.github.io/2019/03/02/cocos2d-x 3.0相对于2.0的变化/</id>
    <published>2019-03-02T07:44:18.102Z</published>
    <updated>2019-03-04T15:52:07.389Z</updated>
    
    <content type="html"><![CDATA[<h3 id="更新步骤"><a href="#更新步骤" class="headerlink" title="更新步骤"></a>更新步骤</h3><ol><li>下载开发包（github也可以)。</li><li>运行build目录下，cocos2d_tests.xcodeproj工程。</li><li>将编译Target选择成cpp-tests iOS。</li><li>运行。</li><li>用命令行命令生成一个模板工程：<br><code>cocos new MyGame -p com.MyCompany.MyGame -l cpp -d ~/MyCompany</code><br>将2.0代码加入模板工程中。如果是3.0升级就用模板工程中的cocos2d代替老版本的文件夹。</li></ol><h3 id="CClog-变成CCLOG"><a href="#CClog-变成CCLOG" class="headerlink" title="CClog 变成CCLOG"></a>CClog 变成CCLOG</h3><h3 id="CCArray-变成Array-CCSet等也变成set"><a href="#CCArray-变成Array-CCSet等也变成set" class="headerlink" title="CCArray 变成Array, CCSet等也变成set"></a>CCArray 变成<strong>Array, CCSet等也变成</strong>set</h3><h3 id="CCLayer中setTouchEnabled无效"><a href="#CCLayer中setTouchEnabled无效" class="headerlink" title="CCLayer中setTouchEnabled无效"></a>CCLayer中setTouchEnabled无效</h3><p>添加触摸事件器后，自动生效。</p><h3 id="ccTouchBegan等改成onTouchesBegan"><a href="#ccTouchBegan等改成onTouchesBegan" class="headerlink" title="ccTouchBegan等改成onTouchesBegan"></a>ccTouchBegan等改成onTouchesBegan</h3><p>触摸事件代码<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> listener = EventListenerTouchAllAtOnce::create();</span><br><span class="line">listener-&gt;onTouchesBegan = CC_CALLBACK_2(ForceTouchTest::onTouchesBegan, <span class="keyword">this</span>);</span><br><span class="line">listener-&gt;onTouchesMoved = CC_CALLBACK_2(ForceTouchTest::onTouchesMoved, <span class="keyword">this</span>);</span><br><span class="line">listener-&gt;onTouchesEnded = CC_CALLBACK_2(ForceTouchTest::onTouchesEnded, <span class="keyword">this</span>);</span><br><span class="line">_eventDispatcher-&gt;addEventListenerWithSceneGraphPriority(listener, <span class="keyword">this</span>);</span><br></pre></td></tr></table></figure></p><h3 id="CCObject改成Ref"><a href="#CCObject改成Ref" class="headerlink" title="CCObject改成Ref"></a>CCObject改成Ref</h3><h3 id="CCPoint改成Vec2"><a href="#CCPoint改成Vec2" class="headerlink" title="CCPoint改成Vec2"></a>CCPoint改成Vec2</h3><h3 id="CCJumpTO等动作改成JumpTo"><a href="#CCJumpTO等动作改成JumpTo" class="headerlink" title="CCJumpTO等动作改成JumpTo"></a>CCJumpTO等动作改成JumpTo</h3><h3 id="CCPointZero改成Vec2-Zero"><a href="#CCPointZero改成Vec2-Zero" class="headerlink" title="CCPointZero改成Vec2::Zero"></a>CCPointZero改成Vec2::Zero</h3><p>除了0向量外，还有很多的静态值。</p><h3 id="动作回调CallFuncN有改动"><a href="#动作回调CallFuncN有改动" class="headerlink" title="动作回调CallFuncN有改动"></a>动作回调CallFuncN有改动</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ac_go =  Sequence::create(ac_p, ac_s, ac_s1,</span><br><span class="line">CallFuncN::create(<span class="keyword">this</span>, callfuncN_selector(BlockBoard::call_onBeginAction)),</span><br><span class="line"><span class="literal">nullptr</span>);</span><br><span class="line"><span class="comment">//改成：</span></span><br><span class="line">ac_go = Sequence::create(ac_p,ac_s,ac_s1,CallFuncN::create(CC_CALLBACK_1(BlockBoard::call_onBeginAction,<span class="keyword">this</span>)),</span><br><span class="line"><span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure><p>CC_CALLBACK_1 CC_CALLBACK_2 CC_CALLBACK_3<br>后面的数字表示带参数的多少。</p><h3 id="ssize-t格式化参数-zd"><a href="#ssize-t格式化参数-zd" class="headerlink" title="ssize_t格式化参数%zd"></a>ssize_t格式化参数%zd</h3><p>ssize_t：有符号整数，与平台无关。适配不同平台的通用整数关键字，在32位平台为32，64位平台为long。<br>size_t：无符号整数，与平台无关。<br>ssize_t格式化参数是%zd, size_t是%tu。</p><h3 id="GameCenter-ReportScore有更新"><a href="#GameCenter-ReportScore有更新" class="headerlink" title="GameCenter ReportScore有更新"></a>GameCenter ReportScore有更新</h3><p>代码如下：<br><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>) reportScore: (int64_t) score forCategory: (<span class="built_in">NSString</span>*) category</span><br><span class="line">&#123;</span><br><span class="line">    GKScore *scoreReporter = [[[GKScore alloc] initWithLeaderboardIdentifier:category] autorelease];</span><br><span class="line">    scoreReporter.value = score;</span><br><span class="line">    scoreReporter.context = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">NSArray</span> *scores = @[scoreReporter];</span><br><span class="line">    [GKScore reportScores:scores withCompletionHandler:^(<span class="built_in">NSError</span> *error) &#123;</span><br><span class="line">        <span class="comment">//Do something interesting here.</span></span><br><span class="line">        <span class="keyword">if</span> (error != <span class="literal">nil</span>)&#123;</span><br><span class="line">            <span class="comment">// handle the reporting error</span></span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"上传分数出错."</span>);</span><br><span class="line"></span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"上传分数成功"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Size会有重名错误，用cocos2d-Size"><a href="#Size会有重名错误，用cocos2d-Size" class="headerlink" title="Size会有重名错误，用cocos2d::Size"></a>Size会有重名错误，用cocos2d::Size</h3><h3 id="MenuItemImage-有变化"><a href="#MenuItemImage-有变化" class="headerlink" title="MenuItemImage 有变化"></a>MenuItemImage 有变化</h3><p>代码<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MenuItemImage* item_close = MenuItemImage::create(<span class="string">"close.png"</span>, <span class="string">"close_p.png"</span>,  CC_CALLBACK_1(InfoShowLayer::onClose, <span class="keyword">this</span>));</span><br></pre></td></tr></table></figure></p><h3 id="向量操作"><a href="#向量操作" class="headerlink" title="向量操作"></a>向量操作</h3><p>Vec2可以直接数学符号操作</p><h3 id="Label-Font"><a href="#Label-Font" class="headerlink" title="Label Font"></a>Label Font</h3><p>TTF, 系统字体，BMFont，都由Label类负责创建<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Label* label = Label::createWithSystemFont(str-&gt;getCString(), <span class="string">"Mark Felt"</span>, fontSize);</span><br><span class="line">Label::createWithTTF</span><br><span class="line">Label::createWithBMFont</span><br><span class="line">Label::createWithCharMap</span><br></pre></td></tr></table></figure></p><h3 id="设置资源路径已经适配模式"><a href="#设置资源路径已经适配模式" class="headerlink" title="设置资源路径已经适配模式"></a>设置资源路径已经适配模式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">searchPath.push_back(<span class="string">"iphone"</span>);</span><br><span class="line">glview&gt;setDesignResolutionSize(<span class="number">750</span>,<span class="number">1334</span>,ResolutionPolicy::FIXED_HEIGHT);</span><br></pre></td></tr></table></figure><h3 id="启动图片和应用图标都有更新"><a href="#启动图片和应用图标都有更新" class="headerlink" title="启动图片和应用图标都有更新"></a>启动图片和应用图标都有更新</h3><p>启动图片决定了初始分辨率的大小。不提供相应的启动图片，不能获得正确的初始分辨率。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.veryitman.com/2018/02/22/NSLog-格式化输出-NSInteger-NSUInteger/" target="_blank" rel="noopener">常用格式化参数</a><br><a href="https://elloop.github.io/cocos2d-x/2016-01-01/cocos2dx-3.x-4-cc-callback-0" target="_blank" rel="noopener">CC_CALLBACK_0, CC_CALLBACK_1, CC_CALLBACK_2, CC_CALLBACK_3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;更新步骤&quot;&gt;&lt;a href=&quot;#更新步骤&quot; class=&quot;headerlink&quot; title=&quot;更新步骤&quot;&gt;&lt;/a&gt;更新步骤&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;下载开发包（github也可以)。&lt;/li&gt;
&lt;li&gt;运行build目录下，cocos2d_tests.xcod
      
    
    </summary>
    
    
      <category term="cocos2d-x" scheme="https://109383670.github.io/tags/cocos2d-x/"/>
    
      <category term="游戏开发" scheme="https://109383670.github.io/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy爬虫项目纪录</title>
    <link href="https://109383670.github.io/2019/02/20/Scrapy%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E7%BA%AA%E5%BD%95/"/>
    <id>https://109383670.github.io/2019/02/20/Scrapy爬虫项目纪录/</id>
    <published>2019-02-19T16:46:54.381Z</published>
    <updated>2019-03-03T08:54:03.522Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。</p><h3 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h3><ul><li>VSCode</li><li>Python3</li><li>Scrapy</li></ul><h3 id="安装记录"><a href="#安装记录" class="headerlink" title="安装记录"></a>安装记录</h3><h4 id="win下安装"><a href="#win下安装" class="headerlink" title="win下安装"></a>win下安装</h4><h5 id="用pip命令安装Scrapy时提示没有MS框架"><a href="#用pip命令安装Scrapy时提示没有MS框架" class="headerlink" title="用pip命令安装Scrapy时提示没有MS框架"></a>用pip命令安装Scrapy时提示没有MS框架</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">安装MS Build TOOL</span><br></pre></td></tr></table></figure><h5 id="提示没有安装win32api"><a href="#提示没有安装win32api" class="headerlink" title="提示没有安装win32api"></a>提示没有安装win32api</h5><p>用pip 安装win32：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure><h5 id="安装命令"><a href="#安装命令" class="headerlink" title="安装命令"></a>安装命令</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure><p>更新命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install --upgrade scrapy</span><br></pre></td></tr></table></figure></p><h4 id="mac-下安装"><a href="#mac-下安装" class="headerlink" title="mac 下安装"></a>mac 下安装</h4><p>mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。<br>mac下用Homebrew来进行升级</p><ol><li><p>安装xcode命令行工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure></li><li><p><a href="https://brew.sh/" target="_blank" rel="noopener">https://brew.sh/</a> 安装Homebrew</p></li><li><p>将Homebrew加入环境变量中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export PATH=/usr/local/bin:/usr/local/sbin:<span class="variable">$PATH</span>"</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p>安装python</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install python</span><br></pre></td></tr></table></figure><p> 如果已经安装，可以进行升级</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew update; brew upgrade python</span><br></pre></td></tr></table></figure><ol start="5"><li>安装scrapy<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install scrapy</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="学习记录"><a href="#学习记录" class="headerlink" title="学习记录"></a>学习记录</h3><h4 id="生成Scrapy框架"><a href="#生成Scrapy框架" class="headerlink" title="生成Scrapy框架"></a>生成Scrapy框架</h4><p>SCrapy必须在固定的框架下运行，可以自动生成后再去改动。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 工程名</span><br></pre></td></tr></table></figure></p><h4 id="HelloWorld代码"><a href="#HelloWorld代码" class="headerlink" title="HelloWorld代码"></a>HelloWorld代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment"># 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">"quotes"</span>    <span class="comment"># 唯一的爬虫名字，在运行时要用到</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span>    <span class="comment"># 复写的方法，初始请求的网址</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span>       <span class="comment"># 复写的方法，在这里对爬下的数据进行处理</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure><p>运行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure> <a id="more"></a><h4 id="深入学习"><a href="#深入学习" class="headerlink" title="深入学习"></a>深入学习</h4><h5 id="例子1-提取内容"><a href="#例子1-提取内容" class="headerlink" title="例子1-提取内容"></a>例子1-提取内容</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取相关格言以及作者等信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">                <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>输出json或者jl(JSON Lines)命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.json</span><br><span class="line"></span><br><span class="line">scrapy crawl quotes -o quotes.jl</span><br></pre></td></tr></table></figure></p><h5 id="例子2-爬取下一个链接"><a href="#例子2-爬取下一个链接" class="headerlink" title="例子2-爬取下一个链接"></a>例子2-爬取下一个链接</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">                <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            next_page = response.urljoin(next_page)     <span class="comment">#获得真实的链接地址</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse)  <span class="comment">#下一个链接的处理回调</span></span><br></pre></td></tr></table></figure><p>后面两句可以用下面的代替，不用写urljoin了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> response.follow(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure><p>进一步简化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">    <span class="keyword">yield</span> response.follow(href, callback=self.parse)</span><br></pre></td></tr></table></figure><p>再进一步简化：<br>对于a 标签，会自动使用它的href属性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> response.css(<span class="string">'li.next a'</span>):</span><br><span class="line">    <span class="keyword">yield</span> response.follow(a, callback=self.parse)</span><br></pre></td></tr></table></figure></p><h5 id="进阶例子"><a href="#进阶例子" class="headerlink" title="进阶例子"></a>进阶例子</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthorSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'author'</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># follow links to author pages</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'.author + a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse_author)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># follow pagination links</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_author</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">extract_with_css</span><span class="params">(query)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">''</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'name'</span>: extract_with_css(<span class="string">'h3.author-title::text'</span>),</span><br><span class="line">            <span class="string">'birthdate'</span>: extract_with_css(<span class="string">'.author-born-date::text'</span>),</span><br><span class="line">            <span class="string">'bio'</span>: extract_with_css(<span class="string">'.author-description::text'</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h5 id="命令行参数例子"><a href="#命令行参数例子" class="headerlink" title="命令行参数例子"></a>命令行参数例子</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        url = <span class="string">'http://quotes.toscrape.com/'</span></span><br><span class="line">        tag = getattr(self, <span class="string">'tag'</span>, <span class="literal">None</span>)    <span class="comment">#从命令行参数获得</span></span><br><span class="line">        <span class="keyword">if</span> tag <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            url = url + <span class="string">'tag/'</span> + tag</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">                <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br></pre></td></tr></table></figure><p>命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes-humor.json -a tag=humor</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://quotes.toscrape.com/tag/humor</span><br></pre></td></tr></table></figure><h5 id="item"><a href="#item" class="headerlink" title="item"></a>item</h5><p>可以自己定义的数据结构<br>格式如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br></pre></td></tr></table></figure></p><h5 id="item-pipeline"><a href="#item-pipeline" class="headerlink" title="item pipeline"></a>item pipeline</h5><p>处理item数据的地方，在parse中返回item,就会调用该方法。<br>格式如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item.get(<span class="string">'price'</span>):</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'price_excludes_vat'</span>):</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>在setting里启动pipeline<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,   <span class="comment">#数字表示优先顺序，越小的越先执行</span></span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> ItcastItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment">#open("teacher.html","wb").write(response.body).close()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存放老师信息的集合</span></span><br><span class="line">    <span class="comment">#items = []</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='li_txt']"</span>):</span><br><span class="line">        <span class="comment"># 将我们得到的数据封装到一个 `ItcastItem` 对象</span></span><br><span class="line">        item = ItcastItem()</span><br><span class="line">        <span class="comment">#extract()方法返回的都是unicode字符串</span></span><br><span class="line">        name = each.xpath(<span class="string">"h3/text()"</span>).extract()</span><br><span class="line">        title = each.xpath(<span class="string">"h4/text()"</span>).extract()</span><br><span class="line">        info = each.xpath(<span class="string">"p/text()"</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#xpath返回的是包含一个元素的列表</span></span><br><span class="line">        item[<span class="string">'name'</span>] = name[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'title'</span>] = title[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'info'</span>] = info[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#items.append(item)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将获取的数据交给pipelines</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据，不经过pipeline</span></span><br><span class="line">    <span class="comment">#return items</span></span><br></pre></td></tr></table></figure></p><h5 id="中文乱码转为utf-8"><a href="#中文乱码转为utf-8" class="headerlink" title="中文乱码转为utf-8"></a>中文乱码转为utf-8</h5><p>python3默认为unicode,如果输出为中文，则要转为utf-8，不然会是乱码<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(</span><br><span class="line">            <span class="string">'items.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.seek(<span class="number">-1</span>, os.SEEK_END)</span><br><span class="line">        self.file.truncate()</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><h5 id="imagepipeline各函数运行流程"><a href="#imagepipeline各函数运行流程" class="headerlink" title="imagepipeline各函数运行流程"></a>imagepipeline各函数运行流程</h5><ol><li>imagepipeline启动</li><li>get_media_requests 将所有的下载请求一次全部完成</li><li>下载完成后再统一执行item_completed</li></ol><h5 id="同时下载多个图片并改名"><a href="#同时下载多个图片并改名" class="headerlink" title="同时下载多个图片并改名"></a>同时下载多个图片并改名</h5><p>重写file_path函数实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">      :param info:</span></span><br><span class="line"><span class="string">      :return:</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">      <span class="comment">#这里传递字符，或者图片列表，如果是单个的对象，则非常容易被覆盖</span></span><br><span class="line">      <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>], meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      : param request: 每一个图片下载管道请求</span></span><br><span class="line"><span class="string">      : param response:</span></span><br><span class="line"><span class="string">      : param info:</span></span><br><span class="line"><span class="string">      : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</span></span><br><span class="line"><span class="string">      : return: 每套图的分类目录</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">      item = request.meta[<span class="string">'item'</span>]</span><br><span class="line">      folder = item</span><br><span class="line">      folder_strip = strip(folder)</span><br><span class="line">      <span class="comment"># img_path = "%s%s" % (self.img_store, folder_strip)</span></span><br><span class="line">      filename = folder_strip + <span class="string">'/'</span> + folder_strip + <span class="string">'.jpg'</span></span><br><span class="line">      <span class="keyword">return</span> filename</span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(path)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  :param path: 需要清洗的文件夹名字</span></span><br><span class="line"><span class="string">  :return: 清洗掉Windows系统非法文件夹名字的字符串</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  path = re.sub(<span class="string">r'[？\\*|“&lt;&gt;:/]'</span>, <span class="string">''</span>, str(path))</span><br><span class="line">  <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure></p><h5 id="Request-回调传递参数"><a href="#Request-回调传递参数" class="headerlink" title="Request 回调传递参数"></a>Request 回调传递参数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item, <span class="string">'param'</span>: name&#125;)</span><br><span class="line"></span><br><span class="line">在parse中提取参数</span><br><span class="line">item = response.meta[<span class="string">'item'</span>]</span><br></pre></td></tr></table></figure><h5 id="结果去重"><a href="#结果去重" class="headerlink" title="结果去重"></a>结果去重</h5><ol><li>Request的参数 dont_filter=False 默认去重</li><li>启用一个爬虫的持久化，运行以下命令:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</span><br></pre></td></tr></table></figure></li></ol><p>然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。<br>恢复这个爬虫也是同样的命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl somespider -s JOBDIR=crawls/somespider-1</span><br></pre></td></tr></table></figure></p><p>这样爬虫断掉后，再启动会接着上次的 url 跑。</p><p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数<br>运行爬虫如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spider1 -L WARNING</span><br></pre></td></tr></table></figure></p><p>不打印Debug信息，可以清楚得看到运行过程。</p><ol><li>scrapy-red</li></ol><h3 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h3><h4 id="pipeline-is-not-a-full-path"><a href="#pipeline-is-not-a-full-path" class="headerlink" title="pipeline is not a full path"></a>pipeline is not a full path</h4><p>应该在 setting 中填入完整的管道的路径，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pic.pipelines.PicImagesDownloadPipeline</span><br></pre></td></tr></table></figure></p><p>如果只填PicImagesDownloadPipeline,就会出现这个错误。</p><h4 id="Symbol-not-found-PyInt-AsLong-错误"><a href="#Symbol-not-found-PyInt-AsLong-错误" class="headerlink" title="Symbol not found:  _PyInt_AsLong 错误"></a>Symbol not found:  _PyInt_AsLong 错误</h4><p>将系统python目录下的PIL和Pillow库都删除，再用pip3安装在 Python3的安装目录下<br>系统python安装目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Library/Python/2.7/site-packages</span><br></pre></td></tr></table></figure></p><h4 id="Missing-scheme-in-request-url-h"><a href="#Missing-scheme-in-request-url-h" class="headerlink" title="Missing scheme in request url: h"></a>Missing scheme in request url: h</h4><p>相关URL必须是一个List，所以遇到该错误只需要将url转换成list即可。<br>例如：<br>start_urls = [‘someurls’]<br>如果是images_url也是如此，使用item存储的时候改成list即可。<br>item[‘images_urls’] = [‘image_url’]</p><h4 id="Request-url-must-be-str-or-unicode"><a href="#Request-url-must-be-str-or-unicode" class="headerlink" title="Request url must be str or unicode"></a>Request url must be str or unicode</h4><p>请求的url参数不能是一个列表，必须是一个字符</p><h4 id="在item-complete中改名多个图片不成功"><a href="#在item-complete中改名多个图片不成功" class="headerlink" title="在item_complete中改名多个图片不成功"></a>在item_complete中改名多个图片不成功</h4><p>item_complete并不是在get_media_requests下载图片后马上启动的，它是要等所有的图片下载完成，再统一启动complete事件，这样就导致多个图片没法改名，不能获得之前的item的字段。改名需要重写file_path</p><h4 id="get-media-requests中回调参数要小心"><a href="#get-media-requests中回调参数要小心" class="headerlink" title="get_media_requests中回调参数要小心"></a>get_media_requests中回调参数要小心</h4><p>meta中可以加入回调的参数，如果传递的是对象要非常小心，如果对象发生变化，会导致后面所有的回调参数发生变化，传递的如果是字符，就没有这个风险。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">       :param info:</span></span><br><span class="line"><span class="string">       :return:</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>], meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br></pre></td></tr></table></figure></p><h4 id="Filtered-duplicate-request"><a href="#Filtered-duplicate-request" class="headerlink" title="Filtered duplicate request"></a>Filtered duplicate request</h4><p>有重复下载的请求，如果要重复下载，在<em>Request函数</em>里加上参数 <em>dont_filter=True</em>，默认是<em>False</em></p><h3 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h3><p>piczz.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> piczz.items <span class="keyword">import</span> PiczzItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">piczzSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"piczz"</span></span><br><span class="line">    allowed_domains = [<span class="string">""</span>]</span><br><span class="line">    start_urls = [<span class="string">""</span>]</span><br><span class="line">    img_paths = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(</span><br><span class="line">                <span class="string">"//div[@class = 'post_box']"</span>):</span><br><span class="line">            <span class="comment"># extract()方法返回的都是unicode字符串</span></span><br><span class="line">            item = PiczzItem()</span><br><span class="line">            item[<span class="string">'name'</span>] = <span class="string">'startpage'</span></span><br><span class="line"></span><br><span class="line">            self.img_paths.clear()</span><br><span class="line">            item[<span class="string">'pic_name'</span>] = each.xpath(</span><br><span class="line">                <span class="string">"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            item[<span class="string">'pic_url'</span>] = each.xpath(</span><br><span class="line">                <span class="string">"descendant::div[@class = 'tit']/h2[@class = 'h1']/a/@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'pic_url'</span>],</span><br><span class="line">                                 callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#递归下一页图片</span></span><br><span class="line">        next_path = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'page_num']/a[last()]"</span>)</span><br><span class="line">        next_con = next_path.xpath(<span class="string">"text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        next_con = next_con.strip()</span><br><span class="line">        next_page = <span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> next_con == <span class="string">"下一頁 »"</span>:</span><br><span class="line">            next_page = next_path.xpath(<span class="string">"@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            print(next_page)</span><br><span class="line">            <span class="keyword">if</span> next_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(next_page, self.parse)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 下载一个索引页的图片</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_imgs</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.img_paths.clear()</span><br><span class="line">        item = response.meta[<span class="string">'item'</span>]</span><br><span class="line">        imgs = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'entry-content']/p/img/@src"</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> imgs:</span><br><span class="line">            self.img_paths.append(e)</span><br><span class="line">        item[<span class="string">'pic_paths'</span>] = self.img_paths</span><br><span class="line">        next_path = response.xpath(</span><br><span class="line">            <span class="string">"descendant::div[@class = 'wp-pagenavi']/p/a[last()]"</span>)</span><br><span class="line">        next_con = next_path.xpath(<span class="string">"text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        next_con = next_con.strip()</span><br><span class="line">        <span class="keyword">if</span> next_con == <span class="string">"下一页"</span>:</span><br><span class="line">            next_page = next_path.xpath(<span class="string">"@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse_imgs, meta=&#123;<span class="string">'item'</span>: item&#125;)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><p>item.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PiczzItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    pic_name = scrapy.Field()  <span class="comment"># 图片目录名</span></span><br><span class="line">    pic_url = scrapy.Field()  <span class="comment"># 图片索引首页地址</span></span><br><span class="line">    pic_paths = scrapy.Field()  <span class="comment"># 图片下载地址列表</span></span><br></pre></td></tr></table></figure><p>pipeline.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PiczzImagesDownloadPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param item: spider.py中返回的item</span></span><br><span class="line"><span class="string">        :param info:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> img_url <span class="keyword">in</span> item[<span class="string">'pic_paths'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(img_url, meta=&#123;<span class="string">'item'</span>: item[<span class="string">'pic_name'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        : param request: 每一个图片下载管道请求</span></span><br><span class="line"><span class="string">        : param response:</span></span><br><span class="line"><span class="string">        : param info:</span></span><br><span class="line"><span class="string">        : param strip: 清洗Windows系统的文件夹非法字符，避免无法创建目录</span></span><br><span class="line"><span class="string">        : return: 每套图的分类目录</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        item = request.meta[<span class="string">'item'</span>]</span><br><span class="line">        folder = item</span><br><span class="line">        folder_strip = strip(folder)</span><br><span class="line">        image_guid = request.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        filename = folder_strip + <span class="string">'/'</span> + image_guid + <span class="string">'.jpg'</span></span><br><span class="line">        <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Item contains no images"</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param path: 需要清洗的文件夹名字</span></span><br><span class="line"><span class="string">    :return: 清洗掉Windows系统非法文件夹名字的字符串</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    path = re.sub(<span class="string">r'[？\\*|“&lt;&gt;:/]'</span>, <span class="string">''</span>, str(path))</span><br><span class="line">    <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从搭建环境到断断续续的学习花了大概五天时间 ，每天平均花二个小时学习，终于成功的将设定的目标完成。</p><h3 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h3><p><a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="noopener">官网</a><br><a href="https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html" target="_blank" rel="noopener">中文参考网站</a><br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">xPath语法</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="noopener">Python中yield的解释</a><br><a href="https://blog.csdn.net/a542551042/article/details/47149959" target="_blank" rel="noopener">mac os Python路径总结</a><br><a href="https://segmentfault.com/a/1190000013178839" target="_blank" rel="noopener">Scrapy框架入门简介</a><br><a href="https://segmentfault.com/a/1190000009597329" target="_blank" rel="noopener">ImagesPipeline下载图片</a><br><a href="https://segmentfault.com/q/1010000000413334" target="_blank" rel="noopener">ImagesPipeline下载图片保持原文件名</a><br><a href="https://cuiqingcai.com/4421.html" target="_blank" rel="noopener">小白进阶之Scrapy第四篇</a><br><a href="http://python.jobbole.com/83610/" target="_blank" rel="noopener">Python中yield的解释</a><br><a href="https://blog.csdn.net/heheyanyanjun/article/details/79199378" target="_blank" rel="noopener">scrapy调用parse()中使用yield引发对yield的分析</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h3&gt;&lt;p&gt;从零开始学习scrapy，从搭建环境到完成一个图片网站爬取实例。&lt;/p&gt;
&lt;h3 id=&quot;编程环境&quot;&gt;&lt;a href=&quot;#编程环境&quot; class=&quot;headerlink&quot; title=&quot;编程环境&quot;&gt;&lt;/a&gt;编程环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;VSCode&lt;/li&gt;
&lt;li&gt;Python3&lt;/li&gt;
&lt;li&gt;Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;安装记录&quot;&gt;&lt;a href=&quot;#安装记录&quot; class=&quot;headerlink&quot; title=&quot;安装记录&quot;&gt;&lt;/a&gt;安装记录&lt;/h3&gt;&lt;h4 id=&quot;win下安装&quot;&gt;&lt;a href=&quot;#win下安装&quot; class=&quot;headerlink&quot; title=&quot;win下安装&quot;&gt;&lt;/a&gt;win下安装&lt;/h4&gt;&lt;h5 id=&quot;用pip命令安装Scrapy时提示没有MS框架&quot;&gt;&lt;a href=&quot;#用pip命令安装Scrapy时提示没有MS框架&quot; class=&quot;headerlink&quot; title=&quot;用pip命令安装Scrapy时提示没有MS框架&quot;&gt;&lt;/a&gt;用pip命令安装Scrapy时提示没有MS框架&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;安装MS Build TOOL&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h5 id=&quot;提示没有安装win32api&quot;&gt;&lt;a href=&quot;#提示没有安装win32api&quot; class=&quot;headerlink&quot; title=&quot;提示没有安装win32api&quot;&gt;&lt;/a&gt;提示没有安装win32api&lt;/h5&gt;&lt;p&gt;用pip 安装win32：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install pywin32&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h5 id=&quot;安装命令&quot;&gt;&lt;a href=&quot;#安装命令&quot; class=&quot;headerlink&quot; title=&quot;安装命令&quot;&gt;&lt;/a&gt;安装命令&lt;/h5&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install scrapy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;更新命令&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo pip install --upgrade scrapy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4 id=&quot;mac-下安装&quot;&gt;&lt;a href=&quot;#mac-下安装&quot; class=&quot;headerlink&quot; title=&quot;mac 下安装&quot;&gt;&lt;/a&gt;mac 下安装&lt;/h4&gt;&lt;p&gt;mac 自带的python是2.7版本的，而且不能升级，否则会影响系统的功能。&lt;br&gt;mac下用Homebrew来进行升级&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;安装xcode命令行工具&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;xcode-select --install&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://brew.sh/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://brew.sh/&lt;/a&gt; 安装Homebrew&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将Homebrew加入环境变量中&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;export PATH=/usr/local/bin:/usr/local/sbin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装python&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;brew install python&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt; 如果已经安装，可以进行升级&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;brew update; brew upgrade python&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;安装scrapy&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip3 install scrapy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;学习记录&quot;&gt;&lt;a href=&quot;#学习记录&quot; class=&quot;headerlink&quot; title=&quot;学习记录&quot;&gt;&lt;/a&gt;学习记录&lt;/h3&gt;&lt;h4 id=&quot;生成Scrapy框架&quot;&gt;&lt;a href=&quot;#生成Scrapy框架&quot; class=&quot;headerlink&quot; title=&quot;生成Scrapy框架&quot;&gt;&lt;/a&gt;生成Scrapy框架&lt;/h4&gt;&lt;p&gt;SCrapy必须在固定的框架下运行，可以自动生成后再去改动。&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy startproject 工程名&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4 id=&quot;HelloWorld代码&quot;&gt;&lt;a href=&quot;#HelloWorld代码&quot; class=&quot;headerlink&quot; title=&quot;HelloWorld代码&quot;&gt;&lt;/a&gt;HelloWorld代码&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; scrapy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;QuotesSpider&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(scrapy.Spider)&lt;/span&gt;:&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 任何爬虫都要继承Scrapy.Spider这个类，复写它的方法&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    name = &lt;span class=&quot;string&quot;&gt;&quot;quotes&quot;&lt;/span&gt;    &lt;span class=&quot;comment&quot;&gt;# 唯一的爬虫名字，在运行时要用到&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;start_requests&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;    &lt;span class=&quot;comment&quot;&gt;# 复写的方法，初始请求的网址&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        urls = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;string&quot;&gt;&#39;http://quotes.toscrape.com/page/1/&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;string&quot;&gt;&#39;http://quotes.toscrape.com/page/2/&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; url &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; urls:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;yield&lt;/span&gt; scrapy.Request(url=url, callback=self.parse)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, response)&lt;/span&gt;:&lt;/span&gt;       &lt;span class=&quot;comment&quot;&gt;# 复写的方法，在这里对爬下的数据进行处理&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        page = response.url.split(&lt;span class=&quot;string&quot;&gt;&quot;/&quot;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;-2&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        filename = &lt;span class=&quot;string&quot;&gt;&#39;quotes-%s.html&#39;&lt;/span&gt; % page&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;with&lt;/span&gt; open(filename, &lt;span class=&quot;string&quot;&gt;&#39;wb&#39;&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; f:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            f.write(response.body)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.log(&lt;span class=&quot;string&quot;&gt;&#39;Saved file %s&#39;&lt;/span&gt; % filename)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;运行命令：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy crawl quotes&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Scrapy" scheme="https://109383670.github.io/tags/Scrapy/"/>
    
      <category term="Python" scheme="https://109383670.github.io/tags/Python/"/>
    
      <category term="爬虫" scheme="https://109383670.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>egret与cocos creator</title>
    <link href="https://109383670.github.io/2017/03/11/egret%E4%B8%8Ecocos%20creator/"/>
    <id>https://109383670.github.io/2017/03/11/egret与cocos creator/</id>
    <published>2017-03-11T08:18:43.000Z</published>
    <updated>2017-03-11T09:04:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>花了点时间分别体验了两个html5游戏制作工具：egret与cocos2d creator。<br>为了更好的对比，同时用两个工具做一个简单的射击demo。<br>使用后的体会：</p><h2 id="egret-（白鹭）"><a href="#egret-（白鹭）" class="headerlink" title="egret （白鹭）"></a>egret （白鹭）</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>开发工具齐全，从龙骨、粒子系统、资源管理、编译器等，基本上你需要的都提供了，省去了四处去找第三方工具库的麻烦，这个非常好。  </li><li>开发用的是TypeScript语言。在没有任何基础的情况下，我边看例子，边学习TypeScript, 没有什么大的障碍。调试功能也整合得很好。  </li><li>用exml进行UI可视化，直观，上手容易。</li><li>提供云测试空间，可以直接发布免费提供的ft空间进行测试。想到真是太周到了，这是一条龙服务的节奏。</li></ol><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ol><li>教程文档混乱，调试一个问题花了几个小时没找到原因，结果发现是一个类文件没有放在src的文件夹下，这么重要的东西，竟然在文档中没任何提及。</li><li>exml这样的ui方式，对于初学者来说太过复杂了，刚开始学习的人，估计一头雾水，加上文档有混乱，入坑门槛比较高。</li><li>在物理系统，碰撞系统的支持上，比较弱，可能html5游戏应该也用不到这么复杂的功能。</li></ol><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>如果要开发html5游戏，之前用过cocos2d-x之类的，没有用过unity开发的，可以用egret。如果要进行大规模商业开发的，也建议用这个，各个功能比较完善，是成熟的产品。</p><a id="more"></a><h2 id="coco2d-creator"><a href="#coco2d-creator" class="headerlink" title="coco2d creator"></a>coco2d creator</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol><li>文档、教程详细，写的非常好，基本上你想知道的全部有。从零开始到完成helloworld, 一套龙。教程友好，文档规范合理，用起来太舒服了。</li><li>用javascript进行开发，基于数据驱动的组件思想，入手快，开发直观。</li><li>对碰撞、地图的支持很好。</li></ol><h3 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h3><ol><li>vscode的代码提示基本没用，得一个一个查文档。</li><li>调试功能差，估计调试错误花费的时间比较高。</li><li>有些功能还不完善。</li></ol><h3 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h3><p>如果是之前用过unity开发的，那就太舒服了，基本上可以一边看文档一边做，没什么难度，上手快，用来做小游戏应该很爽。</p><h2 id="对比总结："><a href="#对比总结：" class="headerlink" title="对比总结："></a>对比总结：</h2><p>cocos2d creator就是模仿unity，打造一个轻型的html5制作工具。不得不说击中了unity在html5开发上的弱点。很看好cocos2d creator，相信在王哲的带领下，功能会越来越多，工具也会越来越完善。<br>cocos2d creator虽然以cocos2d-x为底层，但是有意思的是，egret才是以代码为驱动，用代码控制一切，cocos2d creator以数据为驱动，用组件的方式，跟unity一致。<br>egret像cocos2d-x, cocos2d creator像unity。<br>两个工具各有千秋，一个是以代码驱动，一个以数据驱动。选择一种就是选择一种不同设计方法。</p><h2 id="Demo地址"><a href="#Demo地址" class="headerlink" title="Demo地址"></a>Demo地址</h2><p><a href="https://github.com/109383670/ShootSomething" target="_blank" rel="noopener">Github代码</a><br><a href="http://wingftp.open.egret.com/ftproot/109383670/" target="_blank" rel="noopener">点击预览游戏</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了点时间分别体验了两个html5游戏制作工具：egret与cocos2d creator。&lt;br&gt;为了更好的对比，同时用两个工具做一个简单的射击demo。&lt;br&gt;使用后的体会：&lt;/p&gt;
&lt;h2 id=&quot;egret-（白鹭）&quot;&gt;&lt;a href=&quot;#egret-（白鹭）&quot; class=&quot;headerlink&quot; title=&quot;egret （白鹭）&quot;&gt;&lt;/a&gt;egret （白鹭）&lt;/h2&gt;&lt;h3 id=&quot;优点&quot;&gt;&lt;a href=&quot;#优点&quot; class=&quot;headerlink&quot; title=&quot;优点&quot;&gt;&lt;/a&gt;优点&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;开发工具齐全，从龙骨、粒子系统、资源管理、编译器等，基本上你需要的都提供了，省去了四处去找第三方工具库的麻烦，这个非常好。  &lt;/li&gt;
&lt;li&gt;开发用的是TypeScript语言。在没有任何基础的情况下，我边看例子，边学习TypeScript, 没有什么大的障碍。调试功能也整合得很好。  &lt;/li&gt;
&lt;li&gt;用exml进行UI可视化，直观，上手容易。&lt;/li&gt;
&lt;li&gt;提供云测试空间，可以直接发布免费提供的ft空间进行测试。想到真是太周到了，这是一条龙服务的节奏。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;缺点：&quot;&gt;&lt;a href=&quot;#缺点：&quot; class=&quot;headerlink&quot; title=&quot;缺点：&quot;&gt;&lt;/a&gt;缺点：&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;教程文档混乱，调试一个问题花了几个小时没找到原因，结果发现是一个类文件没有放在src的文件夹下，这么重要的东西，竟然在文档中没任何提及。&lt;/li&gt;
&lt;li&gt;exml这样的ui方式，对于初学者来说太过复杂了，刚开始学习的人，估计一头雾水，加上文档有混乱，入坑门槛比较高。&lt;/li&gt;
&lt;li&gt;在物理系统，碰撞系统的支持上，比较弱，可能html5游戏应该也用不到这么复杂的功能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;总结：&quot;&gt;&lt;a href=&quot;#总结：&quot; class=&quot;headerlink&quot; title=&quot;总结：&quot;&gt;&lt;/a&gt;总结：&lt;/h3&gt;&lt;p&gt;如果要开发html5游戏，之前用过cocos2d-x之类的，没有用过unity开发的，可以用egret。如果要进行大规模商业开发的，也建议用这个，各个功能比较完善，是成熟的产品。&lt;/p&gt;
    
    </summary>
    
    
      <category term="游戏开发" scheme="https://109383670.github.io/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/"/>
    
      <category term="html5" scheme="https://109383670.github.io/tags/html5/"/>
    
      <category term="egret" scheme="https://109383670.github.io/tags/egret/"/>
    
      <category term="cccreator" scheme="https://109383670.github.io/tags/cccreator/"/>
    
  </entry>
  
  <entry>
    <title>ios游戏分辨率问题</title>
    <link href="https://109383670.github.io/2017/03/06/ios%E6%B8%B8%E6%88%8F%E5%88%86%E8%BE%A8%E7%8E%87%E9%97%AE%E9%A2%98/"/>
    <id>https://109383670.github.io/2017/03/06/ios游戏分辨率问题/</id>
    <published>2017-03-06T04:48:56.000Z</published>
    <updated>2019-03-09T04:35:02.737Z</updated>
    
    <content type="html"><![CDATA[<p>屏幕分辨率一般指的是屏幕上像素的多少。所谓像素就是屏幕上的最小发光点，Led灯屏幕的一个像素就是一个Led灯。例如：640*960指的就是屏幕的宽和高上分别有640和960个像素。分辨率越高，图像越精细，也就是常说的高清。</p><h2 id="iPhones设备分辨率"><a href="#iPhones设备分辨率" class="headerlink" title="iPhones设备分辨率"></a>iPhones设备分辨率</h2><h3 id="英寸"><a href="#英寸" class="headerlink" title="英寸"></a>英寸</h3><p><img src="https://i.loli.net/2019/03/04/5c7ce79263590.png" alt="1708203-1768998327604e30"></p><h3 id="像素尺寸"><a href="#像素尺寸" class="headerlink" title="像素尺寸"></a>像素尺寸</h3><p><img src="https://i.loli.net/2019/03/04/5c7cedaea14ef.png" alt="1708203-75913098015ba4c2.png"></p><h3 id="尺寸表"><a href="#尺寸表" class="headerlink" title="尺寸表"></a>尺寸表</h3><p><img src="https://i.loli.net/2019/03/04/5c7cedcdcfc1d.png" alt="1708203-9184bee9ba12d3b5.png"></p><h2 id="游戏开发中用到的分辨率"><a href="#游戏开发中用到的分辨率" class="headerlink" title="游戏开发中用到的分辨率"></a>游戏开发中用到的分辨率</h2><h3 id="iPhone"><a href="#iPhone" class="headerlink" title="iPhone:"></a>iPhone:</h3><p><img src="https://i.loli.net/2019/03/04/5c7cf2bc17a7b.png" alt="屏幕快照 2019-03-04 下午5.40.41.png"></p><h3 id="iPad"><a href="#iPad" class="headerlink" title="iPad"></a>iPad</h3><p><img src="https://i.loli.net/2019/03/04/5c7cf66791bbc.png" alt="屏幕快照 2019-03-04 下午5.56.37.png"></p><a id="more"></a><h2 id="为什么适配不同分辨率"><a href="#为什么适配不同分辨率" class="headerlink" title="为什么适配不同分辨率"></a>为什么适配不同分辨率</h2><p>不同的设备有不同的分辨率，为了减少美术设计人员的工作量，统一化产品设计就必须适配各种分辨率。尽量做到一套设计，不同分辨率的设备都可以通用，不需要美术设计人员针对每一个分辨率版本都给出不同的设计方案，也便于维护升级。<br>一套设计也便于减少游戏安装包大小，优化资源，提高游戏运行速度。</p><h2 id="适配分辨率方案"><a href="#适配分辨率方案" class="headerlink" title="适配分辨率方案"></a>适配分辨率方案</h2><h3 id="1、针对不同的分辨率，给出不同的设计。"><a href="#1、针对不同的分辨率，给出不同的设计。" class="headerlink" title="1、针对不同的分辨率，给出不同的设计。"></a>1、针对不同的分辨率，给出不同的设计。</h3><ul><li>优点：<br>效果最好，因为针对每一个分辨率都做了专门的适配，不同的分辨率都能体现最好的设计效果。</li></ul><ul><li>缺点：<br>工作量大，维护困难，每一次升级修改都需要针对每一个分辨率的版本进行更新，大大增加了工作时间和出错的可能性。不利于扩展，如果市场上出现了新的设备，不同的分辨率，又得更新版本升级。</li></ul><h3 id="2、-按实际屏幕大小进行缩放"><a href="#2、-按实际屏幕大小进行缩放" class="headerlink" title="2、 按实际屏幕大小进行缩放"></a>2、 按实际屏幕大小进行缩放</h3><p>针对不同的分辨率，将游戏画面整个进行缩放，填充满整个屏幕。</p><ul><li><p>优点：<br>通用性高，工作量低。不管什么屏幕都是一套素材，一套代码，不需要额外的工作。</p></li><li><p>缺点：<br>画面严重失真，因为是按照实际屏幕进行缩放，所以如果实际屏幕的宽高比与设计的宽高比不同的话，画面就会出现变形。整个画面看起来像是被压扁或是拉长。</p><p>如下图，变形了：<br><img src="https://i.loli.net/2019/03/04/5c7cee40a9aaa.jpg" alt="123.jpg"></p></li></ul><h3 id="3、-按设计比例进行缩放"><a href="#3、-按设计比例进行缩放" class="headerlink" title="3、 按设计比例进行缩放"></a>3、 按设计比例进行缩放</h3><p>针对不同的分辨率，按固定的宽高比进行缩放。</p><ul><li><p>优点：<br>最大程度的还原设计师的设计，可以做到一套设计通用，不会出现失真。</p></li><li><p>缺点：<br>会在屏幕上下或者左右留下黑边，影响游戏体验。</p></li></ul><p>如下图，有黑边，UI位置暴露了：<br><img src="https://i.loli.net/2019/03/04/5c7cee40bc684.jpg" alt="222.jpg"></p><h3 id="4、固定高度适配"><a href="#4、固定高度适配" class="headerlink" title="4、固定高度适配"></a>4、固定高度适配</h3><p>  在3号方案的基础上，对按钮等UI元素根据分辨率进行动态计算调整。</p><ul><li><p>优点<br>一套设计通用，不会出现2、3中的问题。</p></li><li><p>缺点：<br>不能完全的还原设计师设计，要做出妥协。</p></li></ul><h2 id="实际开发中采用的方案"><a href="#实际开发中采用的方案" class="headerlink" title="实际开发中采用的方案"></a>实际开发中采用的方案</h2><p>实际开发中采用的方案是4号方案。4号方案能在保证画面不变形和出现黑边的情况下，最大程度的减少工作量。但是需要设计师巧妙的设计游戏背景图画。</p><p> 以下图为例：<br>正常的设计分辨率：<br><img src="https://i.loli.net/2019/03/04/5c7cee7671a87.jpg" alt="0x0ss.jpg"></p><p>ipad适配后的分辨率：</p><p><img src="https://i.loli.net/2019/03/04/5c7cee76866b0.jpg" alt="0x0ss-2.jpg"></p><h3 id="设计师设计比例"><a href="#设计师设计比例" class="headerlink" title="设计师设计比例"></a>设计师设计比例</h3><p>根据游戏的主要用户和市场上手机的主要分辨率，决定设计师设计游戏UI时使用的分辨率。设计师只需要注意分辨率的宽高比，宽高比决定了屏幕上的布局。设计师作图时，应该根据宽高比，最大化画布的大小。<br>比如：如果设计师的画布大小只有640x1136大小，当一旦需要1242x2208大小的图片时，设计师只能放大图片，这样就会导致图片质量下降。而如果设计师一开始的画布大小是2484x4416时，只需要将导出的图片缩小就可以了，不会过多的影响图片质量。</p><h3 id="实际使用比例"><a href="#实际使用比例" class="headerlink" title="实际使用比例"></a>实际使用比例</h3><p>游戏的主要人群是iPhone用户，而市场上的主流设备是iphone5以上，所以采用的设备宽高比是<strong>0.562</strong>，也就是iphone6的宽高比。</p><h3 id="计算背景图片需要大小："><a href="#计算背景图片需要大小：" class="headerlink" title="计算背景图片需要大小："></a>计算背景图片需要大小：</h3><p>根据要适配的屏幕宽高比，主要有3种：</p><ul><li>iphone 6 : 0.562 </li><li>iphone 4s : 0.667</li><li>ipad：0.75</li></ul><p>假设高度为1，那么这3种分辨率中，宽度最大的是ipad的宽度，为0.75。那么设计师要设计的背景图片的宽高比根据最大宽度原则，采用0.75。</p><h3 id="设计师如何工作"><a href="#设计师如何工作" class="headerlink" title="设计师如何工作"></a>设计师如何工作</h3><p>说明图如下：<br><img src="https://i.loli.net/2019/03/04/5c7cee9dcfd88.png" alt="粘贴图片.png"></p><h4 id="真实的分辨率："><a href="#真实的分辨率：" class="headerlink" title="真实的分辨率："></a>真实的分辨率：</h4><p>750x1334</p><h4 id="背景图片大小："><a href="#背景图片大小：" class="headerlink" title="背景图片大小："></a>背景图片大小：</h4><p>高度 = 1334<br>宽度 = 1002。计算过程：1334x0.75 = 1000.5 。近似取偶数 = <strong>1002</strong><br>最终大小：<strong>1002x1334</strong><br>设计师做图时，可以选择做一个2倍大的背景图。1002x2 = 2004、 1334x2 = 2668。</p><h4 id="设计师设计步骤："><a href="#设计师设计步骤：" class="headerlink" title="设计师设计步骤："></a>设计师设计步骤：</h4><ol><li>新建大小为 <strong>1500x2668</strong>的画布</li><li>安排按钮等UI布局</li><li>设计游戏背景图</li><li>将背景图单独拿出来，扩充为2004x2668大小的画布，将多出来的部分过渡好。</li></ol><p>注意：<br>设计师主要精力放在1500x2668这个画布上，主要的元素都要在这个画布上呈现。背景宽度扩充的部分只需要过渡好，让玩家看起来不突兀，自然就好。</p><p>作品：<br>设计师需要提交1002x1334的背景图，其他的ui元素正常提交，没有变动。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p> <a href="https://www.jianshu.com/p/41a8ccdf91ed" target="_blank" rel="noopener">iPhone屏幕分辨率和适配规则（基础篇）</a><br> <a href="https://help.apple.com/app-store-connect/?lang=zh-cn#/devd274dd925" target="_blank" rel="noopener">iOS app屏幕快照规范</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;屏幕分辨率一般指的是屏幕上像素的多少。所谓像素就是屏幕上的最小发光点，Led灯屏幕的一个像素就是一个Led灯。例如：640*960指的就是屏幕的宽和高上分别有640和960个像素。分辨率越高，图像越精细，也就是常说的高清。&lt;/p&gt;
&lt;h2 id=&quot;iPhones设备分辨率&quot;&gt;&lt;a href=&quot;#iPhones设备分辨率&quot; class=&quot;headerlink&quot; title=&quot;iPhones设备分辨率&quot;&gt;&lt;/a&gt;iPhones设备分辨率&lt;/h2&gt;&lt;h3 id=&quot;英寸&quot;&gt;&lt;a href=&quot;#英寸&quot; class=&quot;headerlink&quot; title=&quot;英寸&quot;&gt;&lt;/a&gt;英寸&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/03/04/5c7ce79263590.png&quot; alt=&quot;1708203-1768998327604e30&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;像素尺寸&quot;&gt;&lt;a href=&quot;#像素尺寸&quot; class=&quot;headerlink&quot; title=&quot;像素尺寸&quot;&gt;&lt;/a&gt;像素尺寸&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/03/04/5c7cedaea14ef.png&quot; alt=&quot;1708203-75913098015ba4c2.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;尺寸表&quot;&gt;&lt;a href=&quot;#尺寸表&quot; class=&quot;headerlink&quot; title=&quot;尺寸表&quot;&gt;&lt;/a&gt;尺寸表&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/03/04/5c7cedcdcfc1d.png&quot; alt=&quot;1708203-9184bee9ba12d3b5.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;游戏开发中用到的分辨率&quot;&gt;&lt;a href=&quot;#游戏开发中用到的分辨率&quot; class=&quot;headerlink&quot; title=&quot;游戏开发中用到的分辨率&quot;&gt;&lt;/a&gt;游戏开发中用到的分辨率&lt;/h2&gt;&lt;h3 id=&quot;iPhone&quot;&gt;&lt;a href=&quot;#iPhone&quot; class=&quot;headerlink&quot; title=&quot;iPhone:&quot;&gt;&lt;/a&gt;iPhone:&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/03/04/5c7cf2bc17a7b.png&quot; alt=&quot;屏幕快照 2019-03-04 下午5.40.41.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;iPad&quot;&gt;&lt;a href=&quot;#iPad&quot; class=&quot;headerlink&quot; title=&quot;iPad&quot;&gt;&lt;/a&gt;iPad&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/03/04/5c7cf66791bbc.png&quot; alt=&quot;屏幕快照 2019-03-04 下午5.56.37.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="游戏开发" scheme="https://109383670.github.io/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/"/>
    
      <category term="ios" scheme="https://109383670.github.io/tags/ios/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习记录</title>
    <link href="https://109383670.github.io/2017/02/27/Scrapy%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://109383670.github.io/2017/02/27/Scrapy学习记录/</id>
    <published>2017-02-27T05:19:50.000Z</published>
    <updated>2017-02-27T14:09:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scrapy-Shell-命令"><a href="#Scrapy-Shell-命令" class="headerlink" title="Scrapy Shell 命令:"></a>Scrapy Shell 命令:</h2><ul><li><p>开始抓取网页:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell <span class="string">'http://www.dytt8.net/index.htm'</span></span><br></pre></td></tr></table></figure></li><li><p>selector内容:</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">"//a/@href"</span>).extract()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></li><li><p>输出jsonItem：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dmoz -o items.json</span><br></pre></td></tr></table></figure></li></ul><h2 id="xpath"><a href="#xpath" class="headerlink" title="xpath:"></a>xpath:</h2><ul><li><p>following-sibling:<br>除自身外后面的同辈兄弟。如：td/following-sibling::td    同级td兄弟。</p></li><li><p>xpath中的序列从1开始：/a[1],代表a的第一个元素。没有[0]。</p></li><li><p>遍历多个变量：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t , l <span class="keyword">in</span> izip(response.xpath(strname), response.xpath(strurl)):</span><br></pre></td></tr></table></figure></li><li><p>r<br>在Python的string前面加上‘r’， 是为了告诉编译器这个string是个raw string，不要转意backslash ‘\’ 。 例如，\n 在raw string中，是两个字符，\和n， 而不会转意为换行符。由于正则表达式和 \ 会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上’r’。</p></li></ul><h2 id="激活pipeline"><a href="#激活pipeline" class="headerlink" title="激活pipeline:"></a>激活pipeline:</h2><p>在setting.py里，为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &apos;myproject.pipelines.PricePipeline&apos;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;&apos;工程名.pipelines.自定义处理pipe类名&apos;: 1&#125;</span><br></pre></td></tr></table></figure><h2 id="使用相对XPaths"><a href="#使用相对XPaths" class="headerlink" title="使用相对XPaths:"></a>使用相对XPaths:</h2><p>/或者//永远表示的是绝对路径，在嵌套xpath里，用’a/text()’这样的相对路径。</p><h2 id="response-urljoin"><a href="#response-urljoin" class="headerlink" title="response.urljoin:"></a>response.urljoin:</h2><p>方法建立绝对路径并且产生新的请求，并注册回调函数parse_dir_contents()来爬取需要的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">"ul.directory.dir-col &gt; li &gt; a::attr('href')"</span>):</span><br><span class="line">            url = response.urljoin(href.extract())</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_dir_contents)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_dir_contents</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</span><br><span class="line">            item = DmozItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'text()'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="递归抓取"><a href="#递归抓取" class="headerlink" title="递归抓取:"></a>递归抓取:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blurb2Spider</span><span class="params">(BaseSpider)</span>:</span></span><br><span class="line">   name = <span class="string">"blurb2"</span></span><br><span class="line">   allowed_domains = [<span class="string">"www.domain.com"</span>]</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">yield</span> self.make_requests_from_url(<span class="string">"http://www.domain.com/bookstore/new"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       hxs = HtmlXPathSelector(response)</span><br><span class="line">       urls = hxs.select(<span class="string">'//div[@class="bookListingBookTitle"]/a/@href'</span>).extract()</span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> urls:</span><br><span class="line">           <span class="keyword">yield</span> Request(urlparse.urljoin(<span class="string">'https://www.domain.com/'</span>, i[<span class="number">1</span>:]),callback=self.parse_url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse_url</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       hxs = HtmlXPathSelector(response)</span><br><span class="line">       <span class="keyword">print</span> response,<span class="string">'-------&gt;'</span></span><br></pre></td></tr></table></figure><h2 id="相对地址"><a href="#相对地址" class="headerlink" title="相对地址:"></a>相对地址:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urljoin</span><br><span class="line">urlparse.urljoin(response.url, myurl)</span><br></pre></td></tr></table></figure><h2 id="定制图片管道的例子"><a href="#定制图片管道的例子" class="headerlink" title="定制图片管道的例子:"></a>定制图片管道的例子:</h2><p>下面是一个图片管道的完整例子，其方法如上所示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.pipeline.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyImagesPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">'image_urls'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(image_url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Item contains no images"</span>)</span><br><span class="line">        item[<span class="string">'image_paths'</span>] = image_paths</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h2 id="定位要详细"><a href="#定位要详细" class="headerlink" title="定位要详细:"></a>定位要详细:</h2><p>//div[@id = “Zoom”]//img[1]/@src<br>div的定位要详细，如果是<br>//div/span/img[1]/@src<br>就返回为null,虽然firebug里面也没有问题。</p><h2 id="Strip"><a href="#Strip" class="headerlink" title="Strip():"></a>Strip():</h2><p>Python strip() 方法用于移除字符串头尾指定的字符（默认为空格）。<br>MapCompose(unicode.strip, unicode.title))  ，移除空格与换行<br>例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l.add_xpath(<span class="string">'image_time'</span>, <span class="string">'//div[@class = "co_content8"]/ul/text()[1]'</span>, MapCompose(unicode.strip, unicode.title))</span><br></pre></td></tr></table></figure><h2 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片:"></a>下载图片:</h2><p>settings.py中有一行ROBOTSTXT_OBEY = True，需要改成False，否则可能下载不了图片。<br>ROBOTSTXT_OBEY<br>是否遵守robot协议，有些网站的robot.txt中表明，不允许爬去，这时候，如果要爬去的话，就要设置为false，不遵守。</p><h2 id="No-Moulde-PIL-Find"><a href="#No-Moulde-PIL-Find" class="headerlink" title="No Moulde PIL Find:"></a>No Moulde PIL Find:</h2><p>直接用pycharm自带的interpreter安装pillow</p><h2 id="mac下要注意python的安装路径"><a href="#mac下要注意python的安装路径" class="headerlink" title="mac下要注意python的安装路径"></a>mac下要注意python的安装路径</h2><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><blockquote><p><a href="http://www.jianshu.com/p/078ad2067419" target="_blank" rel="noopener">http://www.jianshu.com/p/078ad2067419</a><br><a href="http://www.cnblogs.com/kylinlin/p/5405246.html" target="_blank" rel="noopener">http://www.cnblogs.com/kylinlin/p/5405246.html</a><br><a href="http://wiki.jikexueyuan.com/project/scrapy/item-pipeline.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/scrapy/item-pipeline.html</a><br><a href="http://www.open-open.com/lib/view/open1432868637316.html" target="_blank" rel="noopener">http://www.open-open.com/lib/view/open1432868637316.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Scrapy-Shell-命令&quot;&gt;&lt;a href=&quot;#Scrapy-Shell-命令&quot; class=&quot;headerlink&quot; title=&quot;Scrapy Shell 命令:&quot;&gt;&lt;/a&gt;Scrapy Shell 命令:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;开始抓取网页:&lt;/p&gt;
  &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy shell &lt;span class=&quot;string&quot;&gt;&#39;http://www.dytt8.net/index.htm&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;selector内容:&lt;/p&gt;
  &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;response.xpath(&lt;span class=&quot;string&quot;&gt;&quot;//a/@href&quot;&lt;/span&gt;).extract()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;输出jsonItem：&lt;/p&gt;
  &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy crawl dmoz -o items.json&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;xpath&quot;&gt;&lt;a href=&quot;#xpath&quot; class=&quot;headerlink&quot; title=&quot;xpath:&quot;&gt;&lt;/a&gt;xpath:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;following-sibling:&lt;br&gt;除自身外后面的同辈兄弟。如：td/following-sibling::td    同级td兄弟。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;xpath中的序列从1开始：/a[1],代表a的第一个元素。没有[0]。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;遍历多个变量：&lt;/p&gt;
  &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; t , l &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; izip(response.xpath(strname), response.xpath(strurl)):&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;r&lt;br&gt;在Python的string前面加上‘r’， 是为了告诉编译器这个string是个raw string，不要转意backslash ‘\’ 。 例如，\n 在raw string中，是两个字符，\和n， 而不会转意为换行符。由于正则表达式和 \ 会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上’r’。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;激活pipeline&quot;&gt;&lt;a href=&quot;#激活pipeline&quot; class=&quot;headerlink&quot; title=&quot;激活pipeline:&quot;&gt;&lt;/a&gt;激活pipeline:&lt;/h2&gt;&lt;p&gt;在setting.py里，为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ITEM_PIPELINES = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;apos;myproject.pipelines.PricePipeline&amp;apos;: 300,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。如：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ITEM_PIPELINES = &amp;#123;&amp;apos;工程名.pipelines.自定义处理pipe类名&amp;apos;: 1&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;使用相对XPaths&quot;&gt;&lt;a href=&quot;#使用相对XPaths&quot; class=&quot;headerlink&quot; title=&quot;使用相对XPaths:&quot;&gt;&lt;/a&gt;使用相对XPaths:&lt;/h2&gt;&lt;p&gt;/或者//永远表示的是绝对路径，在嵌套xpath里，用’a/text()’这样的相对路径。&lt;/p&gt;
&lt;h2 id=&quot;response-urljoin&quot;&gt;&lt;a href=&quot;#response-urljoin&quot; class=&quot;headerlink&quot; title=&quot;response.urljoin:&quot;&gt;&lt;/a&gt;response.urljoin:&lt;/h2&gt;&lt;p&gt;方法建立绝对路径并且产生新的请求，并注册回调函数parse_dir_contents()来爬取需要的数据。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, response)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; href &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; response.css(&lt;span class=&quot;string&quot;&gt;&quot;ul.directory.dir-col &amp;gt; li &amp;gt; a::attr(&#39;href&#39;)&quot;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            url = response.urljoin(href.extract())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;yield&lt;/span&gt; scrapy.Request(url, callback=self.parse_dir_contents)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;parse_dir_contents&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, response)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; sel &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; response.xpath(&lt;span class=&quot;string&quot;&gt;&#39;//ul/li&#39;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            item = DmozItem()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            item[&lt;span class=&quot;string&quot;&gt;&#39;title&#39;&lt;/span&gt;] = sel.xpath(&lt;span class=&quot;string&quot;&gt;&#39;a/text()&#39;&lt;/span&gt;).extract()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            item[&lt;span class=&quot;string&quot;&gt;&#39;link&#39;&lt;/span&gt;] = sel.xpath(&lt;span class=&quot;string&quot;&gt;&#39;a/@href&#39;&lt;/span&gt;).extract()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            item[&lt;span class=&quot;string&quot;&gt;&#39;desc&#39;&lt;/span&gt;] = sel.xpath(&lt;span class=&quot;string&quot;&gt;&#39;text()&#39;&lt;/span&gt;).extract()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;yield&lt;/span&gt; item&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Scrapy" scheme="https://109383670.github.io/tags/Scrapy/"/>
    
      <category term="Learning" scheme="https://109383670.github.io/tags/Learning/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy安装与运行记录</title>
    <link href="https://109383670.github.io/2017/02/27/Scrapy%E5%AE%89%E8%A3%85%E4%B8%8E%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/"/>
    <id>https://109383670.github.io/2017/02/27/Scrapy安装与运行记录/</id>
    <published>2017-02-27T04:02:44.000Z</published>
    <updated>2017-02-27T04:50:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装Homebrew"><a href="#安装Homebrew" class="headerlink" title="安装Homebrew"></a>安装Homebrew</h2><pre><code>ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></pre><h2 id="安装python"><a href="#安装python" class="headerlink" title="安装python"></a>安装python</h2><pre><code>brew install python</code></pre><p>Homebrew会自动安装好Setuptools和 pip 。<br>Setuptools提供 easy_install 命令，实现通过网络（通常Internet）下载和安装第三方Python包。 还可以轻松地将这种网络安装的方式加入到自己开发的Python应用中。<br>pip 是一款方便安装和管理Python 包的工具。</p><h2 id="安装Scrapy"><a href="#安装Scrapy" class="headerlink" title="安装Scrapy"></a>安装Scrapy</h2><pre><code>pip install scrapy</code></pre><h3 id="Scrapy-使用"><a href="#Scrapy-使用" class="headerlink" title="Scrapy 使用:"></a>Scrapy 使用:</h3><ul><li>IDE工具：pycharm社区免费版本<ul><li>教程参考: <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial</a> </li></ul></li></ul><h3 id="命令"><a href="#命令" class="headerlink" title="命令:"></a>命令:</h3><ul><li><p>生成HelloWorld的Scrapy工程  </p><pre><code>scrapy startproject HelloWorld  </code></pre></li><li>在pycharm IDE中配置命令<br><img src="/images/A373E711-D1C1-42FB-83BB-772FD44EB369.png" alt="A373E711-D1C1-42FB-83BB-772FD44EB369"></li></ul><h3 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h3><p>   执行:</p><pre><code>scrapy crawl MyPa (MyPa是自己在类中定义的爬虫名字)，</code></pre><p>   相当于在终端执行：</p><pre><code>/usr/local/bin/python /usr/local/lib/python2.7/site-packages/scrapy/cmdline.py crawl MyPa</code></pre><p> 注意：要小心python的路径，如果python的路径不对，还是会报错。这里指的路径是系统路径与pycharm里设置的python路径。<br>            在终端里用which python查看一下路径,如果与pycharm设置里的不同，将修改成更系统路径一样的。</p><h2 id="中文问题"><a href="#中文问题" class="headerlink" title="中文问题:"></a>中文问题:</h2><ul><li>shell里输出的是utf-8编码,用print可打印出中文。</li><li>用变量格式化的方式，不直接在xpath中用中文字符，而是用一个变量代替。如’中文’,用u’中文’。或者在字符串前加u。如u”//a/text()”</li><li><p>打印的时候可以参考：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sel <span class="keyword">in</span>  response.xpath(<span class="string">"//div[@id='mcontent']/div/p"</span>):</span><br><span class="line">      conect = sel.xpath(<span class="string">"text()"</span>).extract()</span><br><span class="line">         <span class="keyword">for</span> t <span class="keyword">in</span> conect:</span><br><span class="line">           print(t.encode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></table></figure></li><li><p>pycharm中支持中文</p><ol><li><p>代码页加入:</p><p> <code># -*-coding:utf-8-*-</code></p></li><li><p>代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">strpath = <span class="string">u"//td[descendant::a[contains(text(),'中文字符')]]"</span>。</span><br><span class="line">或者</span><br><span class="line">strz = <span class="string">'中文字符'</span></span><br><span class="line">strpath = <span class="string">u"//td[descendant::a[contains(text(),%s)]]%strz"</span></span><br></pre></td></tr></table></figure></li></ol></li><li><p>json输出中文：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.file = codecs.open(<span class="string">"items.json"</span>, <span class="string">"wb"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    line = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">"\n"</span></span><br><span class="line">    self.file.write(line)</span><br><span class="line">    <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.file.close()</span><br></pre></td></tr></table></figure><a id="more"></a></li></ul><p>##读取文件<br>with codecs.open(file_name, “r”,encoding=’utf-8’, errors=’ignore’) as fdata:</p><p>##decode encode<br>decode 总是返回unicode字符<br>encode 总是接受一个unicode字符进行转换</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装Homebrew&quot;&gt;&lt;a href=&quot;#安装Homebrew&quot; class=&quot;headerlink&quot; title=&quot;安装Homebrew&quot;&gt;&lt;/a&gt;安装Homebrew&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装python&quot;&gt;&lt;a href=&quot;#安装python&quot; class=&quot;headerlink&quot; title=&quot;安装python&quot;&gt;&lt;/a&gt;安装python&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;brew install python
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Homebrew会自动安装好Setuptools和 pip 。&lt;br&gt;Setuptools提供 easy_install 命令，实现通过网络（通常Internet）下载和安装第三方Python包。 还可以轻松地将这种网络安装的方式加入到自己开发的Python应用中。&lt;br&gt;pip 是一款方便安装和管理Python 包的工具。&lt;/p&gt;
&lt;h2 id=&quot;安装Scrapy&quot;&gt;&lt;a href=&quot;#安装Scrapy&quot; class=&quot;headerlink&quot; title=&quot;安装Scrapy&quot;&gt;&lt;/a&gt;安装Scrapy&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;pip install scrapy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Scrapy-使用&quot;&gt;&lt;a href=&quot;#Scrapy-使用&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 使用:&quot;&gt;&lt;/a&gt;Scrapy 使用:&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;IDE工具：pycharm社区免费版本&lt;ul&gt;
&lt;li&gt;教程参考: &lt;a href=&quot;http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#intro-tutorial&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;命令&quot;&gt;&lt;a href=&quot;#命令&quot; class=&quot;headerlink&quot; title=&quot;命令:&quot;&gt;&lt;/a&gt;命令:&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;生成HelloWorld的Scrapy工程  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy startproject HelloWorld  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;在pycharm IDE中配置命令&lt;br&gt;&lt;img src=&quot;/images/A373E711-D1C1-42FB-83BB-772FD44EB369.png&quot; alt=&quot;A373E711-D1C1-42FB-83BB-772FD44EB369&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;原理：&quot;&gt;&lt;a href=&quot;#原理：&quot; class=&quot;headerlink&quot; title=&quot;原理：&quot;&gt;&lt;/a&gt;原理：&lt;/h3&gt;&lt;p&gt;   执行:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy crawl MyPa (MyPa是自己在类中定义的爬虫名字)，
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;   相当于在终端执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/local/bin/python /usr/local/lib/python2.7/site-packages/scrapy/cmdline.py crawl MyPa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 注意：要小心python的路径，如果python的路径不对，还是会报错。这里指的路径是系统路径与pycharm里设置的python路径。&lt;br&gt;            在终端里用which python查看一下路径,如果与pycharm设置里的不同，将修改成更系统路径一样的。&lt;/p&gt;
&lt;h2 id=&quot;中文问题&quot;&gt;&lt;a href=&quot;#中文问题&quot; class=&quot;headerlink&quot; title=&quot;中文问题:&quot;&gt;&lt;/a&gt;中文问题:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;shell里输出的是utf-8编码,用print可打印出中文。&lt;/li&gt;
&lt;li&gt;用变量格式化的方式，不直接在xpath中用中文字符，而是用一个变量代替。如’中文’,用u’中文’。或者在字符串前加u。如u”//a/text()”&lt;/li&gt;
&lt;li&gt;&lt;p&gt;打印的时候可以参考：&lt;/p&gt;
 &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; sel &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt;  response.xpath(&lt;span class=&quot;string&quot;&gt;&quot;//div[@id=&#39;mcontent&#39;]/div/p&quot;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      conect = sel.xpath(&lt;span class=&quot;string&quot;&gt;&quot;text()&quot;&lt;/span&gt;).extract()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; t &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; conect:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;           print(t.encode(&lt;span class=&quot;string&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pycharm中支持中文&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;代码页加入:&lt;/p&gt;
&lt;p&gt; &lt;code&gt;# -*-coding:utf-8-*-&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;代码:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;strpath = &lt;span class=&quot;string&quot;&gt;u&quot;//td[descendant::a[contains(text(),&#39;中文字符&#39;)]]&quot;&lt;/span&gt;。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;或者&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;strz = &lt;span class=&quot;string&quot;&gt;&#39;中文字符&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;strpath = &lt;span class=&quot;string&quot;&gt;u&quot;//td[descendant::a[contains(text(),%s)]]%strz&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;json输出中文：&lt;/p&gt;
  &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.file = codecs.open(&lt;span class=&quot;string&quot;&gt;&quot;items.json&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;wb&quot;&lt;/span&gt;, encoding=&lt;span class=&quot;string&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;process_item&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, item, spider)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    line = json.dumps(dict(item), ensure_ascii=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;) + &lt;span class=&quot;string&quot;&gt;&quot;\n&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.file.write(line)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; item&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;spider_closed&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, spider)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.file.close()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Scrapy" scheme="https://109383670.github.io/tags/Scrapy/"/>
    
      <category term="Setup" scheme="https://109383670.github.io/tags/Setup/"/>
    
  </entry>
  
  <entry>
    <title>hex+mac安装记录</title>
    <link href="https://109383670.github.io/2017/02/22/hex+mac%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/"/>
    <id>https://109383670.github.io/2017/02/22/hex+mac安装记录/</id>
    <published>2017-02-22T12:06:47.000Z</published>
    <updated>2019-03-03T16:08:25.948Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有用的命令-hexo所在目录-："><a href="#有用的命令-hexo所在目录-：" class="headerlink" title="有用的命令(hexo所在目录)："></a>有用的命令(hexo所在目录)：</h2><ul><li>sudo hexo clean -清除</li><li>sudo hexo g -d  直接发布部署</li><li>sudo hexo g  生成   </li><li>sudo hexo s   打开本地服务器   <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> 浏览</li></ul><h3 id="本地预览步骤："><a href="#本地预览步骤：" class="headerlink" title="本地预览步骤："></a>本地预览步骤：</h3><ol><li>sudo hexo g</li><li>sudo hexo s</li><li><a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a></li></ol><h2 id="安装参考："><a href="#安装参考：" class="headerlink" title="安装参考："></a>安装参考：</h2><p><a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">参考域名绑定部分、修改主题</a><br><a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">各种出现的问题总结</a> </p><h2 id="hexo更新"><a href="#hexo更新" class="headerlink" title="hexo更新"></a>hexo更新</h2><p><a href="http://www.starlin.top/2018/08/24/更新Hexo版本和Next主题/" target="_blank" rel="noopener">更新Hexo版本和Next主题</a><br><a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo官网</a></p><h2 id="具体安装时出现的问题："><a href="#具体安装时出现的问题：" class="headerlink" title="具体安装时出现的问题："></a>具体安装时出现的问题：</h2><h3 id="不能执行hexo命令"><a href="#不能执行hexo命令" class="headerlink" title="不能执行hexo命令"></a>不能执行hexo命令</h3><p>只有init,help,version三个命令。解决方案：要在hexo目录下执行。</p><h3 id="注意坑：执行hexo-server时出错"><a href="#注意坑：执行hexo-server时出错" class="headerlink" title="注意坑：执行hexo server时出错"></a>注意坑：执行hexo server时出错</h3><p>_config.xml里，type: repo: branch:后面，要有一个空格。</p><h3 id="在DNS的配置里，加入固定的两个IP"><a href="#在DNS的配置里，加入固定的两个IP" class="headerlink" title="在DNS的配置里，加入固定的两个IP"></a>在DNS的配置里，加入固定的两个IP</h3><blockquote><p>@        A        192.30.252.153<br>@        A        192.30.252.154</p></blockquote><h3 id="不能连接git，提示22端口错误"><a href="#不能连接git，提示22端口错误" class="headerlink" title="不能连接git，提示22端口错误"></a>不能连接git，提示22端口错误</h3><p>git网站中的ssh证书失效，要重新生成，参考帮助说明。<br><a href="https://help.github.com/articles/connecting-to-github-with-ssh/" target="_blank" rel="noopener">ssh生成</a><br><a id="more"></a></p><h3 id="git证书生成时，不能填写passphrase这个东西，自己回车跳过"><a href="#git证书生成时，不能填写passphrase这个东西，自己回车跳过" class="headerlink" title="git证书生成时，不能填写passphrase这个东西，自己回车跳过"></a>git证书生成时，不能填写passphrase这个东西，自己回车跳过</h3><h2 id="其他："><a href="#其他：" class="headerlink" title="其他："></a>其他：</h2><p><a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">Next风格不错</a><br><a href="https://github.com/iissnan/hexo-theme-next/wiki" target="_blank" rel="noopener">Next的设置GitWiki里很详细</a><br><a href="http://prozhuchen.com/2015/10/05/Hexo博客之改字体/" target="_blank" rel="noopener">字体及字体大小修改</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;有用的命令-hexo所在目录-：&quot;&gt;&lt;a href=&quot;#有用的命令-hexo所在目录-：&quot; class=&quot;headerlink&quot; title=&quot;有用的命令(hexo所在目录)：&quot;&gt;&lt;/a&gt;有用的命令(hexo所在目录)：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;sudo hexo clean -清除&lt;/li&gt;
&lt;li&gt;sudo hexo g -d  直接发布部署&lt;/li&gt;
&lt;li&gt;sudo hexo g  生成   &lt;/li&gt;
&lt;li&gt;sudo hexo s   打开本地服务器   &lt;a href=&quot;http://localhost:4000/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://localhost:4000/&lt;/a&gt; 浏览&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;本地预览步骤：&quot;&gt;&lt;a href=&quot;#本地预览步骤：&quot; class=&quot;headerlink&quot; title=&quot;本地预览步骤：&quot;&gt;&lt;/a&gt;本地预览步骤：&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;sudo hexo g&lt;/li&gt;
&lt;li&gt;sudo hexo s&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://localhost:4000/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://localhost:4000/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;安装参考：&quot;&gt;&lt;a href=&quot;#安装参考：&quot; class=&quot;headerlink&quot; title=&quot;安装参考：&quot;&gt;&lt;/a&gt;安装参考：&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://my.oschina.net/ryaneLee/blog/638440&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考域名绑定部分、修改主题&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://my.oschina.net/ryaneLee/blog/638440&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;各种出现的问题总结&lt;/a&gt; &lt;/p&gt;
&lt;h2 id=&quot;hexo更新&quot;&gt;&lt;a href=&quot;#hexo更新&quot; class=&quot;headerlink&quot; title=&quot;hexo更新&quot;&gt;&lt;/a&gt;hexo更新&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://www.starlin.top/2018/08/24/更新Hexo版本和Next主题/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;更新Hexo版本和Next主题&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://hexo.io/zh-cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hexo官网&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;具体安装时出现的问题：&quot;&gt;&lt;a href=&quot;#具体安装时出现的问题：&quot; class=&quot;headerlink&quot; title=&quot;具体安装时出现的问题：&quot;&gt;&lt;/a&gt;具体安装时出现的问题：&lt;/h2&gt;&lt;h3 id=&quot;不能执行hexo命令&quot;&gt;&lt;a href=&quot;#不能执行hexo命令&quot; class=&quot;headerlink&quot; title=&quot;不能执行hexo命令&quot;&gt;&lt;/a&gt;不能执行hexo命令&lt;/h3&gt;&lt;p&gt;只有init,help,version三个命令。解决方案：要在hexo目录下执行。&lt;/p&gt;
&lt;h3 id=&quot;注意坑：执行hexo-server时出错&quot;&gt;&lt;a href=&quot;#注意坑：执行hexo-server时出错&quot; class=&quot;headerlink&quot; title=&quot;注意坑：执行hexo server时出错&quot;&gt;&lt;/a&gt;注意坑：执行hexo server时出错&lt;/h3&gt;&lt;p&gt;_config.xml里，type: repo: branch:后面，要有一个空格。&lt;/p&gt;
&lt;h3 id=&quot;在DNS的配置里，加入固定的两个IP&quot;&gt;&lt;a href=&quot;#在DNS的配置里，加入固定的两个IP&quot; class=&quot;headerlink&quot; title=&quot;在DNS的配置里，加入固定的两个IP&quot;&gt;&lt;/a&gt;在DNS的配置里，加入固定的两个IP&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;@        A        192.30.252.153&lt;br&gt;@        A        192.30.252.154&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;不能连接git，提示22端口错误&quot;&gt;&lt;a href=&quot;#不能连接git，提示22端口错误&quot; class=&quot;headerlink&quot; title=&quot;不能连接git，提示22端口错误&quot;&gt;&lt;/a&gt;不能连接git，提示22端口错误&lt;/h3&gt;&lt;p&gt;git网站中的ssh证书失效，要重新生成，参考帮助说明。&lt;br&gt;&lt;a href=&quot;https://help.github.com/articles/connecting-to-github-with-ssh/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ssh生成&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="https://109383670.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://109383670.github.io/2017/02/22/hello-world/"/>
    <id>https://109383670.github.io/2017/02/22/hello-world/</id>
    <published>2017-02-22T04:41:28.000Z</published>
    <updated>2017-02-22T04:42:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
